{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unexpected-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-principal",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continuous-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Path where the data is stored\n",
    "SOURCE_PATH = '../../../data'\n",
    "# Directory inside SOURCE_PATH where the original data is stored\n",
    "ORIGINAL_DATA_DIR = '/EXOSAFE'\n",
    "# Directory inside SOURCE_PATH where the derived data is stored\n",
    "DERIVED_DATA_DIR = '/derived_data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Experiment params\n",
    "DATE_EXPERIMENTS = '24022021'\n",
    "N_EXPERIMENTS = 15\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "# ID of the training and test data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0003_11042021'\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "CV = 5\n",
    "# % of the data for the train, validation and test sets\n",
    "SPLIT_PCT = (0.7, 0.15, 0.15) # (train, validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-transition",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worth-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate force vectors of each force cell to align them\n",
    "rotations = {\n",
    "    1: [180, 90, 0],\n",
    "    2: [180, 90, 0],\n",
    "    3: [180, 0, -90],\n",
    "    4: [0, 0, -90],\n",
    "    5: [0, 0, 0],\n",
    "    6: [0, 180, 0],\n",
    "    7: [0, 90, 0],\n",
    "    8: [0, 0, 90],\n",
    "}\n",
    "\n",
    "def rotate_vector(v, axis, angle):\n",
    "    '''\n",
    "    Args:\n",
    "    - v (np.array): Vector to be rotated\n",
    "    - axis (int): Axis along the rotation is performed\n",
    "    - angle (int): Rotation angle\n",
    "    \n",
    "    Returns:\n",
    "    - (np.array)): Rotated vector\n",
    "    '''\n",
    "    if axis == 0:\n",
    "        # X\n",
    "        v = v.dot(np.array([[1, 0, 0], [0, np.cos(np.radians(angle)), np.sin(np.radians(angle))], [0, np.sin(np.radians(angle)), np.cos(np.radians(angle))]]))\n",
    "    elif axis == 1:\n",
    "        # Y\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), 0, np.sin(np.radians(angle))], [0, 1, 0], [-np.sin(np.radians(angle)), 0, np.cos(np.radians(angle))]]))\n",
    "    elif axis == 2:\n",
    "        # Z\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle)), 0], [np.sin(np.radians(angle)), np.cos(np.radians(angle)), 0], [0, 0, 1]]))\n",
    "    else:\n",
    "        raise ValueError('Invalid axis')\n",
    "\n",
    "    return v\n",
    "\n",
    "@dask.delayed\n",
    "def rotate_row(row):\n",
    "    '''\n",
    "    Rotate the force vectors in a row. Dask function.\n",
    "    '''\n",
    "    for i in range(1, N_CELLS + 1):\n",
    "        cols = ['F{}x'.format(str(i)), 'F{}y'.format(str(i)), 'F{}z'.format(str(i))]\n",
    "        for ax in range(3):\n",
    "            row[cols] = rotate_vector(row[cols], ax, rotations[i][ax])\n",
    "            \n",
    "    return row\n",
    "\n",
    "def process_parameters_sheet(params_df):\n",
    "    '''\n",
    "    Process the data in the given pd.DataFrame from the raw excel sheet. \n",
    "    \n",
    "    Args:\n",
    "    - params_df (pd.DataFrame): DataFrame of the parameters excel sheet.\n",
    "    \n",
    "    Returns:\n",
    "    - params_dict (dict): Dictionary with all the parameter in the input DataFrame.\n",
    "    '''\n",
    "    params_dict = {}\n",
    "    params_dict['ExoHipMissalign'] = params_df.iloc[2, 1]\n",
    "    params_dict['ExoKneeMissalign'] = params_df.iloc[2, 2]\n",
    "    params_dict['MarchVelocity'] = params_df.iloc[0, 11]\n",
    "    params_dict['TimeShift'] = params_df.iloc[0, 12]\n",
    "    params_dict['SkinConfig'] = params_df.iloc[0, 13]\n",
    "    \n",
    "    return params_dict\n",
    "\n",
    "def shift_leg_data(df, time_shift, total_len, data_res=0.01):\n",
    "    '''\n",
    "    Shift the data from the leg replica using the known time_shift from the experiment\n",
    "    parameters to match the exoskeleton data in time and lenght.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with the data of the leg replica\n",
    "    - time_shift (float): Shifting time to applied to the data.\n",
    "    - total_len (int): Total desired lenght for the data.\n",
    "    - data_res (float): Data resolution (in seconds).\n",
    "    \n",
    "    Returns:\n",
    "    - (pd.DataFrame): DataFrame with the data of the leg replica shifted.\n",
    "    '''\n",
    "    idx_start = math.ceil(time_shift / data_res)\n",
    "    idx_end = total_len + idx_start\n",
    "    return df.iloc[idx_start:idx_end].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "irish-drive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1:\n",
      "[########################################] | 100% Completed |  1min  6.0s\n",
      "\n",
      "Processing file 2:\n",
      "[########################################] | 100% Completed |  1min  4.3s\n",
      "\n",
      "Processing file 3:\n",
      "[########################################] | 100% Completed |  1min  3.8s\n",
      "\n",
      "Processing file 4:\n",
      "[########################################] | 100% Completed |  1min  6.3s\n",
      "\n",
      "Processing file 5:\n",
      "[########################################] | 100% Completed |  1min  4.4s\n",
      "\n",
      "Processing file 6:\n",
      "[########################################] | 100% Completed |  1min 11.3s\n",
      "\n",
      "Processing file 7:\n",
      "[########################################] | 100% Completed |  1min  3.9s\n",
      "\n",
      "Processing file 8:\n",
      "[########################################] | 100% Completed |  1min 10.8s\n",
      "\n",
      "Processing file 9:\n",
      "[########################################] | 100% Completed |  1min  6.1s\n",
      "\n",
      "Processing file 10:\n",
      "[########################################] | 100% Completed |  1min  2.6s\n",
      "\n",
      "Processing file 11:\n",
      "[########################################] | 100% Completed |  1min  2.7s\n",
      "\n",
      "Processing file 12:\n",
      "[########################################] | 100% Completed |  1min  9.5s\n",
      "\n",
      "Processing file 13:\n",
      "[########################################] | 100% Completed |  1min  5.0s\n",
      "\n",
      "Processing file 14:\n",
      "[########################################] | 100% Completed |  1min  3.7s\n",
      "\n",
      "Processing file 15:\n",
      "[########################################] | 100% Completed |  1min  3.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_EXPERIMENTS):\n",
    "    print('Processing file {}:'.format(i + 1))\n",
    "    # Create the directory to save the resulting data\n",
    "    save_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i + 1))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Select only the relevant excel sheets \n",
    "    sheets = ['Parameters', 'RawForces', 'ForceCells', 'H3raw', 'H3processed', 'Leg-Replica']\n",
    "    # Load the data\n",
    "    data_df = pd.read_excel(SOURCE_PATH + ORIGINAL_DATA_DIR + '/' + DATE_EXPERIMENTS + '/0{}-'.format(i + 1) + DATE_EXPERIMENTS + '.xlsx', sheet_name=sheets)\n",
    "\n",
    "    # Pre-process the data\n",
    "    data_df[sheets[0]] = process_parameters_sheet(data_df[sheets[0]])\n",
    "    \n",
    "    # Apply the rotation matrix to each force vector\n",
    "    forces_ddf = dd.from_pandas(data_df[sheets[2]], npartitions=int(len(data_df[sheets[2]]) / 100))\n",
    "    forces_ddf = forces_ddf.apply(rotate_row, axis=1, meta=forces_ddf)\n",
    "    with ProgressBar():\n",
    "        data_df[sheets[2]] = forces_ddf.compute()\n",
    "        \n",
    "        \n",
    "    leg_df_raw = data_df[sheets[5]].iloc[:, :3]\n",
    "    # Correct the time shift between the data from the leg and the data from the exo\n",
    "    leg_df_processed = shift_leg_data(data_df[sheets[5]].iloc[:, 3:], data_df[sheets[0]]['TimeShift'], len(data_df[sheets[4]]))\n",
    "    \n",
    "    assert(len(leg_df_processed) == len(data_df[sheets[4]]) == len(data_df[sheets[2]]))    \n",
    "\n",
    "    json.dump(data_df[sheets[0]], open(save_dir + '/parameters.json', 'w'))\n",
    "    data_df[sheets[1]].to_csv(save_dir + '/force_cells_raw.csv', index=False)\n",
    "    data_df[sheets[2]].to_csv(save_dir + '/force_cells_processed.csv', index=False)\n",
    "    data_df[sheets[3]].to_csv(save_dir + '/H3_raw.csv', index=False)\n",
    "    data_df[sheets[4]].to_csv(save_dir + '/H3_processed.csv', index=False)\n",
    "    leg_df_raw.to_csv(save_dir + '/leg_raw.csv', index=False)\n",
    "    leg_df_processed.to_csv(save_dir + '/leg_processed.csv', index=False)\n",
    "    \n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-schedule",
   "metadata": {},
   "source": [
    "## Features and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vocational-carroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 15\n",
      "Selected features: ['LHipPos', 'LHipVel', 'LHipAcc', 'LHipTorque', 'LKneePos', 'LKneeVel', 'LKneeAcc', 'LKneeTorque', 'LAnklePos', 'LAnkleVel', 'LAnkleAcc', 'LAnkleTorque', 'LegKneePositionFiltered', 'LegKneeVelocityFiltered', 'LegKneeTorqueFiltered']\n",
      "\n",
      "\n",
      "Number of targets: 24\n",
      "Selected targets: ['F1x', 'F1y', 'F1z', 'F2x', 'F2y', 'F2z', 'F3x', 'F3y', 'F3z', 'F4x', 'F4y', 'F4z', 'F5x', 'F5y', 'F5z', 'F6x', 'F6y', 'F6z', 'F7x', 'F7y', 'F7z', 'F8x', 'F8y', 'F8z']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exceptional-humanity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping 8722 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 1 -> X: (8721, 15), Y: (8721, 24) \n",
      "\n",
      "Droping 8736 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 2 -> X: (8735, 15), Y: (8735, 24) \n",
      "\n",
      "Droping 8589 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 3 -> X: (8588, 15), Y: (8588, 24) \n",
      "\n",
      "Droping 8726 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 4 -> X: (8725, 15), Y: (8725, 24) \n",
      "\n",
      "Droping 8624 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 5 -> X: (8623, 15), Y: (8623, 24) \n",
      "\n",
      "Droping 8760 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 6 -> X: (8759, 15), Y: (8759, 24) \n",
      "\n",
      "Droping 8639 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 7 -> X: (8638, 15), Y: (8638, 24) \n",
      "\n",
      "Droping 8773 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 8 -> X: (8772, 15), Y: (8772, 24) \n",
      "\n",
      "Droping 8769 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 9 -> X: (8768, 15), Y: (8768, 24) \n",
      "\n",
      "Droping 8705 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 10 -> X: (8704, 15), Y: (8704, 24) \n",
      "\n",
      "Droping 8659 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 11 -> X: (8658, 15), Y: (8658, 24) \n",
      "\n",
      "Droping 8793 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 12 -> X: (8792, 15), Y: (8792, 24) \n",
      "\n",
      "Droping 8599 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 13 -> X: (8598, 15), Y: (8598, 24) \n",
      "\n",
      "Droping 8750 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 14 -> X: (8749, 15), Y: (8749, 24) \n",
      "\n",
      "Droping 8717 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 15 -> X: (8716, 15), Y: (8716, 24) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i in range(1, N_EXPERIMENTS + 1):\n",
    "    # Define the path to load the data\n",
    "    data_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(data_dir + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(data_dir + '/H3_processed.csv')\n",
    "    leg_df = pd.read_csv(data_dir + '/leg_processed.csv')\n",
    "    features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    \n",
    "    idx_aux = targets_df.duplicated(keep='first')\n",
    "    targets_df = targets_df.loc[~idx_aux]\n",
    "    features_df = features_df.loc[~idx_aux]\n",
    "    print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "    \n",
    "    # Rename columns to manage with some typos\n",
    "    features_df = features_df.rename(columns={'LankleTorque': 'LAnkleTorque', 'RankleTorque': 'RAnkleTorque'})\n",
    "\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    # Store the final array\n",
    "    targets_dict[i] = targets_df[targets].values\n",
    "    features_dict[i] = features_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-bracelet",
   "metadata": {},
   "source": [
    "## Normalization and split for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "possible-allowance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (12): [2, 11, 10, 6, 12, 3, 4, 8, 9, 5, 1, 15]\n",
      "Test experiments ids (2): [7, 14]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(1, N_EXPERIMENTS + 1))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[:int(N_EXPERIMENTS * (SPLIT_PCT[0] + SPLIT_PCT[1]))]\n",
    "test_experiments = experiments[-int(N_EXPERIMENTS * SPLIT_PCT[2]):]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fifth-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> X: (104561, 15), Y: (104561, 24)\n",
      "Test -> X: (17387, 15), Y: (17387, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([features_dict[i] for i in train_experiments], axis=0)\n",
    "Y_train = np.concatenate([targets_dict[i] for i in train_experiments], axis=0)\n",
    "X_test = np.concatenate([features_dict[i] for i in test_experiments], axis=0)\n",
    "Y_test = np.concatenate([targets_dict[i] for i in test_experiments], axis=0)\n",
    "\n",
    "print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Test -> X: {}, Y: {}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "japanese-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> \n",
      " min: [-1.33629602 -1.26484648 -3.10316622 -2.26223808 -0.72196185 -2.90216626\n",
      " -4.35645598 -1.82089691 -2.55209014 -2.36611435 -4.71975594 -2.32476878\n",
      " -0.91924672 -2.556158   -2.68157726], \n",
      " max: [1.66771134 2.13901227 2.91209061 3.13601047 2.83730418 2.20772623\n",
      " 2.54286126 3.87695105 1.6506851  3.29211458 3.68978761 7.49181857\n",
      " 3.03174483 3.90023529 0.81419454], \n",
      " mean: [ 6.59162072e-17  1.25036929e-17  1.53577967e-17 -2.17455529e-18\n",
      "  4.70247581e-17 -1.52898419e-18 -7.72986450e-19 -3.26183293e-17\n",
      " -2.96826797e-16 -1.01252731e-17 -9.93839721e-19 -7.82839904e-17\n",
      "  0.00000000e+00  1.16542572e-17  6.95857692e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Test -> \n",
      " min: [-1.31007256 -1.21947194 -2.39689804 -1.88368534 -0.72154655 -2.71436524\n",
      " -3.22263164 -1.59430896 -2.38972514 -2.3557292  -1.61207976 -1.77671285\n",
      " -0.79982154 -1.65763474 -2.65879689], \n",
      " max: [ 1.6556621   1.98928208  2.60887042  2.93934282  2.37315545  1.67043347\n",
      "  2.22465618  3.90613018  1.65454986  2.09792576  3.56101584 11.30813031\n",
      "  2.32522312  3.71964724  0.77958802], \n",
      " mean: [-7.74959035e-03 -5.78678856e-04 -5.15798991e-04  9.48201875e-03\n",
      " -3.13215156e-02 -5.66664886e-04  7.35872132e-04  1.98125466e-01\n",
      "  2.63029316e-03 -1.92315365e-03 -1.04516323e-04  4.51347035e-01\n",
      " -1.24274659e-01  1.47679261e-03 -3.05708675e-01], \n",
      " std: [1.00356395 1.0004567  0.97228408 0.90866479 0.92650641 0.88433077\n",
      " 0.83075222 1.27120639 0.99849035 1.00164974 1.00007195 1.3884947\n",
      " 0.89900867 0.84630495 1.07531584]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "print('Test -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_test_norm, axis=0), np.max(X_test_norm, axis=0), np.mean(X_test_norm, axis=0), np.std(X_test_norm, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thousand-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(save_dir + '/X_train_' + DATA_ID + '.npy', X_train_norm)    \n",
    "np.save(save_dir + '/X_test_' + DATA_ID + '.npy', X_test_norm)    \n",
    "np.save(save_dir + '/Y_train_' + DATA_ID + '.npy', Y_train)    \n",
    "np.save(save_dir + '/Y_test_' + DATA_ID + '.npy', Y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "miniature-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train experiments ids (10): [3, 2, 6, 8, 4, 15, 5, 11, 1, 12]\n",
      "Validation experiments ids (2): [10, 9]\n",
      "Train -> X: (87089, 15), Y: (87089, 24)\n",
      "Validation -> X: (17472, 15), Y: (17472, 24)\n",
      "Train -> \n",
      " min: [-1.33454024 -1.26392381 -3.09438819 -2.27875063 -0.72045164 -2.86984257\n",
      " -4.29493493 -1.86738295 -2.55090504 -2.36461941 -4.71368547 -2.24729417\n",
      " -0.74520758 -2.4942297  -2.78618974], \n",
      " max: [1.66594967 2.11709175 2.86061392 3.13743426 2.81096549 2.17730692\n",
      " 2.50700426 4.03063161 1.64958985 3.29043326 3.68559326 7.21081788\n",
      " 2.99798456 3.85475204 0.7510239 ], \n",
      " mean: [-6.25780843e-17  8.24039963e-18  1.04432787e-17 -1.04432787e-17\n",
      "  3.19825411e-17 -5.05846314e-18  1.27277460e-17 -1.30540984e-18\n",
      " -6.58416089e-17  9.38263324e-18 -8.93389861e-18  2.61081968e-17\n",
      "  6.07015576e-17  2.81478997e-18 -5.22163937e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.30040673 -1.20929252 -2.68321425 -2.0413404  -0.72029841 -2.86149945\n",
      " -4.21930231 -1.84223306 -2.31098838 -2.36032995 -1.62778937 -1.71836615\n",
      " -0.92727745 -2.5274025  -2.76450941], \n",
      " max: [1.65269015 2.13707925 2.90400754 3.11907111 2.81271833 2.18338111\n",
      " 2.1515402  3.6066524  1.64225472 2.09782588 3.52304067 4.53490608\n",
      " 2.98330351 3.6710364  0.78649048], \n",
      " mean: [ 1.14354342e-03 -8.28738536e-04  4.76862263e-04 -5.38363971e-02\n",
      " -2.26400161e-02  8.29512833e-04  1.99532954e-04  1.04623560e-01\n",
      " -1.19460797e-03  9.97709114e-04  1.85199416e-03 -4.44029687e-02\n",
      " -8.38847689e-02 -3.80882110e-03 -2.72992492e-01], \n",
      " std: [0.99297151 0.99496843 0.98310586 1.01853836 0.95506012 0.93166476\n",
      " 0.91228289 1.19110931 0.99674793 0.99663547 0.99267141 0.75449545\n",
      " 0.95731337 0.92906083 1.09733639]\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train experiments ids (10): [3, 12, 4, 1, 5, 8, 6, 10, 11, 15]\n",
      "Validation experiments ids (2): [2, 9]\n",
      "Train -> X: (87058, 15), Y: (87058, 24)\n",
      "Validation -> X: (17503, 15), Y: (17503, 24)\n",
      "Train -> \n",
      " min: [-1.33940778 -1.26673914 -3.107456   -2.23267973 -0.71981552 -2.86365652\n",
      " -4.28738402 -1.84227757 -2.55005311 -2.36588708 -4.714463   -2.33383354\n",
      " -0.9115032  -2.51169397 -2.43770396], \n",
      " max: [1.66935655 2.14196505 2.91684031 3.11338197 2.80791321 2.17925607\n",
      " 2.50286603 4.00867627 1.65056246 3.29048727 3.68571484 7.39625996\n",
      " 2.9832621  3.83106238 0.90360604], \n",
      " mean: [ 2.19550180e-17  2.38322129e-17  1.01205288e-17  1.95881202e-17\n",
      "  7.67201373e-17  2.35465528e-17  2.65255794e-18  4.70114884e-17\n",
      "  1.12305222e-16 -1.73436481e-17 -2.03022704e-17 -2.61174936e-17\n",
      " -4.57056137e-17  9.95729442e-18 -1.04469974e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.30937779 -1.16514571 -2.69450665 -1.6336668  -0.71966253 -2.84507643\n",
      " -4.27670063 -1.75390292 -2.29363964 -2.36159661 -1.62823658 -2.27344722\n",
      " -0.72334781 -2.38769476  0.21344812], \n",
      " max: [1.66628496 2.02160951 2.52408734 3.10049954 2.7686425  2.05630412\n",
      " 2.48426636 3.58808002 1.64485737 2.08256761 3.47567669 4.64339851\n",
      " 2.8663057  3.77897559 0.87947928], \n",
      " mean: [-5.94807846e-03 -5.50408979e-04  2.23141465e-03  4.59300197e-02\n",
      " -2.53932103e-02  2.79860930e-03  1.21476794e-03  1.64570775e-01\n",
      "  4.33461627e-03 -3.27525951e-03  2.18313995e-04 -1.76518122e-01\n",
      " -3.18898643e-02 -3.14287439e-03  7.49047850e-01], \n",
      " std: [1.00940842 1.00847394 1.00894201 0.9398395  0.94555137 0.91891769\n",
      " 0.90145235 1.14140713 0.99691844 0.99803588 0.9933283  0.93233533\n",
      " 0.91122728 0.8896928  0.12882033]\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train experiments ids (10): [15, 12, 4, 6, 1, 8, 9, 2, 3, 5]\n",
      "Validation experiments ids (2): [11, 10]\n",
      "Train -> X: (87199, 15), Y: (87199, 24)\n",
      "Validation -> X: (17362, 15), Y: (17362, 24)\n",
      "Train -> \n",
      " min: [-1.32894481 -1.25890136 -3.08359793 -2.33385735 -0.72454981 -2.9379276\n",
      " -4.41605468 -1.81165168 -2.5544961  -2.36707911 -4.72578011 -2.22061824\n",
      " -0.74374663 -2.57034801 -3.45812342], \n",
      " max: [1.6637468  2.11094333 2.84994415 3.20848503 2.86296088 2.22778455\n",
      " 2.57760212 3.76540345 1.65063708 3.27536647 3.65893379 7.15755228\n",
      " 3.07929395 3.97463322 0.63751675], \n",
      " mean: [-3.73202185e-17  2.49344691e-17 -1.78859999e-17 -1.12449567e-17\n",
      " -1.25650168e-16  1.66637220e-17 -1.03282482e-17  8.27889562e-17\n",
      " -1.80000792e-16 -2.40381320e-17 -1.20394373e-17 -4.97059678e-17\n",
      " -1.62644446e-16 -1.32006013e-17  8.34408378e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.23303547 -1.26068341 -2.29847419 -2.0909172  -0.72360794 -2.92938849\n",
      " -4.3382903  -1.78787041 -2.55569109 -2.36154195 -4.68579053 -1.69659915\n",
      " -0.93013722 -2.60454489 -3.45915797], \n",
      " max: [1.63365392 2.13087532 2.8931814  3.18969416 2.86474157 2.23400143\n",
      " 2.25539183 2.31354979 1.64550074 3.29460959 3.69466091 2.78690509\n",
      " 3.0882754  3.78524609 0.66731296], \n",
      " mean: [ 1.39571563e-02 -2.45424094e-03 -1.69582312e-03 -6.76558747e-02\n",
      "  2.10927814e-02 -3.17770878e-03 -1.78237156e-04 -1.76801522e-01\n",
      " -8.69317886e-03  2.90290871e-03  5.53491668e-04  1.95405487e-03\n",
      "  2.88824729e-02  1.35495423e-03 -1.76924228e+00], \n",
      " std: [0.97701157 0.97803327 0.96083171 1.14977484 1.04960253 1.07103174\n",
      " 1.07973318 0.84928886 1.0050491  1.00367331 1.00777878 0.68853318\n",
      " 1.09845108 1.10961229 0.87101871]\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train experiments ids (10): [15, 2, 4, 9, 5, 3, 1, 11, 12, 10]\n",
      "Validation experiments ids (2): [6, 8]\n",
      "Train -> X: (87030, 15), Y: (87030, 24)\n",
      "Validation -> X: (17531, 15), Y: (17531, 24)\n",
      "Train -> \n",
      " min: [-1.33957539 -1.26656587 -2.69419196 -2.22625066 -0.71984421 -2.8718675\n",
      " -4.30434518 -1.83876067 -2.55192126 -2.36594525 -4.71621445 -2.32107488\n",
      " -0.92413535 -2.52865351 -2.43788017], \n",
      " max: [1.66863137 2.1432416  2.91655921 3.08511842 2.81456833 2.18618403\n",
      " 2.50695213 3.99375822 1.65068718 3.29119989 3.68758714 7.48228277\n",
      " 2.99132268 3.85576782 0.90461687], \n",
      " mean: [-6.35185854e-17  1.58388246e-17  2.44930278e-18 -1.17566533e-17\n",
      " -8.00105574e-17  4.69449699e-18  3.83724102e-18 -2.28601593e-16\n",
      " -1.23608147e-16  1.38385607e-17 -2.06353759e-17  1.95944222e-17\n",
      " -7.07032068e-17  1.42059561e-17 -3.86663265e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.30528561 -1.18686112 -3.10709746 -1.7403081  -0.7199975  -2.79027153\n",
      " -4.21945882 -1.66747557 -2.30306722 -2.36219686 -2.12735541 -2.01661819\n",
      " -0.70845002 -2.34660791  0.18435057], \n",
      " max: [1.66551477 2.01997888 2.49717014 2.91054033 2.75977637 2.0247063\n",
      " 2.51255567 3.47287297 1.64708811 2.07876854 3.49797957 5.54960483\n",
      " 2.85682615 3.56720454 0.87143535], \n",
      " mean: [-8.41758040e-03  2.92912905e-03  2.39818177e-03 -2.51135903e-03\n",
      " -1.81664326e-02  5.10345686e-03  4.19737718e-04  1.50145732e-01\n",
      "  4.03123746e-04 -1.69452997e-03  1.90191880e-03  3.34505902e-03\n",
      " -7.84666340e-02 -5.90477496e-03  7.52231485e-01], \n",
      " std: [1.00827982 1.01037525 1.00829775 0.89975498 0.95772183 0.93785552\n",
      " 0.92647485 1.12539214 0.99976319 0.99885584 0.99591915 0.99193011\n",
      " 0.94237792 0.93152469 0.13214991]\n",
      "\n",
      "\n",
      "Fold 5\n",
      "Train experiments ids (10): [6, 1, 9, 10, 5, 4, 15, 11, 12, 3]\n",
      "Validation experiments ids (2): [8, 2]\n",
      "Train -> X: (87054, 15), Y: (87054, 24)\n",
      "Validation -> X: (17507, 15), Y: (17507, 24)\n",
      "Train -> \n",
      " min: [-1.33933188 -1.26658922 -3.10406799 -2.23279891 -0.7199501  -2.86990908\n",
      " -4.29745108 -1.82668979 -2.54989407 -2.36546541 -4.7134346  -2.37150456\n",
      " -0.9094891  -2.51784752 -2.43672939], \n",
      " max: [1.66941263 2.1422189  2.91361451 3.1133714  2.81404323 2.18399912\n",
      " 2.50839405 3.96606427 1.65039462 3.29028076 3.68457491 7.55339912\n",
      " 2.99072945 3.84065087 0.90180968], \n",
      " mean: [-5.94200280e-17  1.50182488e-17  1.62833731e-17  2.48127589e-17\n",
      "  1.05780709e-16 -1.43652815e-17 -1.24880004e-17  6.20318973e-17\n",
      "  1.28144841e-16 -1.48550070e-17  3.67294129e-18 -7.83560809e-18\n",
      "  1.17534121e-17 -7.39179435e-18 -7.31323421e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.30930207 -1.1649927  -2.42359661 -1.74367235 -0.71970204 -2.85128847\n",
      " -4.28674315 -1.73919421 -2.29350056 -2.36171794 -1.64366993 -2.30990922\n",
      " -0.69464324 -2.39354055  0.21222411], \n",
      " max: [1.66634106 2.01899231 2.52129273 3.10048871 2.77470279 2.06077909\n",
      " 2.48975166 3.4487303  1.64468997 2.08249525 3.47459099 5.59675905\n",
      " 2.87360929 3.7884348  0.86839315], \n",
      " mean: [-5.54604456e-03  5.75760995e-04  2.08791019e-03  4.54795781e-02\n",
      " -1.86018181e-02  2.75260852e-03 -9.83678025e-05  1.46546983e-01\n",
      "  4.09796190e-03 -2.32498900e-03 -9.07835643e-04 -1.25926227e-01\n",
      " -1.22755907e-02 -2.67090934e-03  7.42002415e-01], \n",
      " std: [1.00936906 1.00865308 1.00240423 0.93999836 0.95665916 0.93265208\n",
      " 0.91611375 1.08753818 0.99645416 0.9973742  0.99177993 1.05798233\n",
      " 0.92052082 0.90567688 0.12819571]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_id in range(CV):\n",
    "    print('Fold {}'.format(fold_id + 1))\n",
    "    \n",
    "    # From the training experiment ids, a different validation set is created for each fold\n",
    "    random.shuffle(train_experiments)\n",
    "    train_experiments_fold = train_experiments[:int(N_EXPERIMENTS * SPLIT_PCT[0])]\n",
    "    valid_experiments_fold = train_experiments[-int(N_EXPERIMENTS * SPLIT_PCT[1]):]\n",
    "\n",
    "    print('Train experiments ids ({}): {}'.format(len(train_experiments_fold), train_experiments_fold))\n",
    "    print('Validation experiments ids ({}): {}'.format(len(valid_experiments_fold), valid_experiments_fold))\n",
    "\n",
    "    # Check that no validation experiments are in train\n",
    "    assert(not any([i in valid_experiments_fold for i in train_experiments_fold]))\n",
    "    # Check that no test experiments are in train or validation folds\n",
    "    assert(not any([i in test_experiments for i in train_experiments_fold]))\n",
    "    assert(not any([i in test_experiments for i in valid_experiments_fold]))\n",
    "    \n",
    "    X_train = np.concatenate([features_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    Y_train = np.concatenate([targets_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    X_valid = np.concatenate([features_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "    Y_valid = np.concatenate([targets_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "\n",
    "    print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "    print('Validation -> X: {}, Y: {}'.format(X_valid.shape, Y_valid.shape))\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_valid_norm =  scaler.transform(X_valid)\n",
    "\n",
    "    print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "    print('Valid -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}'.format(np.min(X_valid_norm, axis=0), np.max(X_valid_norm, axis=0), np.mean(X_valid_norm, axis=0), np.std(X_valid_norm, axis=0)))\n",
    "    \n",
    "    save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    np.save(save_dir + '/X_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_train_norm)    \n",
    "    np.save(save_dir + '/X_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_valid_norm)    \n",
    "    np.save(save_dir + '/Y_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_train)    \n",
    "    np.save(save_dir + '/Y_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_valid)      \n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-luxury",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
