{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "filled-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-istanbul",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternate-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Path where the data is stored\n",
    "SOURCE_PATH = '../../../data'\n",
    "# Directory inside SOURCE_PATH where the original data is stored\n",
    "ORIGINAL_DATA_DIR = '/EXOSAFE'\n",
    "# Directory inside SOURCE_PATH where the derived data is stored\n",
    "DERIVED_DATA_DIR = '/derived_data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Experiment params\n",
    "DATE_EXPERIMENTS = '24022021'\n",
    "N_EXPERIMENTS = 15\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "# ID of the training and test data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0003_11042021'\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "CV = 6\n",
    "# Experiments for training set (int)\n",
    "TRAIN_SIZE = 12\n",
    "# Experiments for test set (int)\n",
    "TEST_SIZE = 3\n",
    "\n",
    "assert(TRAIN_SIZE + TEST_SIZE == N_EXPERIMENTS)\n",
    "assert(TRAIN_SIZE % CV == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-thing",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protected-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate force vectors of each force cell to align them\n",
    "rotations = {\n",
    "    1: [180, 90, 0],\n",
    "    2: [180, 90, 0],\n",
    "    3: [180, 0, -90],\n",
    "    4: [0, 0, -90],\n",
    "    5: [0, 0, 0],\n",
    "    6: [0, 180, 0],\n",
    "    7: [0, 90, 0],\n",
    "    8: [0, 0, 90],\n",
    "}\n",
    "\n",
    "def rotate_vector(v, axis, angle):\n",
    "    '''\n",
    "    Args:\n",
    "    - v (np.array): Vector to be rotated\n",
    "    - axis (int): Axis along the rotation is performed\n",
    "    - angle (int): Rotation angle\n",
    "    \n",
    "    Returns:\n",
    "    - (np.array)): Rotated vector\n",
    "    '''\n",
    "    if axis == 0:\n",
    "        # X\n",
    "        v = v.dot(np.array([[1, 0, 0], [0, np.cos(np.radians(angle)), np.sin(np.radians(angle))], [0, np.sin(np.radians(angle)), np.cos(np.radians(angle))]]))\n",
    "    elif axis == 1:\n",
    "        # Y\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), 0, np.sin(np.radians(angle))], [0, 1, 0], [-np.sin(np.radians(angle)), 0, np.cos(np.radians(angle))]]))\n",
    "    elif axis == 2:\n",
    "        # Z\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle)), 0], [np.sin(np.radians(angle)), np.cos(np.radians(angle)), 0], [0, 0, 1]]))\n",
    "    else:\n",
    "        raise ValueError('Invalid axis')\n",
    "\n",
    "    return v\n",
    "\n",
    "@dask.delayed\n",
    "def rotate_row(row):\n",
    "    '''\n",
    "    Rotate the force vectors in a row. Dask function.\n",
    "    '''\n",
    "    for i in range(1, N_CELLS + 1):\n",
    "        cols = ['F{}x'.format(str(i)), 'F{}y'.format(str(i)), 'F{}z'.format(str(i))]\n",
    "        for ax in range(3):\n",
    "            row[cols] = rotate_vector(row[cols], ax, rotations[i][ax])\n",
    "            \n",
    "    return row\n",
    "\n",
    "def process_parameters_sheet(params_df):\n",
    "    '''\n",
    "    Process the data in the given pd.DataFrame from the raw excel sheet. \n",
    "    \n",
    "    Args:\n",
    "    - params_df (pd.DataFrame): DataFrame of the parameters excel sheet.\n",
    "    \n",
    "    Returns:\n",
    "    - params_dict (dict): Dictionary with all the parameter in the input DataFrame.\n",
    "    '''\n",
    "    params_dict = {}\n",
    "    params_dict['ExoHipMissalign'] = params_df.iloc[2, 1]\n",
    "    params_dict['ExoKneeMissalign'] = params_df.iloc[2, 2]\n",
    "    params_dict['MarchVelocity'] = params_df.iloc[0, 11]\n",
    "    params_dict['TimeShift'] = params_df.iloc[0, 12]\n",
    "    params_dict['SkinConfig'] = params_df.iloc[0, 13]\n",
    "    \n",
    "    return params_dict\n",
    "\n",
    "def shift_leg_data(df, time_shift, total_len, data_res=0.01):\n",
    "    '''\n",
    "    Shift the data from the leg replica using the known time_shift from the experiment\n",
    "    parameters to match the exoskeleton data in time and lenght.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with the data of the leg replica\n",
    "    - time_shift (float): Shifting time to applied to the data.\n",
    "    - total_len (int): Total desired lenght for the data.\n",
    "    - data_res (float): Data resolution (in seconds).\n",
    "    \n",
    "    Returns:\n",
    "    - (pd.DataFrame): DataFrame with the data of the leg replica shifted.\n",
    "    '''\n",
    "    idx_start = math.ceil(time_shift / data_res)\n",
    "    idx_end = total_len + idx_start\n",
    "    return df.iloc[idx_start:idx_end].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "independent-killing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1:\n",
      "[########################################] | 100% Completed |  1min  6.0s\n",
      "\n",
      "Processing file 2:\n",
      "[########################################] | 100% Completed |  1min  4.3s\n",
      "\n",
      "Processing file 3:\n",
      "[########################################] | 100% Completed |  1min  3.8s\n",
      "\n",
      "Processing file 4:\n",
      "[########################################] | 100% Completed |  1min  6.3s\n",
      "\n",
      "Processing file 5:\n",
      "[########################################] | 100% Completed |  1min  4.4s\n",
      "\n",
      "Processing file 6:\n",
      "[########################################] | 100% Completed |  1min 11.3s\n",
      "\n",
      "Processing file 7:\n",
      "[########################################] | 100% Completed |  1min  3.9s\n",
      "\n",
      "Processing file 8:\n",
      "[########################################] | 100% Completed |  1min 10.8s\n",
      "\n",
      "Processing file 9:\n",
      "[########################################] | 100% Completed |  1min  6.1s\n",
      "\n",
      "Processing file 10:\n",
      "[########################################] | 100% Completed |  1min  2.6s\n",
      "\n",
      "Processing file 11:\n",
      "[########################################] | 100% Completed |  1min  2.7s\n",
      "\n",
      "Processing file 12:\n",
      "[########################################] | 100% Completed |  1min  9.5s\n",
      "\n",
      "Processing file 13:\n",
      "[########################################] | 100% Completed |  1min  5.0s\n",
      "\n",
      "Processing file 14:\n",
      "[########################################] | 100% Completed |  1min  3.7s\n",
      "\n",
      "Processing file 15:\n",
      "[########################################] | 100% Completed |  1min  3.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_EXPERIMENTS):\n",
    "    print('Processing file {}:'.format(i + 1))\n",
    "    # Create the directory to save the resulting data\n",
    "    save_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i + 1))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Select only the relevant excel sheets \n",
    "    sheets = ['Parameters', 'RawForces', 'ForceCells', 'H3raw', 'H3processed', 'Leg-Replica']\n",
    "    # Load the data\n",
    "    data_df = pd.read_excel(SOURCE_PATH + ORIGINAL_DATA_DIR + '/' + DATE_EXPERIMENTS + '/0{}-'.format(i + 1) + DATE_EXPERIMENTS + '.xlsx', sheet_name=sheets)\n",
    "\n",
    "    # Pre-process the data\n",
    "    data_df[sheets[0]] = process_parameters_sheet(data_df[sheets[0]])\n",
    "    \n",
    "    # Apply the rotation matrix to each force vector\n",
    "    forces_ddf = dd.from_pandas(data_df[sheets[2]], npartitions=int(len(data_df[sheets[2]]) / 100))\n",
    "    forces_ddf = forces_ddf.apply(rotate_row, axis=1, meta=forces_ddf)\n",
    "    with ProgressBar():\n",
    "        data_df[sheets[2]] = forces_ddf.compute()\n",
    "        \n",
    "        \n",
    "    leg_df_raw = data_df[sheets[5]].iloc[:, :3]\n",
    "    # Correct the time shift between the data from the leg and the data from the exo\n",
    "    leg_df_processed = shift_leg_data(data_df[sheets[5]].iloc[:, 3:], data_df[sheets[0]]['TimeShift'], len(data_df[sheets[4]]))\n",
    "    \n",
    "    assert(len(leg_df_processed) == len(data_df[sheets[4]]) == len(data_df[sheets[2]]))    \n",
    "\n",
    "    json.dump(data_df[sheets[0]], open(save_dir + '/parameters.json', 'w'))\n",
    "    data_df[sheets[1]].to_csv(save_dir + '/force_cells_raw.csv', index=False)\n",
    "    data_df[sheets[2]].to_csv(save_dir + '/force_cells_processed.csv', index=False)\n",
    "    data_df[sheets[3]].to_csv(save_dir + '/H3_raw.csv', index=False)\n",
    "    data_df[sheets[4]].to_csv(save_dir + '/H3_processed.csv', index=False)\n",
    "    leg_df_raw.to_csv(save_dir + '/leg_raw.csv', index=False)\n",
    "    leg_df_processed.to_csv(save_dir + '/leg_processed.csv', index=False)\n",
    "    \n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-webster",
   "metadata": {},
   "source": [
    "## Features and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hearing-shadow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 15\n",
      "Selected features: ['LHipPos', 'LHipVel', 'LHipAcc', 'LHipTorque', 'LKneePos', 'LKneeVel', 'LKneeAcc', 'LKneeTorque', 'LAnklePos', 'LAnkleVel', 'LAnkleAcc', 'LAnkleTorque', 'LegKneePositionFiltered', 'LegKneeVelocityFiltered', 'LegKneeTorqueFiltered']\n",
      "\n",
      "\n",
      "Number of targets: 24\n",
      "Selected targets: ['F1x', 'F1y', 'F1z', 'F2x', 'F2y', 'F2z', 'F3x', 'F3y', 'F3z', 'F4x', 'F4y', 'F4z', 'F5x', 'F5y', 'F5z', 'F6x', 'F6y', 'F6z', 'F7x', 'F7y', 'F7z', 'F8x', 'F8y', 'F8z']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "surprising-experience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping 8722 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 1 -> X: (8721, 15), Y: (8721, 24) \n",
      "\n",
      "Droping 8736 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 2 -> X: (8735, 15), Y: (8735, 24) \n",
      "\n",
      "Droping 8589 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 3 -> X: (8588, 15), Y: (8588, 24) \n",
      "\n",
      "Droping 8726 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 4 -> X: (8725, 15), Y: (8725, 24) \n",
      "\n",
      "Droping 8624 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 5 -> X: (8623, 15), Y: (8623, 24) \n",
      "\n",
      "Droping 8760 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 6 -> X: (8759, 15), Y: (8759, 24) \n",
      "\n",
      "Droping 8639 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 7 -> X: (8638, 15), Y: (8638, 24) \n",
      "\n",
      "Droping 8773 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 8 -> X: (8772, 15), Y: (8772, 24) \n",
      "\n",
      "Droping 8769 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 9 -> X: (8768, 15), Y: (8768, 24) \n",
      "\n",
      "Droping 8705 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 10 -> X: (8704, 15), Y: (8704, 24) \n",
      "\n",
      "Droping 8659 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 11 -> X: (8658, 15), Y: (8658, 24) \n",
      "\n",
      "Droping 8793 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 12 -> X: (8792, 15), Y: (8792, 24) \n",
      "\n",
      "Droping 8599 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 13 -> X: (8598, 15), Y: (8598, 24) \n",
      "\n",
      "Droping 8750 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 14 -> X: (8749, 15), Y: (8749, 24) \n",
      "\n",
      "Droping 8717 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 15 -> X: (8716, 15), Y: (8716, 24) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i in range(1, N_EXPERIMENTS + 1):\n",
    "    # Define the path to load the data\n",
    "    data_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(data_dir + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(data_dir + '/H3_processed.csv')\n",
    "    leg_df = pd.read_csv(data_dir + '/leg_processed.csv')\n",
    "    features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    \n",
    "    idx_aux = targets_df.duplicated(keep='first')\n",
    "    targets_df = targets_df.loc[~idx_aux]\n",
    "    features_df = features_df.loc[~idx_aux]\n",
    "    print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "    \n",
    "    # Rename columns to manage with some typos\n",
    "    features_df = features_df.rename(columns={'LankleTorque': 'LAnkleTorque', 'RankleTorque': 'RAnkleTorque'})\n",
    "\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    # Store the final array\n",
    "    targets_dict[i] = targets_df[targets].values\n",
    "    features_dict[i] = features_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-aurora",
   "metadata": {},
   "source": [
    "## Normalization and split for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "educational-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (12): [2, 11, 10, 6, 12, 3, 4, 8, 9, 5, 1, 15]\n",
      "Test experiments ids (3): [13, 7, 14]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(1, N_EXPERIMENTS + 1))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[:TRAIN_SIZE]\n",
    "test_experiments = experiments[TRAIN_SIZE:]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "assert(len(train_experiments) + len(test_experiments) == N_EXPERIMENTS)\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "secondary-seattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> X: (104561, 15), Y: (104561, 24)\n",
      "Test -> X: (25985, 15), Y: (25985, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([features_dict[i] for i in train_experiments], axis=0)\n",
    "Y_train = np.concatenate([targets_dict[i] for i in train_experiments], axis=0)\n",
    "X_test = np.concatenate([features_dict[i] for i in test_experiments], axis=0)\n",
    "Y_test = np.concatenate([targets_dict[i] for i in test_experiments], axis=0)\n",
    "\n",
    "print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Test -> X: {}, Y: {}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "minus-pride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> \n",
      " min: [-1.33629602 -1.26484648 -3.10316622 -2.26223808 -0.72196185 -2.90216626\n",
      " -4.35645598 -1.82089691 -2.55209014 -2.36611435 -4.71975594 -2.32476878\n",
      " -0.91924672 -2.556158   -2.68157726], \n",
      " max: [1.66771134 2.13901227 2.91209061 3.13601047 2.83730418 2.20772623\n",
      " 2.54286126 3.87695105 1.6506851  3.29211458 3.68978761 7.49181857\n",
      " 3.03174483 3.90023529 0.81419454], \n",
      " mean: [ 6.59162072e-17  1.25036929e-17  1.53577967e-17 -2.17455529e-18\n",
      "  4.70247581e-17 -1.52898419e-18 -7.72986450e-19 -3.26183293e-17\n",
      " -2.96826797e-16 -1.01252731e-17 -9.93839721e-19 -7.82839904e-17\n",
      "  0.00000000e+00  1.16542572e-17  6.95857692e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Test -> \n",
      " min: [-1.31007256 -1.21947194 -2.39689804 -1.98521388 -0.72154655 -2.87493359\n",
      " -4.27516027 -1.81340945 -2.38972514 -2.35581996 -1.61207976 -1.77671285\n",
      " -0.79982154 -2.54003847 -2.684084  ], \n",
      " max: [ 1.6556621   2.07734193  2.74539279  3.07156386  2.83887633  2.19956122\n",
      "  2.22465618  3.90613018  1.65454986  2.09792576  3.56101584 11.30813031\n",
      "  3.02245127  3.71964724  0.77958802], \n",
      " mean: [ 2.16157527e-03  1.70658398e-03 -8.90862413e-04  3.66093570e-03\n",
      " -1.32857969e-02 -1.00024901e-03  3.47354476e-04  8.04899012e-02\n",
      "  5.92649776e-04 -1.47336199e-03 -1.86910567e-04  5.57907982e-01\n",
      " -7.28462021e-02  2.18776814e-03 -6.56859997e-01], \n",
      " std: [0.99188675 0.99029596 0.97062917 0.98235935 0.96690402 0.9477156\n",
      " 0.91826453 1.17648008 0.9993203  1.00255372 1.00280558 1.43140207\n",
      " 0.96839934 0.93785194 1.07885434]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "print('Test -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_test_norm, axis=0), np.max(X_test_norm, axis=0), np.mean(X_test_norm, axis=0), np.std(X_test_norm, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "korean-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(save_dir + '/X_train_' + DATA_ID + '.npy', X_train_norm)    \n",
    "np.save(save_dir + '/X_test_' + DATA_ID + '.npy', X_test_norm)    \n",
    "np.save(save_dir + '/Y_train_' + DATA_ID + '.npy', Y_train)    \n",
    "np.save(save_dir + '/Y_test_' + DATA_ID + '.npy', Y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exotic-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train experiments ids (10): [10, 6, 12, 3, 4, 8, 9, 5, 1, 15]\n",
      "Validation experiments ids (2): [2, 11]\n",
      "Train -> X: (87168, 15), Y: (87168, 24)\n",
      "Validation -> X: (17393, 15), Y: (17393, 24)\n",
      "Train -> \n",
      " min: [-1.33376324 -1.26168489 -3.0965032  -2.28530382 -0.72387903 -2.93134342\n",
      " -4.40791313 -1.78827813 -2.55364304 -2.36834823 -4.7265658  -2.30455731\n",
      " -0.91397124 -2.58740864 -2.86139649], \n",
      " max: [1.66711752 2.13571172 2.90585559 3.1819199  2.85972302 2.22958845\n",
      " 2.5731265  3.74645433 1.65160838 3.27541859 3.65905707 7.33652805\n",
      " 3.06356752 3.94876011 0.76971848], \n",
      " mean: [-9.62682373e-17  2.28239682e-18  1.41019518e-17  1.82591746e-17\n",
      " -3.30947539e-17 -1.77293324e-17 -7.73365708e-18  3.26056689e-17\n",
      "  4.05940577e-17 -2.51878792e-17  1.01892715e-18  1.30422675e-16\n",
      " -8.54268524e-17 -1.05968424e-18  1.04338140e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.30381192 -1.26347094 -2.38269004 -1.98164092 -0.72338675 -2.9123285\n",
      " -4.39692956 -1.75678625 -2.55483806 -2.36280977 -4.68657188 -2.24472338\n",
      " -0.68297177 -2.53972945 -2.86230708], \n",
      " max: [1.66405397 1.99308539 2.56279334 3.16874551 2.85768471 2.20547098\n",
      " 2.55400422 2.30561837 1.64647189 3.29466622 3.69478807 2.84335077\n",
      " 3.07247753 3.89508502 0.69097927], \n",
      " mean: [ 6.86504771e-03 -2.17658974e-03  5.49661655e-05  3.51058206e-02\n",
      "  1.81500820e-02 -1.13811469e-03  8.72020317e-04 -1.17282097e-01\n",
      " -3.11779709e-03 -1.39897664e-03 -1.09087291e-03 -1.28355111e-01\n",
      "  8.13080449e-02  2.00399561e-03 -4.58153664e-01], \n",
      " std: [0.99370679 0.99171074 0.98703948 1.0740485  1.04029538 1.05862741\n",
      " 1.06923303 0.80576719 1.00523584 1.00507291 1.0084128  0.87933632\n",
      " 1.05017781 1.0721359  1.14139164]\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train experiments ids (10): [2, 11, 12, 3, 4, 8, 9, 5, 1, 15]\n",
      "Validation experiments ids (2): [10, 6]\n",
      "Train -> X: (87098, 15), Y: (87098, 24)\n",
      "Validation -> X: (17463, 15), Y: (17463, 24)\n",
      "Train -> \n",
      " min: [-1.33431623 -1.26392547 -2.69104286 -2.28417644 -0.72393455 -2.93205516\n",
      " -4.41238472 -1.79746235 -2.55196626 -2.3658192  -4.7177903  -2.26396447\n",
      " -0.74339313 -2.56186312 -2.78377356], \n",
      " max: [1.66615473 2.11677611 2.8691105  3.15107858 2.85748838 2.22465076\n",
      " 2.57013805 3.76311857 1.65036927 3.2917771  3.68882937 7.24852625\n",
      " 3.06776154 3.95953893 0.75404716], \n",
      " mean: [ 3.18160769e-17 -3.06739614e-17  1.51330315e-17 -6.52637476e-18\n",
      "  4.37267109e-17 -6.03689665e-18 -7.99480908e-18 -1.31832770e-16\n",
      "  2.36581085e-17  1.55409299e-17  2.44739053e-19  3.65476986e-17\n",
      "  3.91582485e-18 -7.91322939e-18 -2.08843992e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.29992893 -1.20929926 -3.10342759 -2.0459303  -0.72408996 -2.92353094\n",
      " -4.33468028 -1.77375133 -2.31194448 -2.35705379 -2.12806313 -1.96854105\n",
      " -0.92876873 -2.5959368  -2.76214863], \n",
      " max: [1.66304614 2.13676176 2.91263191 3.13265078 2.85926612 2.2308568\n",
      " 2.57588245 2.28394942 1.64677045 2.09863328 3.52613471 2.84643187\n",
      " 3.05281394 3.77083409 0.77977867], \n",
      " mean: [ 2.43493781e-03 -1.54184558e-03  9.14357655e-04 -3.85006837e-02\n",
      "  1.65156404e-02  1.33846040e-03  1.42007361e-03 -1.22342475e-01\n",
      " -8.56997701e-04  1.83209426e-04  1.94383041e-03 -6.71326378e-02\n",
      "  6.42546203e-03 -3.29891709e-03 -3.00591167e-01], \n",
      " std: [0.99292794 0.99440709 1.00079837 1.0397786  1.03975863 1.06061087\n",
      " 1.07488789 0.83816078 0.9993731  0.99933035 0.99791482 0.79437245\n",
      " 1.06713885 1.08865815 1.07665018]\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train experiments ids (10): [2, 11, 10, 6, 4, 8, 9, 5, 1, 15]\n",
      "Validation experiments ids (2): [12, 3]\n",
      "Train -> X: (87181, 15), Y: (87181, 24)\n",
      "Validation -> X: (17380, 15), Y: (17380, 24)\n",
      "Train -> \n",
      " min: [-1.33339544 -1.26231635 -3.0988775  -2.14802387 -0.72327949 -2.93723848\n",
      " -4.42398674 -1.77270927 -2.55443937 -2.36460593 -4.68735739 -2.35075593\n",
      " -0.91376195 -2.5943826  -2.86790649], \n",
      " max: [1.66541024 2.13777834 2.90787642 3.17160597 2.86401187 2.23410868\n",
      " 2.58167941 3.75819873 1.65098594 3.29505758 3.69563329 7.63414519\n",
      " 3.08049101 3.95903176 0.76773389], \n",
      " mean: [-8.28060494e-17  1.33663308e-17  2.07830144e-18  3.84689521e-17\n",
      " -1.09212703e-17 -1.40998490e-17 -6.78504293e-18 -8.08500010e-17\n",
      "  1.01062501e-17 -2.55916334e-17  9.92287059e-18  3.12967746e-17\n",
      " -1.23231050e-16  2.77106858e-18 -1.04322582e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.29814835 -1.19965836 -2.393187   -2.28668742 -0.72259012 -2.91928991\n",
      " -4.39324361 -1.79639476 -2.55324464 -2.36835848 -4.72735909 -2.28521556\n",
      " -0.67636641 -2.56031963 -2.86699499], \n",
      " max: [1.66695659 2.04530318 2.51316506 3.18479062 2.86223217 2.2278925\n",
      " 2.57592071 2.4259002  1.64512625 3.27580859 3.65989535 6.53826631\n",
      " 3.07156356 3.69904191 0.68319669], \n",
      " mean: [ 7.66791955e-03  6.80690657e-03 -5.88299834e-04  3.75340164e-02\n",
      "  2.62727197e-02 -1.01674043e-03 -2.27747306e-03 -1.28015494e-01\n",
      " -4.45199874e-03 -4.51313218e-04 -3.31813129e-04  8.34622731e-02\n",
      "  9.35518159e-02  1.12575650e-03 -4.75501169e-01], \n",
      " std: [0.99263224 0.99330917 0.99146525 1.07849715 1.04620216 1.07031653\n",
      " 1.0894478  0.82923077 1.0037793  1.00550258 1.00958299 1.09647856\n",
      " 1.06074136 1.08727594 1.14128663]\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train experiments ids (10): [2, 11, 10, 6, 12, 3, 9, 5, 1, 15]\n",
      "Validation experiments ids (2): [4, 8]\n",
      "Train -> X: (87064, 15), Y: (87064, 24)\n",
      "Validation -> X: (17497, 15), Y: (17497, 24)\n",
      "Train -> \n",
      " min: [-1.32888925 -1.2670875  -3.1064329  -2.23361759 -0.72016246 -2.86231811\n",
      " -4.3019557  -1.83709813 -2.55018111 -2.36405692 -4.71328596 -2.34045324\n",
      " -0.92202952 -2.52381872 -2.43855408], \n",
      " max: [1.66922806 2.14261835 2.91530704 3.08105789 2.81337846 2.18420715\n",
      " 2.51082664 3.98633869 1.65033369 3.28960167 3.68450841 7.49419662\n",
      " 2.9886729  3.85000703 0.90486159], \n",
      " mean: [-3.26446171e-19 -1.95867703e-18 -1.05278890e-17 -1.04462775e-17\n",
      " -1.14256160e-16 -1.74240644e-17 -5.34555605e-18 -4.96198180e-17\n",
      " -6.47995649e-17 -1.15480333e-17 -4.69266371e-19 -2.61156937e-17\n",
      " -1.12297483e-16 -8.16115427e-20 -1.14909052e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.33955335 -1.26529592 -2.4255027  -1.74737252 -0.71991443 -2.87066395\n",
      " -4.26147706 -1.69134781 -2.30004519 -2.36031083 -1.64359993 -2.03896658\n",
      " -0.70660615 -2.36024405  0.1900128 ], \n",
      " max: [1.66767736 2.1225797  2.87174454 2.82315299 2.79409616 2.04428292\n",
      " 2.44086456 3.46626454 1.64403353 2.08247231 3.48130156 5.55534959\n",
      " 2.85679394 3.69168498 0.87139625], \n",
      " mean: [-6.77463964e-03 -4.08074402e-04  4.65193497e-04 -3.82602623e-02\n",
      " -2.04299215e-02  1.51123417e-03 -8.40822069e-04  1.43028081e-01\n",
      "  3.20548321e-03  8.73942656e-04 -7.41658091e-04 -6.81668280e-02\n",
      " -7.26468859e-02 -2.09234381e-03  7.53877051e-01], \n",
      " std: [1.00944101 1.01022189 1.00642349 0.9028733  0.95581417 0.93380814\n",
      " 0.92251434 1.11772369 0.99677727 0.99516297 0.99162136 1.00903048\n",
      " 0.9350674  0.9209382  0.13124526]\n",
      "\n",
      "\n",
      "Fold 5\n",
      "Train experiments ids (10): [2, 11, 10, 6, 12, 3, 4, 8, 1, 15]\n",
      "Validation experiments ids (2): [9, 5]\n",
      "Train -> X: (87170, 15), Y: (87170, 24)\n",
      "Validation -> X: (17391, 15), Y: (17391, 24)\n",
      "Train -> \n",
      " min: [-1.33881954 -1.26664904 -3.11119362 -2.22787302 -0.71967384 -2.86659381\n",
      " -4.29501624 -1.85383525 -2.55152141 -2.36557098 -4.71631234 -2.3342897\n",
      " -0.925137   -2.52108444 -2.44092797], \n",
      " max: [1.66992708 2.14328398 2.91954761 3.09082944 2.80903859 2.18089873\n",
      " 2.5070603  4.03645101 1.64971678 3.29101541 3.68767338 7.51849279\n",
      " 2.98515473 3.84657723 0.90571852], \n",
      " mean: [-3.34200438e-18  6.03191034e-18 -1.80142187e-17 -4.56468891e-18\n",
      "  2.80402319e-17  1.71990957e-17  3.36238245e-19 -1.30419683e-16\n",
      " -1.88293417e-16  2.28234445e-18  5.95039804e-18 -5.21678732e-18\n",
      " -1.04335746e-17  1.26751629e-17  1.66937194e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.3050175  -1.25310423 -2.69780249 -1.61849065 -0.71952081 -2.78473524\n",
      " -4.1612198  -1.67210126 -2.3023799  -2.36128035 -1.65886854 -2.14929766\n",
      " -0.73623154 -2.30817881  0.17200731], \n",
      " max: [1.66501201 2.02288505 2.49892692 3.0019919  2.74948932 2.0121151\n",
      " 2.45090737 3.61302735 1.6454863  2.08258885 3.49456922 4.73091988\n",
      " 2.84652614 3.59951167 0.88155323], \n",
      " mean: [-2.49701599e-03  2.73314405e-03 -2.36116773e-04  6.19134525e-03\n",
      " -2.35052612e-02  7.95838257e-04  2.33029655e-04  1.71715313e-01\n",
      " -2.19227236e-03 -8.62733740e-04  1.95002356e-03 -5.70672637e-03\n",
      " -9.23474695e-02 -3.38329400e-04  7.59099964e-01], \n",
      " std: [1.00944565 1.01067864 1.0153786  0.90779003 0.94697376 0.92415094\n",
      " 0.911984   1.17830284 0.99779713 0.99825308 0.99601826 1.02195471\n",
      " 0.93255545 0.91426313 0.13262628]\n",
      "\n",
      "\n",
      "Fold 6\n",
      "Train experiments ids (10): [2, 11, 10, 6, 12, 3, 4, 8, 9, 5]\n",
      "Validation experiments ids (2): [1, 15]\n",
      "Train -> X: (87124, 15), Y: (87124, 24)\n",
      "Validation -> X: (17437, 15), Y: (17437, 24)\n",
      "Train -> \n",
      " min: [-1.33796013 -1.26565276 -3.10263391 -2.25863199 -0.7208974  -2.87806828\n",
      " -4.2935091  -1.86384813 -2.54960286 -2.36453843 -4.71726394 -2.30098397\n",
      " -0.91246334 -2.51878958 -2.81813856], \n",
      " max: [1.66690599 2.13795689 2.91139275 3.13079485 2.81993666 2.1889474\n",
      " 2.51253373 3.5845251  1.64750379 3.2905777  3.68732517 6.64608538\n",
      " 3.00059182 3.7920594  0.76859179], \n",
      " mean: [ 6.63044906e-17 -2.62608192e-17  1.62295125e-17  2.08781668e-17\n",
      "  3.22959143e-17  2.78103706e-17 -4.75059850e-18  8.48175526e-17\n",
      " -2.61670305e-16 -1.45168503e-17  1.80135355e-17 -1.04390834e-16\n",
      " -1.04390834e-17  6.80987081e-18 -5.21954170e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.3273099  -1.15154076 -2.42522255 -2.12204786 -0.72021387 -2.83726739\n",
      " -4.30423421 -1.78349058 -2.31129285 -2.33981078 -1.6288206  -2.36379419\n",
      " -0.73095967 -2.25213088 -2.78572901], \n",
      " max: [1.66495677 1.98060712 2.60427175 3.01933983 2.74673431 1.9661876\n",
      " 2.41138025 4.00651671 1.65110122 2.11944642 3.5385879  7.7568641\n",
      " 2.81547268 3.84431328 0.68966264], \n",
      " mean: [-0.00768809 -0.00538993 -0.00061354 -0.00054455 -0.01603639 -0.00152266\n",
      "  0.0005842   0.07304384  0.00737323  0.00164445 -0.0017307   0.1977935\n",
      " -0.01225078  0.00260067 -0.40049288], \n",
      " std: [1.00168843 0.99954897 0.99877288 0.9901601  0.96842126 0.94857139\n",
      " 0.92593933 1.16797701 0.99701853 0.99669543 0.99646072 1.15958667\n",
      " 0.94089005 0.90992071 1.08741264]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the experiments of the training sets in different folds\n",
    "exp_per_fold = len(train_experiments) // CV\n",
    "cv_folds = [train_experiments[x:x+exp_per_fold] for x in range(0, len(train_experiments), exp_per_fold)]\n",
    "print('CV folds {}\\n'.format(cv_folds))\n",
    "\n",
    "for fold_id in range(CV):\n",
    "    print('Fold {}'.format(fold_id + 1))\n",
    "    \n",
    "    cv_folds_cp = cv_folds.copy()\n",
    "    valid_experiments_fold = cv_folds_cp.pop(fold_id)\n",
    "    train_experiments_fold = [item for sublist in cv_folds_cp for item in sublist]\n",
    "\n",
    "    print('Train experiments ids ({}): {}'.format(len(train_experiments_fold), train_experiments_fold))\n",
    "    print('Validation experiments ids ({}): {}'.format(len(valid_experiments_fold), valid_experiments_fold))\n",
    "\n",
    "    assert(len(train_experiments_fold) + len(valid_experiments_fold) == len(train_experiments))\n",
    "    # Check that no validation experiments are in train\n",
    "    assert(not any([i in valid_experiments_fold for i in train_experiments_fold]))\n",
    "    # Check that no test experiments are in train or validation folds\n",
    "    assert(not any([i in test_experiments for i in train_experiments_fold]))\n",
    "    assert(not any([i in test_experiments for i in valid_experiments_fold]))\n",
    "    \n",
    "    X_train = np.concatenate([features_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    Y_train = np.concatenate([targets_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    X_valid = np.concatenate([features_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "    Y_valid = np.concatenate([targets_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "\n",
    "    print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "    print('Validation -> X: {}, Y: {}'.format(X_valid.shape, Y_valid.shape))\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_valid_norm =  scaler.transform(X_valid)\n",
    "\n",
    "    print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "    print('Valid -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}'.format(np.min(X_valid_norm, axis=0), np.max(X_valid_norm, axis=0), np.mean(X_valid_norm, axis=0), np.std(X_valid_norm, axis=0)))\n",
    "    \n",
    "    save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    np.save(save_dir + '/X_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_train_norm)    \n",
    "    np.save(save_dir + '/X_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_valid_norm)    \n",
    "    np.save(save_dir + '/Y_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_train)    \n",
    "    np.save(save_dir + '/Y_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_valid)      \n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-analysis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
