{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "earlier-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.signal import savgol_filter\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-jordan",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "operational-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Directory where the original data is stored\n",
    "ORIGINAL_DATA_DIR = '../../../../EXOSAFE-DATA'\n",
    "# Directory where the derived data is stored\n",
    "DERIVED_DATA_DIR = '../../../data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-straight",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chronic-lover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found (101):\n",
      "['01-06022021.xlsx', '02-06022021.xlsx', '03-06022021.xlsx', '07-06022021.xlsx', '08-06022021.xlsx', '10-06022021.xlsx', '03-08022021.xlsx', '04-08022021.xlsx', '02-10022021.xlsx', '03-10022021.xlsx', '04-10022021.xlsx', '05-10022021.xlsx', '01-10032021.xlsx', '01-12022021.xlsx', '02-12022021.xlsx', '03-12022021.xlsx', '04-12022021.xlsx', '05-12022021.xlsx', '06-12022021.xlsx', '07-12022021.xlsx', '02-15022021.xlsx', '03-15022021.xlsx', '04-15022021.xlsx', '01-16022021.xlsx', '02-16022021.xlsx', '03-16022021.xlsx', '04-16022021.xlsx', '05-16022021.xlsx', '06-16022021.xlsx', '02-17022021.xlsx', '03-17022021.xlsx', '04-17022021.xlsx', '01-19022021.xlsx', '010-19022021.xlsx', '011-19022021.xlsx', '012-19022021.xlsx', '013-19022021.xlsx', '014-19022021.xlsx', '015-19022021.xlsx', '016-19022021.xlsx', '017-19022021.xlsx', '018-19022021.xlsx', '02-19022021.xlsx', '03-19022021.xlsx', '04-19022021.xlsx', '05-19022021.xlsx', '06-19022021.xlsx', '07-19022021.xlsx', '08-19022021.xlsx', '09-19022021.xlsx', '02-21042021.xlsx', '03-21042021.xlsx', '04-21042021.xlsx', '05-21042021.xlsx', '06-21042021.xlsx', '07-21042021.xlsx', '08-21042021.xlsx', '01-22022021.xlsx', '010-22022021.xlsx', '011-22022021.xlsx', '012-22022021.xlsx', '013-22022021.xlsx', '014-22022021.xlsx', '015-22022021.xlsx', '016-22022021.xlsx', '017-22022021.xlsx', '018-22022021.xlsx', '019-22022021.xlsx', '02-22022021.xlsx', '03-22022021.xlsx', '04-22022021.xlsx', '05-22022021.xlsx', '06-22022021.xlsx', '07-22022021.xlsx', '08-22022021.xlsx', '09-22022021.xlsx', '01-24022021.xlsx', '010-24022021.xlsx', '011-24022021.xlsx', '012-24022021.xlsx', '013-24022021.xlsx', '014-24022021.xlsx', '015-24022021.xlsx', '016-24022021.xlsx', '02-24022021.xlsx', '03-24022021.xlsx', '04-24022021.xlsx', '05-24022021.xlsx', '06-24022021.xlsx', '07-24022021.xlsx', '08-24022021.xlsx', '09-24022021.xlsx', '01-26032021.xlsx', '011-26032021.xlsx', '012-26032021.xlsx', '02-26032021.xlsx', '03-26032021.xlsx', '04-26032021.xlsx', '06-26032021.xlsx', '07-26032021.xlsx', '09-26032021.xlsx']\n"
     ]
    }
   ],
   "source": [
    "data_ls = glob.glob(ORIGINAL_DATA_DIR + '/*/*.xlsx')\n",
    "\n",
    "print('Files found ({}):'.format(len(data_ls)))\n",
    "print([file.split('/')[-1] for file in data_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tropical-mechanics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process (63):\n",
      "['01-10032021.xlsx', '01-16022021.xlsx', '02-16022021.xlsx', '03-16022021.xlsx', '04-16022021.xlsx', '05-16022021.xlsx', '06-16022021.xlsx', '02-17022021.xlsx', '03-17022021.xlsx', '04-17022021.xlsx', '01-19022021.xlsx', '010-19022021.xlsx', '011-19022021.xlsx', '012-19022021.xlsx', '013-19022021.xlsx', '014-19022021.xlsx', '015-19022021.xlsx', '016-19022021.xlsx', '017-19022021.xlsx', '018-19022021.xlsx', '02-19022021.xlsx', '03-19022021.xlsx', '04-19022021.xlsx', '05-19022021.xlsx', '06-19022021.xlsx', '07-19022021.xlsx', '08-19022021.xlsx', '09-19022021.xlsx', '01-22022021.xlsx', '010-22022021.xlsx', '011-22022021.xlsx', '012-22022021.xlsx', '013-22022021.xlsx', '014-22022021.xlsx', '015-22022021.xlsx', '016-22022021.xlsx', '017-22022021.xlsx', '018-22022021.xlsx', '019-22022021.xlsx', '02-22022021.xlsx', '03-22022021.xlsx', '04-22022021.xlsx', '05-22022021.xlsx', '06-22022021.xlsx', '07-22022021.xlsx', '08-22022021.xlsx', '09-22022021.xlsx', '01-24022021.xlsx', '010-24022021.xlsx', '011-24022021.xlsx', '012-24022021.xlsx', '013-24022021.xlsx', '014-24022021.xlsx', '015-24022021.xlsx', '016-24022021.xlsx', '02-24022021.xlsx', '03-24022021.xlsx', '04-24022021.xlsx', '05-24022021.xlsx', '06-24022021.xlsx', '07-24022021.xlsx', '08-24022021.xlsx', '09-24022021.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to exclude specific experiments (date: [experiment ids])\n",
    "# These experiments are excluded by lack of data\n",
    "EXPERIMENTS_TO_EXCLUDE = {\n",
    "    '06022021': ['01', '02', '03', '07', '08', '10'],\n",
    "    '08022021': ['03', '04'],\n",
    "    '10022021': ['02', '03', '04', '05'],\n",
    "    '12022021': ['01', '02', '03', '04', '05', '06', '07'],\n",
    "    '15022021': ['02', '03', '04'],\n",
    "    '26032021': ['01', '02', '03', '04', '06', '07', '09', '011', '012'],\n",
    "    '21042021': ['02', '03', '04', '05', '06', '07', '08']\n",
    "}\n",
    "\n",
    "# Exclude some experiments from the list of files to process\n",
    "for exp_date, exp_ids in EXPERIMENTS_TO_EXCLUDE.items():\n",
    "    for i in exp_ids:\n",
    "        data_ls.remove(ORIGINAL_DATA_DIR + '/{}/{}-{}.xlsx'.format(exp_date, i, exp_date))\n",
    "        \n",
    "print('Files to process ({}):'.format(len(data_ls)))\n",
    "print([file.split('/')[-1] for file in data_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "combined-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate force vectors of each force cell to align them\n",
    "rotations = {\n",
    "    1: [180, 90, 0],\n",
    "    2: [180, 90, 0],\n",
    "    3: [180, 0, -90],\n",
    "    4: [0, 0, -90],\n",
    "    5: [0, 0, 0],\n",
    "    6: [0, 180, 0],\n",
    "    7: [0, 90, 0],\n",
    "    8: [0, 0, 90],\n",
    "}\n",
    "\n",
    "def rotate_vector(v, axis, angle):\n",
    "    '''\n",
    "    Args:\n",
    "    - v (np.array): Vector to be rotated\n",
    "    - axis (int): Axis along the rotation is performed\n",
    "    - angle (int): Rotation angle\n",
    "    \n",
    "    Returns:\n",
    "    - (np.array)): Rotated vector\n",
    "    '''\n",
    "    if axis == 0:\n",
    "        # X\n",
    "        v = v.dot(np.array([[1, 0, 0], [0, np.cos(np.radians(angle)), np.sin(np.radians(angle))], [0, np.sin(np.radians(angle)), np.cos(np.radians(angle))]]))\n",
    "    elif axis == 1:\n",
    "        # Y\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), 0, np.sin(np.radians(angle))], [0, 1, 0], [-np.sin(np.radians(angle)), 0, np.cos(np.radians(angle))]]))\n",
    "    elif axis == 2:\n",
    "        # Z\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle)), 0], [np.sin(np.radians(angle)), np.cos(np.radians(angle)), 0], [0, 0, 1]]))\n",
    "    else:\n",
    "        raise ValueError('Invalid axis')\n",
    "\n",
    "    return v\n",
    "\n",
    "@dask.delayed\n",
    "def rotate_row(row):\n",
    "    '''\n",
    "    Rotate the force vectors in a row. Dask function.\n",
    "    '''\n",
    "    for i in range(1, N_CELLS + 1):\n",
    "        cols = ['F{}x'.format(str(i)), 'F{}y'.format(str(i)), 'F{}z'.format(str(i))]\n",
    "        for ax in range(3):\n",
    "            row[cols] = rotate_vector(row[cols], ax, rotations[i][ax])\n",
    "            \n",
    "    return row\n",
    "\n",
    "def shift_leg_data(df, time_shift, total_len, data_res=0.01):\n",
    "    '''\n",
    "    Shift the data from the leg replica using the known time_shift from the experiment\n",
    "    parameters to match the exoskeleton data in time and lenght.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with the data of the leg replica\n",
    "    - time_shift (float): Shifting time to applied to the data.\n",
    "    - total_len (int): Total desired lenght for the data.\n",
    "    - data_res (float): Data resolution (in seconds).\n",
    "    \n",
    "    Returns:\n",
    "    - (pd.DataFrame): DataFrame with the data of the leg replica shifted.\n",
    "    '''\n",
    "    idx_start = math.ceil(time_shift / data_res)\n",
    "    idx_end = total_len + idx_start\n",
    "    return df.iloc[idx_start:idx_end].reset_index(drop=True)\n",
    "\n",
    "def compute_time_shift(exo_knee_pos, leg_knee_pos, data_res=0.01, window_size=51, pol_order=3, threshold=30, max_time_shift=10):\n",
    "    '''\n",
    "    Find the time shift between the exo knee position and the robotic leg knee position based on the local maximas\n",
    "    \n",
    "    Args:\n",
    "    - exo_knee_pos (np.array): Exoeskeleton knee position signal.\n",
    "    - leg_knee_pos (np.array): Robotic leg knee position signal.\n",
    "    - data_res (float) [default = 0.01]: Time resolution of the data (0.01 = 100Hz)\n",
    "    - window_size (int) [default = 51]: Window size to smooth the signals with savgol filter\n",
    "    - pol_order (int) [default = 3]: Polinomial degree to smooth the signals with savgol filter\n",
    "    - threshold (int) [default = 30]: Threshold to filter the 0 cross of the derivates of the signals\n",
    "    - max_time_shift (int) [default = 10]: Max value of seconds to generate the cost function and find the minumum\n",
    "    \n",
    "    Return:\n",
    "    - time_shift (float): Time shift between the exoeskeleton and robotic leg signals\n",
    "    '''\n",
    "    # Smooth the exo and leg position signal\n",
    "    exo_arr_smooth = savgol_filter(exo_knee_pos, window_size, pol_order) # window size 51, polynomial order 3\n",
    "    leg_arr_smooth = savgol_filter(leg_knee_pos, window_size, pol_order) \n",
    "    \n",
    "    # Compute derivative of the signal to find de local maxima\n",
    "    exo_dev = np.gradient(exo_arr_smooth, data_res)\n",
    "    exo_dev_smooth = savgol_filter(exo_dev, window_size, pol_order) \n",
    "    leg_dev = np.gradient(leg_arr_smooth, data_res)\n",
    "    leg_dev_smooth = savgol_filter(leg_dev, window_size, pol_order)\n",
    "    \n",
    "    # Find the x coordinate of the maximum points\n",
    "    exo_idx_max = []\n",
    "    searching = False\n",
    "    for i in range(1, exo_dev_smooth.shape[0]):\n",
    "        # Search for a point where the gradient is decreasing before cross 0\n",
    "        if exo_dev_smooth[i - 1] > threshold and exo_dev_smooth[i] < threshold:\n",
    "            searching = True\n",
    "        # Only if the gradient is decreasing from a point higher than threshold,\n",
    "        # then search for a 0 crossing\n",
    "        if searching and exo_dev_smooth[i - 1] > 0 and exo_dev_smooth[i] < 0:\n",
    "            searching = False\n",
    "            exo_idx_max.append(i)\n",
    "\n",
    "    leg_idx_max = []\n",
    "    searching = False\n",
    "    for i in range(1, leg_dev_smooth.shape[0]):\n",
    "        # Search for a point where the gradient is decreasing before cross 0\n",
    "        if leg_dev_smooth[i - 1] > threshold and leg_dev_smooth[i] < threshold:\n",
    "            searching = True\n",
    "        # Only if the gradient is decreasing from a point higher than threshold,\n",
    "        # then search for a 0 crossing\n",
    "        if searching and leg_dev_smooth[i - 1] > 0 and leg_dev_smooth[i] < 0:\n",
    "            searching = False\n",
    "            leg_idx_max.append(i)\n",
    "\n",
    "    # Compute the location of the max points in time scale\n",
    "    exo_maxs = np.array(np.arange(0, len(exo_arr) * data_res, data_res)[exo_idx_max])\n",
    "    leg_maxs = np.array(np.arange(0, len(leg_arr) * data_res, data_res)[leg_idx_max])\n",
    "\n",
    "    min_aux = np.min([len(exo_maxs), len(leg_maxs)])\n",
    "\n",
    "    exo_maxs = exo_maxs[:min_aux]\n",
    "    leg_maxs = leg_maxs[:min_aux]\n",
    "    \n",
    "    time_shift_ls = np.arange(0, max_time_shift, data_res)\n",
    "\n",
    "    MAE_ls = []\n",
    "    for ts in time_shift_ls:\n",
    "        MAE_ls.append(mean_absolute_error(exo_maxs, leg_maxs - ts))\n",
    "\n",
    "    time_shift = time_shift_ls[np.argmin(MAE_ls)]\n",
    "    \n",
    "    return time_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "african-eugene",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [2:31:02<00:00, 143.85s/it]  \n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(data_ls):\n",
    "    # print('Processing file {}'.format(file))\n",
    "    exp_aux_ls = file.split('/')[-1].split('-')\n",
    "    exp_date = exp_aux_ls[-1][:8]\n",
    "    exp_id = int(exp_aux_ls[0])\n",
    "\n",
    "    # Create the directory to save the resulting data\n",
    "    save_dir = os.path.join(DERIVED_DATA_DIR, exp_date, str(exp_id))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    # Load the data\n",
    "    data_dict = pd.read_excel(file, sheet_name=None)\n",
    "    \n",
    "    # Extract the time shift between the exo and the robotic leg for the experiment\n",
    "    exo_arr = data_dict['H3processed'].iloc[:, 1].values\n",
    "    leg_arr = data_dict['Leg-Replica'].iloc[:, 3].values\n",
    "    \n",
    "    time_shift = compute_time_shift(exo_arr, leg_arr)\n",
    "    assert(not np.isnan(time_shift))\n",
    "    \n",
    "    # Fix typos\n",
    "    data_dict['H3processed'].columns = (['L{}Pos'.format(a) for a in ['Hip', 'Knee', 'Ankle']] + list(data_dict['H3processed'].columns))[:-3]\n",
    "    data_dict['H3processed'] = data_dict['H3processed'].rename(columns={'LankleTorque': 'LAnkleTorque', 'RankleTorque': 'RAnkleTorque'})\n",
    "    \n",
    "    # Extract the necessary data from the xlsx\n",
    "    force_cols = ['F{}{}'.format(i + 1, ax) for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "    leg_cols = ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "    h3_cols = ['L{}{}'.format(a, m) for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']]\n",
    "    forces_df = data_dict['ForceCells'][force_cols]\n",
    "    leg_df = data_dict['Leg-Replica'][leg_cols]\n",
    "    h3_df = data_dict['H3processed'][h3_cols]\n",
    "    \n",
    "    # Apply the rotation matrix to each force vector\n",
    "    forces_ddf = dd.from_pandas(forces_df, npartitions=int(len(forces_df) / 100))\n",
    "    forces_ddf = forces_ddf.apply(rotate_row, axis=1, meta=forces_ddf)\n",
    "    forces_df = forces_ddf.compute()\n",
    "    # with ProgressBar():\n",
    "    #     forces_df = forces_ddf.compute()\n",
    "\n",
    "    # Correct the time shift between the data from the leg and the data from the exo\n",
    "    forces_df = shift_leg_data(forces_df, time_shift, len(h3_df))\n",
    "    leg_df = shift_leg_data(leg_df, time_shift, len(h3_df))\n",
    "    \n",
    "    # Cut data of different sources to get the same lenght\n",
    "    max_available_data = min([len(forces_df), len(h3_df), len(leg_df)])\n",
    "    forces_df = forces_df.iloc[:max_available_data]\n",
    "    leg_df = leg_df.iloc[:max_available_data]\n",
    "    h3_df = h3_df.iloc[:max_available_data]\n",
    "    assert(len(forces_df) == len(h3_df) == len(leg_df))\n",
    "    # print('Total data points: {}'.format(len(forces_df)))\n",
    "\n",
    "    forces_df.to_csv(save_dir + '/force_cells_processed.csv', index=False)\n",
    "    h3_df.to_csv(save_dir + '/H3_processed.csv', index=False)\n",
    "    leg_df.to_csv(save_dir + '/leg_processed.csv', index=False)\n",
    "    \n",
    "    # print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-isaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
