{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "norman-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-liechtenstein",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latin-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Path where the data is stored\n",
    "SOURCE_PATH = '../../../data'\n",
    "# Directory inside SOURCE_PATH where the original data is stored\n",
    "ORIGINAL_DATA_DIR = '/EXOSAFE'\n",
    "# Directory inside SOURCE_PATH where the derived data is stored\n",
    "DERIVED_DATA_DIR = '/derived_data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Experiment params\n",
    "DATE_EXPERIMENTS = '24022021'\n",
    "N_EXPERIMENTS = 15\n",
    "\n",
    "\n",
    "# ID fo the training and test data resulting from this notebook, stored in DERIVED_DATA_DIR\n",
    "RESULTS_ID = '0003_08042021'\n",
    "\n",
    "# % of the data for the test set\n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-millennium",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continuing-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate force vectors of each force cell to align them\n",
    "rotations = {\n",
    "    1: [180, 90, 0],\n",
    "    2: [180, 90, 0],\n",
    "    3: [180, 0, -90],\n",
    "    4: [0, 0, -90],\n",
    "    5: [0, 0, 0],\n",
    "    6: [0, 180, 0],\n",
    "    7: [0, 90, 0],\n",
    "    8: [0, 0, 90],\n",
    "}\n",
    "\n",
    "def rotate_vector(v, axis, angle):\n",
    "    '''\n",
    "    Args:\n",
    "    - v (np.array): Vector to be rotated\n",
    "    - axis (int): Axis along the rotation is performed\n",
    "    - angle (int): Rotation angle\n",
    "    \n",
    "    Returns:\n",
    "    - (np.array)): Rotated vector\n",
    "    '''\n",
    "    if axis == 0:\n",
    "        # X\n",
    "        v = v.dot(np.array([[1, 0, 0], [0, np.cos(np.radians(angle)), np.sin(np.radians(angle))], [0, np.sin(np.radians(angle)), np.cos(np.radians(angle))]]))\n",
    "    elif axis == 1:\n",
    "        # Y\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), 0, np.sin(np.radians(angle))], [0, 1, 0], [-np.sin(np.radians(angle)), 0, np.cos(np.radians(angle))]]))\n",
    "    elif axis == 2:\n",
    "        # Z\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle)), 0], [np.sin(np.radians(angle)), np.cos(np.radians(angle)), 0], [0, 0, 1]]))\n",
    "    else:\n",
    "        raise ValueError('Invalid axis')\n",
    "\n",
    "    return v\n",
    "\n",
    "@dask.delayed\n",
    "def rotate_row(row):\n",
    "    '''\n",
    "    Rotate the force vectors in a row. Dask function.\n",
    "    '''\n",
    "    for i in range(1, N_CELLS + 1):\n",
    "        cols = ['F{}x'.format(str(i)), 'F{}y'.format(str(i)), 'F{}z'.format(str(i))]\n",
    "        for ax in range(3):\n",
    "            row[cols] = rotate_vector(row[cols], ax, rotations[i][ax])\n",
    "            \n",
    "    return row\n",
    "\n",
    "def process_parameters_sheet(params_df):\n",
    "    '''\n",
    "    Process the data in the given pd.DataFrame from the raw excel sheet. \n",
    "    \n",
    "    Args:\n",
    "    - params_df (pd.DataFrame): DataFrame of the parameters excel sheet.\n",
    "    \n",
    "    Returns:\n",
    "    - params_dict (dict): Dictionary with all the parameter in the input DataFrame.\n",
    "    '''\n",
    "    params_dict = {}\n",
    "    params_dict['ExoHipMissalign'] = params_df.iloc[2, 1]\n",
    "    params_dict['ExoKneeMissalign'] = params_df.iloc[2, 2]\n",
    "    params_dict['MarchVelocity'] = params_df.iloc[0, 11]\n",
    "    params_dict['TimeShift'] = params_df.iloc[0, 12]\n",
    "    params_dict['SkinConfig'] = params_df.iloc[0, 13]\n",
    "    \n",
    "    return params_dict\n",
    "\n",
    "def shift_leg_data(df, time_shift, total_len, data_res=0.01):\n",
    "    '''\n",
    "    Shift the data from the leg replica using the known time_shift from the experiment\n",
    "    parameters to match the exoskeleton data in time and lenght.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with the data of the leg replica\n",
    "    - time_shift (float): Shifting time to applied to the data.\n",
    "    - total_len (int): Total desired lenght for the data.\n",
    "    - data_res (float): Data resolution (in seconds).\n",
    "    \n",
    "    Returns:\n",
    "    - (pd.DataFrame): DataFrame with the data of the leg replica shifted.\n",
    "    '''\n",
    "    idx_start = math.ceil(time_shift / data_res)\n",
    "    idx_end = total_len + idx_start\n",
    "    return df.iloc[idx_start:idx_end].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "usual-honolulu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1:\n",
      "[########################################] | 100% Completed |  1min  6.0s\n",
      "\n",
      "Processing file 2:\n",
      "[########################################] | 100% Completed |  1min  4.3s\n",
      "\n",
      "Processing file 3:\n",
      "[########################################] | 100% Completed |  1min  3.8s\n",
      "\n",
      "Processing file 4:\n",
      "[########################################] | 100% Completed |  1min  6.3s\n",
      "\n",
      "Processing file 5:\n",
      "[########################################] | 100% Completed |  1min  4.4s\n",
      "\n",
      "Processing file 6:\n",
      "[########################################] | 100% Completed |  1min 11.3s\n",
      "\n",
      "Processing file 7:\n",
      "[########################################] | 100% Completed |  1min  3.9s\n",
      "\n",
      "Processing file 8:\n",
      "[########################################] | 100% Completed |  1min 10.8s\n",
      "\n",
      "Processing file 9:\n",
      "[########################################] | 100% Completed |  1min  6.1s\n",
      "\n",
      "Processing file 10:\n",
      "[########################################] | 100% Completed |  1min  2.6s\n",
      "\n",
      "Processing file 11:\n",
      "[########################################] | 100% Completed |  1min  2.7s\n",
      "\n",
      "Processing file 12:\n",
      "[########################################] | 100% Completed |  1min  9.5s\n",
      "\n",
      "Processing file 13:\n",
      "[########################################] | 100% Completed |  1min  5.0s\n",
      "\n",
      "Processing file 14:\n",
      "[########################################] | 100% Completed |  1min  3.7s\n",
      "\n",
      "Processing file 15:\n",
      "[########################################] | 100% Completed |  1min  3.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_EXPERIMENTS):\n",
    "    print('Processing file {}:'.format(i + 1))\n",
    "    # Create the directory to save the resulting data\n",
    "    save_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i + 1))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Select only the relevant excel sheets \n",
    "    sheets = ['Parameters', 'RawForces', 'ForceCells', 'H3raw', 'H3processed', 'Leg-Replica']\n",
    "    # Load the data\n",
    "    data_df = pd.read_excel(SOURCE_PATH + ORIGINAL_DATA_DIR + '/' + DATE_EXPERIMENTS + '/0{}-'.format(i + 1) + DATE_EXPERIMENTS + '.xlsx', sheet_name=sheets)\n",
    "\n",
    "    # Pre-process the data\n",
    "    data_df[sheets[0]] = process_parameters_sheet(data_df[sheets[0]])\n",
    "    \n",
    "    # Apply the rotation matrix to each force vector\n",
    "    forces_ddf = dd.from_pandas(data_df[sheets[2]], npartitions=int(len(data_df[sheets[2]]) / 100))\n",
    "    forces_ddf = forces_ddf.apply(rotate_row, axis=1, meta=forces_ddf)\n",
    "    with ProgressBar():\n",
    "        data_df[sheets[2]] = forces_ddf.compute()\n",
    "        \n",
    "        \n",
    "    leg_df_raw = data_df[sheets[5]].iloc[:, :3]\n",
    "    # Correct the time shift between the data from the leg and the data from the exo\n",
    "    leg_df_processed = shift_leg_data(data_df[sheets[5]].iloc[:, 3:], data_df[sheets[0]]['TimeShift'], len(data_df[sheets[4]]))\n",
    "    \n",
    "    assert(len(leg_df_processed) == len(data_df[sheets[4]]) == len(data_df[sheets[2]]))    \n",
    "\n",
    "    json.dump(data_df[sheets[0]], open(save_dir + '/parameters.json', 'w'))\n",
    "    data_df[sheets[1]].to_csv(save_dir + '/force_cells_raw.csv', index=False)\n",
    "    data_df[sheets[2]].to_csv(save_dir + '/force_cells_processed.csv', index=False)\n",
    "    data_df[sheets[3]].to_csv(save_dir + '/H3_raw.csv', index=False)\n",
    "    data_df[sheets[4]].to_csv(save_dir + '/H3_processed.csv', index=False)\n",
    "    leg_df_raw.to_csv(save_dir + '/leg_raw.csv', index=False)\n",
    "    leg_df_processed.to_csv(save_dir + '/leg_processed.csv', index=False)\n",
    "    \n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-shakespeare",
   "metadata": {},
   "source": [
    "## Features and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "foster-louis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 15\n",
      "Selected features: ['LHipPos', 'LHipVel', 'LHipAcc', 'LHipTorque', 'LKneePos', 'LKneeVel', 'LKneeAcc', 'LKneeTorque', 'LAnklePos', 'LAnkleVel', 'LAnkleAcc', 'LAnkleTorque', 'LegKneePositionFiltered', 'LegKneeVelocityFiltered', 'LegKneeTorqueFiltered']\n",
      "\n",
      "\n",
      "Number of targets: 24\n",
      "Selected targets: ['F1x', 'F1y', 'F1z', 'F2x', 'F2y', 'F2z', 'F3x', 'F3y', 'F3z', 'F4x', 'F4y', 'F4z', 'F5x', 'F5y', 'F5z', 'F6x', 'F6y', 'F6z', 'F7x', 'F7y', 'F7z', 'F8x', 'F8y', 'F8z']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yellow-symposium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping 2 data points by null features\n",
      "Experiment 1 -> X: (17952, 15), Y: (17952, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 2 -> X: (17980, 15), Y: (17980, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 3 -> X: (17678, 15), Y: (17678, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 4 -> X: (17957, 15), Y: (17957, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 5 -> X: (17747, 15), Y: (17747, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 6 -> X: (18025, 15), Y: (18025, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 7 -> X: (17768, 15), Y: (17768, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 8 -> X: (18042, 15), Y: (18042, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 9 -> X: (18036, 15), Y: (18036, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 10 -> X: (17903, 15), Y: (17903, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 11 -> X: (17803, 15), Y: (17803, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 12 -> X: (18077, 15), Y: (18077, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 13 -> X: (17674, 15), Y: (17674, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 14 -> X: (17985, 15), Y: (17985, 24) \n",
      "\n",
      "Droping 2 data points by null features\n",
      "Experiment 15 -> X: (17919, 15), Y: (17919, 24) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i in range(1, N_EXPERIMENTS + 1):\n",
    "    # Define the path to load the data\n",
    "    data_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(data_dir + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(data_dir + '/H3_processed.csv')\n",
    "    leg_df = pd.read_csv(data_dir + '/leg_processed.csv')\n",
    "    features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    \n",
    "    # Rename columns to manage with some typos\n",
    "    features_df = features_df.rename(columns={'LankleTorque': 'LAnkleTorque', 'RankleTorque': 'RAnkleTorque'})\n",
    "\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    # Store the final array\n",
    "    targets_dict[i] = targets_df[targets].values\n",
    "    features_dict[i] = features_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-prefix",
   "metadata": {},
   "source": [
    "## Split and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "changing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (11): [12, 3, 4, 8, 9, 5, 1, 15, 13, 7, 14]\n",
      "Test experiments ids (4): [2, 11, 10, 6]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(1, N_EXPERIMENTS + 1))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[int(N_EXPERIMENTS * TEST_SIZE):]\n",
    "test_experiments = experiments[:int(N_EXPERIMENTS * TEST_SIZE)]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thousand-drink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> X: (196835, 15), Y: (196835, 24)\n",
      "Test -> X: (71711, 15), Y: (71711, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([features_dict[i] for i in train_experiments], axis=0)\n",
    "Y_train = np.concatenate([targets_dict[i] for i in train_experiments], axis=0)\n",
    "X_test = np.concatenate([features_dict[i] for i in test_experiments], axis=0)\n",
    "Y_test = np.concatenate([targets_dict[i] for i in test_experiments], axis=0)\n",
    "\n",
    "print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Test -> X: {}, Y: {}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aquatic-cabinet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> min: -4.7494235121468265, max: 9.309577254501134, mean: -9.857268086378168e-18, std: 1.0000000000000002\n",
      "Test -> min: -4.681015530353451, max: 4.014120323189233, mean: -0.03396178956156562, std: 1.0124789966278167\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm =  scaler.transform(X_test)\n",
    "\n",
    "print('Train -> min: {}, max: {}, mean: {}, std: {}'.format(np.min(X_train_norm), np.max(X_train_norm), np.mean(X_train_norm), np.std(X_train_norm)))\n",
    "print('Test -> min: {}, max: {}, mean: {}, std: {}'.format(np.min(X_test_norm), np.max(X_test_norm), np.mean(X_test_norm), np.std(X_test_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-copyright",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "congressional-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(save_dir + '/X_train_' + RESULTS_ID + '.npy', X_train_norm)    \n",
    "np.save(save_dir + '/X_test_' + RESULTS_ID + '.npy', X_test_norm)    \n",
    "np.save(save_dir + '/Y_train_' + RESULTS_ID + '.npy', Y_train)    \n",
    "np.save(save_dir + '/Y_test_' + RESULTS_ID + '.npy', Y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-conflict",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
