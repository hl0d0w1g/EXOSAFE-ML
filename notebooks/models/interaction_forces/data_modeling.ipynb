{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hearing-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-consortium",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "august-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Path where the data is stored\n",
    "SOURCE_PATH = '../../../data'\n",
    "# Directory inside SOURCE_PATH where the derived data is stored\n",
    "DERIVED_DATA_DIR = '/derived_data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Experiment params\n",
    "DATE_EXPERIMENTS = '24022021'\n",
    "N_EXPERIMENTS = 15\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "# ID of the training and test data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0004_15042021'\n",
    "\n",
    "# Scaler to normalize the data\n",
    "SCALER = MinMaxScaler() # StandardScaler()\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "CV = 6\n",
    "# Experiments for training set (int)\n",
    "TRAIN_SIZE = 12\n",
    "# Experiments for test set (int)\n",
    "TEST_SIZE = 3\n",
    "\n",
    "assert(TRAIN_SIZE + TEST_SIZE == N_EXPERIMENTS)\n",
    "assert(TRAIN_SIZE % CV == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-domain",
   "metadata": {},
   "source": [
    "## Features and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "binding-wound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 15\n",
      "Selected features: ['LHipPos', 'LHipVel', 'LHipAcc', 'LHipTorque', 'LKneePos', 'LKneeVel', 'LKneeAcc', 'LKneeTorque', 'LAnklePos', 'LAnkleVel', 'LAnkleAcc', 'LAnkleTorque', 'LegKneePositionFiltered', 'LegKneeVelocityFiltered', 'LegKneeTorqueFiltered']\n",
      "\n",
      "\n",
      "Number of targets: 24\n",
      "Selected targets: ['F1x', 'F1y', 'F1z', 'F2x', 'F2y', 'F2z', 'F3x', 'F3y', 'F3z', 'F4x', 'F4y', 'F4z', 'F5x', 'F5y', 'F5z', 'F6x', 'F6y', 'F6z', 'F7x', 'F7y', 'F7z', 'F8x', 'F8y', 'F8z']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fourth-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping 8722 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 1 -> X: (8721, 15), Y: (8721, 24) \n",
      "\n",
      "Droping 8736 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 2 -> X: (8735, 15), Y: (8735, 24) \n",
      "\n",
      "Droping 8589 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 3 -> X: (8588, 15), Y: (8588, 24) \n",
      "\n",
      "Droping 8726 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 4 -> X: (8725, 15), Y: (8725, 24) \n",
      "\n",
      "Droping 8624 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 5 -> X: (8623, 15), Y: (8623, 24) \n",
      "\n",
      "Droping 8760 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 6 -> X: (8759, 15), Y: (8759, 24) \n",
      "\n",
      "Droping 8639 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 7 -> X: (8638, 15), Y: (8638, 24) \n",
      "\n",
      "Droping 8773 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 8 -> X: (8772, 15), Y: (8772, 24) \n",
      "\n",
      "Droping 8769 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 9 -> X: (8768, 15), Y: (8768, 24) \n",
      "\n",
      "Droping 8705 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 10 -> X: (8704, 15), Y: (8704, 24) \n",
      "\n",
      "Droping 8659 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 11 -> X: (8658, 15), Y: (8658, 24) \n",
      "\n",
      "Droping 8793 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 12 -> X: (8792, 15), Y: (8792, 24) \n",
      "\n",
      "Droping 8599 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 13 -> X: (8598, 15), Y: (8598, 24) \n",
      "\n",
      "Droping 8750 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 14 -> X: (8749, 15), Y: (8749, 24) \n",
      "\n",
      "Droping 8717 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 15 -> X: (8716, 15), Y: (8716, 24) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i in range(1, N_EXPERIMENTS + 1):\n",
    "    # Define the path to load the data\n",
    "    data_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(data_dir + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(data_dir + '/H3_processed.csv')\n",
    "    leg_df = pd.read_csv(data_dir + '/leg_processed.csv')\n",
    "    features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    \n",
    "    idx_aux = targets_df.duplicated(keep='first')\n",
    "    targets_df = targets_df.loc[~idx_aux]\n",
    "    features_df = features_df.loc[~idx_aux]\n",
    "    print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "    \n",
    "    # Rename columns to manage with some typos\n",
    "    features_df = features_df.rename(columns={'LankleTorque': 'LAnkleTorque', 'RankleTorque': 'RAnkleTorque'})\n",
    "\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    # Store the final array\n",
    "    targets_dict[i] = targets_df[targets].values\n",
    "    features_dict[i] = features_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-martial",
   "metadata": {},
   "source": [
    "## Normalization and split for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alternate-uzbekistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (12): [2, 11, 10, 6, 12, 3, 4, 8, 9, 5, 1, 15]\n",
      "Test experiments ids (3): [13, 7, 14]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(1, N_EXPERIMENTS + 1))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[:TRAIN_SIZE]\n",
    "test_experiments = experiments[TRAIN_SIZE:]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "assert(len(train_experiments) + len(test_experiments) == N_EXPERIMENTS)\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "under-naples",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> X: (104561, 15), Y: (104561, 24)\n",
      "Test -> X: (25985, 15), Y: (25985, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([features_dict[i] for i in train_experiments], axis=0)\n",
    "Y_train = np.concatenate([targets_dict[i] for i in train_experiments], axis=0)\n",
    "X_test = np.concatenate([features_dict[i] for i in test_experiments], axis=0)\n",
    "Y_test = np.concatenate([targets_dict[i] for i in test_experiments], axis=0)\n",
    "\n",
    "print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Test -> X: {}, Y: {}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cellular-emperor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.4448378  0.37159194 0.51588258 0.4190689  0.20284009 0.56795055\n",
      " 0.63143291 0.31957625 0.60723926 0.41817225 0.56123806 0.23682047\n",
      " 0.23266228 0.39591114 0.76709162], \n",
      " std: [0.33288867 0.29378422 0.16624394 0.18524527 0.2809568  0.19569883\n",
      " 0.14494188 0.17550486 0.23793802 0.17673375 0.11891252 0.1018684\n",
      " 0.25310102 0.15488524 0.28605986]\n",
      "\n",
      "Test -> \n",
      " min: [ 8.72949314e-03  1.33303228e-02  1.17412805e-01  5.13174234e-02\n",
      "  1.16680663e-04  5.32940183e-03  1.17831535e-02  1.31408591e-03\n",
      "  3.86328051e-02  1.81936517e-03  3.69541600e-01  5.58295788e-02\n",
      "  3.02266353e-02  2.49667728e-03 -7.17077873e-04], \n",
      " max: [0.99598895 0.98188223 0.9722875  0.98806157 1.00044171 0.99840212\n",
      " 0.95387876 1.00512108 1.00091957 0.78894653 0.98468742 1.38876155\n",
      " 0.99764779 0.97202958 0.99010046], \n",
      " mean: [0.44555736 0.37209331 0.51573448 0.41974707 0.19910736 0.5677548\n",
      " 0.63148326 0.33370262 0.60738028 0.41791186 0.56121584 0.29365366\n",
      " 0.21422484 0.39624999 0.57919034], \n",
      " std: [0.33018785 0.29093333 0.16136122 0.18197742 0.27165826 0.18546684\n",
      " 0.13309499 0.20647797 0.23777629 0.17718507 0.11924614 0.14581463\n",
      " 0.24510286 0.14525942 0.30861692]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = SCALER.fit(X_train)\n",
    "\n",
    "X_train_norm = s.transform(X_train)\n",
    "X_test_norm = s.transform(X_test)\n",
    "\n",
    "print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "print('Test -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_test_norm, axis=0), np.max(X_test_norm, axis=0), np.mean(X_test_norm, axis=0), np.std(X_test_norm, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "personal-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(save_dir + '/X_train_' + DATA_ID + '.npy', X_train_norm)    \n",
    "np.save(save_dir + '/X_test_' + DATA_ID + '.npy', X_test_norm)    \n",
    "np.save(save_dir + '/Y_train_' + DATA_ID + '.npy', Y_train)    \n",
    "np.save(save_dir + '/Y_test_' + DATA_ID + '.npy', Y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "operational-child",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV folds [[2, 11], [10, 6], [12, 3], [4, 8], [9, 5], [1, 15]]\n",
      "\n",
      "Fold 1\n",
      "Train experiments ids (10): [10, 6, 12, 3, 4, 8, 9, 5, 1, 15]\n",
      "Validation experiments ids (2): [2, 11]\n",
      "Train -> X: (87168, 15), Y: (87168, 24)\n",
      "Validation -> X: (17393, 15), Y: (17393, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44445726 0.37136815 0.51588106 0.41800079 0.2019976  0.56798724\n",
      " 0.63141213 0.3231011  0.60725098 0.41963963 0.56365113 0.23903505\n",
      " 0.22978311 0.39586013 0.78802145], \n",
      " std: [0.3332355  0.29434303 0.16660117 0.18290819 0.27904884 0.19376346\n",
      " 0.14324514 0.18067721 0.23779791 0.17718663 0.11925173 0.10372276\n",
      " 0.25141175 0.15299483 0.2753975 ]\n",
      "\n",
      "Valid -> \n",
      " min: [ 0.00998084 -0.00052571  0.11892211  0.05554243  0.00013737  0.0036844\n",
      "  0.00157334  0.00568987 -0.00028417  0.00098134  0.00476934  0.00620614\n",
      "  0.05807598  0.00729467 -0.00025077], \n",
      " max: [0.99897912 0.95801894 0.94284543 0.9975903  0.99943121 0.99532692\n",
      " 0.99726083 0.73967378 0.99877855 1.00341042 1.00426098 0.53395524\n",
      " 1.00224008 0.99178799 0.97831542], \n",
      " mean: [0.44674494 0.37072748 0.51589021 0.42442193 0.20706236 0.56776671\n",
      " 0.63153705 0.3019109  0.60650957 0.41939175 0.56352104 0.2257217\n",
      " 0.25022491 0.39616674 0.66184707], \n",
      " std: [0.33113838 0.29190314 0.16444193 0.19645227 0.29029322 0.20512331\n",
      " 0.15316243 0.14558377 0.23904298 0.17808548 0.12025497 0.09120719\n",
      " 0.26402705 0.16403125 0.31433641]\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train experiments ids (10): [2, 11, 12, 3, 4, 8, 9, 5, 1, 15]\n",
      "Validation experiments ids (2): [10, 6]\n",
      "Train -> X: (87098, 15), Y: (87098, 24)\n",
      "Validation -> X: (17463, 15), Y: (17463, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44470227 0.37386484 0.48398716 0.42025193 0.20213601 0.56859073\n",
      " 0.63191841 0.32325082 0.60727332 0.41816685 0.56119945 0.23799913\n",
      " 0.19505719 0.39283932 0.78686112], \n",
      " std: [0.33328101 0.29579659 0.17985115 0.18398401 0.27921863 0.19392225\n",
      " 0.14321471 0.17983733 0.23796291 0.17675351 0.11895388 0.10512494\n",
      " 0.26238767 0.15334126 0.28265989]\n",
      "\n",
      "Valid -> \n",
      " min: [ 1.14606352e-02  1.61582483e-02 -7.41678711e-02  4.38334790e-02\n",
      " -4.33913680e-05  1.65303599e-03  1.11284201e-02  4.26412696e-03\n",
      "  5.71162828e-02  1.54931668e-03  3.08058087e-01  3.10563685e-02\n",
      " -4.86402701e-02 -5.22490169e-03  6.11250137e-03], \n",
      " max: [0.99896397 1.00591169 1.00782738 0.99660958 1.00049638 1.00120349\n",
      " 1.00082268 0.73399018 0.99914361 0.78910764 0.98064684 0.5372301\n",
      " 0.99607793 0.97106376 1.00727327], \n",
      " mean: [0.44551378 0.37340877 0.48415161 0.41316842 0.20674749 0.56885028\n",
      " 0.63212179 0.30124908 0.60706939 0.41819923 0.56143067 0.23094181\n",
      " 0.19674316 0.39233346 0.70189605], \n",
      " std: [0.33092403 0.29414223 0.17999474 0.19130263 0.29031998 0.20567604\n",
      " 0.15393976 0.15073259 0.23781373 0.17663515 0.11870584 0.08350835\n",
      " 0.28000408 0.16693621 0.30432582]\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train experiments ids (10): [2, 11, 10, 6, 4, 8, 9, 5, 1, 15]\n",
      "Validation experiments ids (2): [12, 3]\n",
      "Train -> X: (87181, 15), Y: (87181, 24)\n",
      "Validation -> X: (17380, 15), Y: (17380, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44464216 0.37125917 0.51589886 0.40379198 0.20162273 0.56798323\n",
      " 0.63148695 0.32050963 0.60741523 0.41779974 0.55915097 0.23543107\n",
      " 0.22876917 0.39588258 0.78883118], \n",
      " std: [0.33346609 0.29410946 0.16647927 0.187983   0.27876186 0.19337321\n",
      " 0.1427416  0.18080214 0.23778808 0.17668895 0.11928917 0.10015122\n",
      " 0.25035971 0.15259221 0.27505471]\n",
      "\n",
      "Valid -> \n",
      " min: [ 0.01175371  0.01842831  0.11748284 -0.02606639  0.00019217  0.00347077\n",
      "  0.00438832 -0.00428239  0.00028409 -0.00066303 -0.00477177  0.00656395\n",
      "  0.05943428  0.00519774  0.00025071], \n",
      " max: [1.00051566 0.97280218 0.93428874 1.00247849 0.99950389 0.99879796\n",
      " 0.99917799 0.75911757 0.99860664 0.99659892 0.99573685 0.8902464\n",
      " 0.99776493 0.96032757 0.97674765], \n",
      " mean: [0.44719915 0.37326115 0.51580092 0.41084774 0.20894657 0.56778662\n",
      " 0.63116186 0.29736415 0.6063566  0.41772    0.55911139 0.24378992\n",
      " 0.25219078 0.39605436 0.65804235], \n",
      " std: [0.33100919 0.29214162 0.16505841 0.20273913 0.29164126 0.20697054\n",
      " 0.15550952 0.1499267  0.23868675 0.17766119 0.12043232 0.10981366\n",
      " 0.2655669  0.16590984 0.31391626]\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train experiments ids (10): [2, 11, 10, 6, 12, 3, 9, 5, 1, 15]\n",
      "Validation experiments ids (2): [4, 8]\n",
      "Train -> X: (87064, 15), Y: (87064, 24)\n",
      "Validation -> X: (17497, 15), Y: (17497, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44324124 0.37161197 0.51586965 0.42027356 0.20380759 0.56718593\n",
      " 0.63145356 0.31546631 0.60711156 0.41814639 0.56125284 0.23798033\n",
      " 0.23577082 0.39596607 0.72936013], \n",
      " std: [0.33354265 0.29328043 0.16606496 0.18815824 0.28300224 0.19815615\n",
      " 0.14678291 0.1717199  0.23806606 0.17687662 0.11907889 0.1016813\n",
      " 0.25570854 0.15689164 0.29909533]\n",
      "\n",
      "Valid -> \n",
      " min: [-3.55693494e-03  5.25435748e-04  1.13078645e-01  9.14910173e-02\n",
      "  7.01926471e-05 -1.65377944e-03  5.94157155e-03  2.50282322e-02\n",
      "  5.95488726e-02  6.62594664e-04  3.65534793e-01  3.06555560e-02\n",
      "  5.50855950e-02  2.56634977e-02  7.86192068e-01], \n",
      " max: [0.99948278 0.99412306 0.99276579 0.95147307 0.99454307 0.97227315\n",
      " 0.98973076 0.91069292 0.99850015 0.78648704 0.97580235 0.80285551\n",
      " 0.96627742 0.97516059 0.98999067], \n",
      " mean: [0.44098161 0.37149229 0.51594691 0.41307458 0.19802588 0.56748539\n",
      " 0.63133015 0.34002708 0.60787468 0.41830097 0.56116453 0.23104904\n",
      " 0.21719439 0.3956378  0.95484123], \n",
      " std: [0.33669163 0.29627831 0.16713168 0.16988305 0.27049755 0.18503982\n",
      " 0.13540934 0.1919354  0.23729884 0.17602106 0.11808117 0.10259953\n",
      " 0.23910472 0.14448751 0.03925484]\n",
      "\n",
      "\n",
      "Fold 5\n",
      "Train experiments ids (10): [2, 11, 10, 6, 12, 3, 4, 8, 1, 15]\n",
      "Validation experiments ids (2): [9, 5]\n",
      "Train -> X: (87170, 15), Y: (87170, 24)\n",
      "Validation -> X: (17391, 15), Y: (17391, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44497583 0.37145863 0.51588909 0.41887529 0.203948   0.56792433\n",
      " 0.63142721 0.31472753 0.60732605 0.41819762 0.56119947 0.2369168\n",
      " 0.23659028 0.39591997 0.72936534], \n",
      " std: [0.33236431 0.29326089 0.1658171  0.18801578 0.28338949 0.19811817\n",
      " 0.14701393 0.16977104 0.23802507 0.17678507 0.11899116 0.10149417\n",
      " 0.2557354  0.15704352 0.29880658]\n",
      "\n",
      "Valid -> \n",
      " min: [1.12345907e-02 3.97216197e-03 6.85473169e-02 1.14573502e-01\n",
      " 4.33679593e-05 1.62176710e-02 1.96699412e-02 3.08531664e-02\n",
      " 5.93019252e-02 7.58518304e-04 3.63808781e-01 1.87756135e-02\n",
      " 4.83098127e-02 3.34354482e-02 7.80762261e-01], \n",
      " max: [0.99836641 0.9646917  0.9302539  0.98329714 0.98312436 0.96656089\n",
      " 0.99174474 0.92811493 0.99899304 0.78636823 0.97702231 0.7170776\n",
      " 0.96454776 0.96119995 0.99277925], \n",
      " mean: [0.44414592 0.37226015 0.51584994 0.42003936 0.19728686 0.568082\n",
      " 0.63146147 0.34387982 0.60680424 0.4180451  0.56143151 0.2363376\n",
      " 0.21297376 0.39586684 0.95618941], \n",
      " std: [0.33550371 0.29639252 0.16836713 0.17067885 0.26836241 0.1830911\n",
      " 0.13407435 0.20004169 0.23750073 0.17647624 0.11851737 0.10372245\n",
      " 0.23848744 0.1435791  0.03962961]\n",
      "\n",
      "\n",
      "Fold 6\n",
      "Train experiments ids (10): [2, 11, 10, 6, 12, 3, 4, 8, 9, 5]\n",
      "Validation experiments ids (2): [1, 15]\n",
      "Train -> X: (87124, 15), Y: (87124, 24)\n",
      "Validation -> X: (17437, 15), Y: (17437, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44526447 0.37185603 0.51589959 0.41908575 0.20359536 0.56800067\n",
      " 0.6308378  0.34209259 0.60746678 0.41812376 0.5612724  0.25717739\n",
      " 0.23318438 0.39912056 0.78571241], \n",
      " std: [0.33279353 0.29380573 0.16627795 0.18554849 0.28241933 0.19735483\n",
      " 0.14692826 0.18354102 0.23825937 0.17683103 0.11898262 0.11176844\n",
      " 0.25555479 0.15845729 0.27880546]\n",
      "\n",
      "Valid -> \n",
      " min: [ 3.54432800e-03  3.35267565e-02  1.12638570e-01  2.53429786e-02\n",
      "  1.93041745e-04  8.05225404e-03 -1.57582146e-03  1.47489068e-02\n",
      "  5.67795936e-02  4.37261601e-03  3.67471068e-01 -7.02020052e-03\n",
      "  4.63841334e-02  4.22540139e-02  9.03595920e-03], \n",
      " max: [0.99935131 0.95376974 0.94893255 0.97931969 0.97932624 0.95603728\n",
      " 0.9851377  1.07745277 1.00085712 0.79290765 0.98230285 1.12415001\n",
      " 0.95269192 1.00828001 0.97799412], \n",
      " mean: [0.44270592 0.37027243 0.51579758 0.41898471 0.19906638 0.56770016\n",
      " 0.63092364 0.35549913 0.60922352 0.41841455 0.56106648 0.27928446\n",
      " 0.23005363 0.39953266 0.67405281], \n",
      " std: [0.33335543 0.29367321 0.1660739  0.18372271 0.27350089 0.18720514\n",
      " 0.13604665 0.2143717  0.23754901 0.17624668 0.1185615  0.12960519\n",
      " 0.24044896 0.14418357 0.30317658]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the experiments of the training sets in different folds\n",
    "exp_per_fold = len(train_experiments) // CV\n",
    "cv_folds = [train_experiments[x:x+exp_per_fold] for x in range(0, len(train_experiments), exp_per_fold)]\n",
    "print('CV folds {}\\n'.format(cv_folds))\n",
    "\n",
    "for fold_id in range(CV):\n",
    "    print('Fold {}'.format(fold_id + 1))\n",
    "    \n",
    "    cv_folds_cp = cv_folds.copy()\n",
    "    valid_experiments_fold = cv_folds_cp.pop(fold_id)\n",
    "    train_experiments_fold = [item for sublist in cv_folds_cp for item in sublist]\n",
    "\n",
    "    print('Train experiments ids ({}): {}'.format(len(train_experiments_fold), train_experiments_fold))\n",
    "    print('Validation experiments ids ({}): {}'.format(len(valid_experiments_fold), valid_experiments_fold))\n",
    "\n",
    "    assert(len(train_experiments_fold) + len(valid_experiments_fold) == len(train_experiments))\n",
    "    # Check that no validation experiments are in train\n",
    "    assert(not any([i in valid_experiments_fold for i in train_experiments_fold]))\n",
    "    # Check that no test experiments are in train or validation folds\n",
    "    assert(not any([i in test_experiments for i in train_experiments_fold]))\n",
    "    assert(not any([i in test_experiments for i in valid_experiments_fold]))\n",
    "    \n",
    "    X_train = np.concatenate([features_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    Y_train = np.concatenate([targets_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    X_valid = np.concatenate([features_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "    Y_valid = np.concatenate([targets_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "\n",
    "    print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "    print('Validation -> X: {}, Y: {}'.format(X_valid.shape, Y_valid.shape))\n",
    "    \n",
    "    # Normalize the data\n",
    "    s = SCALER.fit(X_train)\n",
    "\n",
    "    X_train_norm = s.transform(X_train)\n",
    "    X_valid_norm =  s.transform(X_valid)\n",
    "\n",
    "    print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "    print('Valid -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}'.format(np.min(X_valid_norm, axis=0), np.max(X_valid_norm, axis=0), np.mean(X_valid_norm, axis=0), np.std(X_valid_norm, axis=0)))\n",
    "    \n",
    "    save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    np.save(save_dir + '/X_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_train_norm)    \n",
    "    np.save(save_dir + '/X_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_valid_norm)    \n",
    "    np.save(save_dir + '/Y_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_train)    \n",
    "    np.save(save_dir + '/Y_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_valid)      \n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-ontario",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
