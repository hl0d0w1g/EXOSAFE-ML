{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "approximate-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-america",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "multiple-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Directory where the derived data is stored\n",
    "DERIVED_DATA_DIR = '../../../data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "# ID of the training and test data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0006_14062021'\n",
    "\n",
    "# Scaler to normalize the data\n",
    "SCALER = MinMaxScaler() # StandardScaler()\n",
    "\n",
    "# Total number of experiments\n",
    "N_EXPERIMENTS = 63\n",
    "# Number of folds for cross-validation\n",
    "CV = 6\n",
    "# Experiments for training set (int)\n",
    "TRAIN_SIZE = 54\n",
    "# Experiments for test set (int)\n",
    "TEST_SIZE = 9\n",
    "\n",
    "assert(TRAIN_SIZE + TEST_SIZE == N_EXPERIMENTS)\n",
    "assert(TRAIN_SIZE % CV == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-tours",
   "metadata": {},
   "source": [
    "## Features and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "difficult-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 4\n",
      "Selected features: ['LHipPos', 'LHipTorque', 'LKneePos', 'LKneeTorque']\n",
      "\n",
      "\n",
      "Number of targets: 24\n",
      "Selected targets: ['F1x', 'F1y', 'F1z', 'F2x', 'F2y', 'F2z', 'F3x', 'F3y', 'F3z', 'F4x', 'F4y', 'F4z', 'F5x', 'F5y', 'F5z', 'F6x', 'F6y', 'F6z', 'F7x', 'F7y', 'F7z', 'F8x', 'F8y', 'F8z']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "# features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "features = [H3_LEG + a + m for a in ['Hip', 'Knee'] for m in ['Pos', 'Torque']]\n",
    "targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collaborative-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_dirs_path = glob.glob(DERIVED_DATA_DIR + '/*/*')\n",
    "\n",
    "assert(len(experiments_dirs_path) == N_EXPERIMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specified-simple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Experiment 1 from 10032021\n",
      "Droping 2917 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 0 -> X: (2916, 4), Y: (2916, 24) \n",
      "\n",
      "1 - Experiment 1 from 16022021\n",
      "Droping 8709 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 1 -> X: (8708, 4), Y: (8708, 24) \n",
      "\n",
      "2 - Experiment 2 from 16022021\n",
      "Droping 8696 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 2 -> X: (8695, 4), Y: (8695, 24) \n",
      "\n",
      "3 - Experiment 3 from 16022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 3 -> X: (8707, 4), Y: (8707, 24) \n",
      "\n",
      "4 - Experiment 4 from 16022021\n",
      "Droping 8736 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 4 -> X: (8735, 4), Y: (8735, 24) \n",
      "\n",
      "5 - Experiment 5 from 16022021\n",
      "Droping 8706 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 5 -> X: (8705, 4), Y: (8705, 24) \n",
      "\n",
      "6 - Experiment 6 from 16022021\n",
      "Droping 8680 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 6 -> X: (8679, 4), Y: (8679, 24) \n",
      "\n",
      "7 - Experiment 2 from 17022021\n",
      "Droping 8711 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 7 -> X: (8710, 4), Y: (8710, 24) \n",
      "\n",
      "8 - Experiment 3 from 17022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 8 -> X: (8734, 4), Y: (8734, 24) \n",
      "\n",
      "9 - Experiment 4 from 17022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 9 -> X: (8698, 4), Y: (8698, 24) \n",
      "\n",
      "10 - Experiment 1 from 19022021\n",
      "Droping 8697 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 10 -> X: (8696, 4), Y: (8696, 24) \n",
      "\n",
      "11 - Experiment 10 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 11 -> X: (8724, 4), Y: (8724, 24) \n",
      "\n",
      "12 - Experiment 11 from 19022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 12 -> X: (8719, 4), Y: (8719, 24) \n",
      "\n",
      "13 - Experiment 12 from 19022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 13 -> X: (8732, 4), Y: (8732, 24) \n",
      "\n",
      "14 - Experiment 13 from 19022021\n",
      "Droping 5792 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 14 -> X: (5791, 4), Y: (5791, 24) \n",
      "\n",
      "15 - Experiment 14 from 19022021\n",
      "Droping 8718 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 15 -> X: (8717, 4), Y: (8717, 24) \n",
      "\n",
      "16 - Experiment 15 from 19022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 16 -> X: (8738, 4), Y: (8738, 24) \n",
      "\n",
      "17 - Experiment 16 from 19022021\n",
      "Droping 8243 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 17 -> X: (8242, 4), Y: (8242, 24) \n",
      "\n",
      "18 - Experiment 17 from 19022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 18 -> X: (8734, 4), Y: (8734, 24) \n",
      "\n",
      "19 - Experiment 18 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 19 -> X: (8724, 4), Y: (8724, 24) \n",
      "\n",
      "20 - Experiment 2 from 19022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 20 -> X: (8707, 4), Y: (8707, 24) \n",
      "\n",
      "21 - Experiment 3 from 19022021\n",
      "Droping 8734 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 21 -> X: (8733, 4), Y: (8733, 24) \n",
      "\n",
      "22 - Experiment 4 from 19022021\n",
      "Droping 8717 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 22 -> X: (8716, 4), Y: (8716, 24) \n",
      "\n",
      "23 - Experiment 5 from 19022021\n",
      "Droping 8730 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 23 -> X: (8729, 4), Y: (8729, 24) \n",
      "\n",
      "24 - Experiment 6 from 19022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 24 -> X: (8737, 4), Y: (8737, 24) \n",
      "\n",
      "25 - Experiment 7 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 25 -> X: (8724, 4), Y: (8724, 24) \n",
      "\n",
      "26 - Experiment 8 from 19022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 26 -> X: (8747, 4), Y: (8747, 24) \n",
      "\n",
      "27 - Experiment 9 from 19022021\n",
      "Droping 8704 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 27 -> X: (8703, 4), Y: (8703, 24) \n",
      "\n",
      "28 - Experiment 1 from 22022021\n",
      "Droping 8715 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 28 -> X: (8714, 4), Y: (8714, 24) \n",
      "\n",
      "29 - Experiment 10 from 22022021\n",
      "Droping 8736 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 29 -> X: (8735, 4), Y: (8735, 24) \n",
      "\n",
      "30 - Experiment 11 from 22022021\n",
      "Droping 8702 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 30 -> X: (8701, 4), Y: (8701, 24) \n",
      "\n",
      "31 - Experiment 12 from 22022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 31 -> X: (8707, 4), Y: (8707, 24) \n",
      "\n",
      "32 - Experiment 13 from 22022021\n",
      "Droping 8749 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 32 -> X: (8748, 4), Y: (8748, 24) \n",
      "\n",
      "33 - Experiment 14 from 22022021\n",
      "Droping 8727 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 33 -> X: (8726, 4), Y: (8726, 24) \n",
      "\n",
      "34 - Experiment 15 from 22022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 34 -> X: (8698, 4), Y: (8698, 24) \n",
      "\n",
      "35 - Experiment 16 from 22022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 35 -> X: (8732, 4), Y: (8732, 24) \n",
      "\n",
      "36 - Experiment 17 from 22022021\n",
      "Droping 8705 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 36 -> X: (8704, 4), Y: (8704, 24) \n",
      "\n",
      "37 - Experiment 18 from 22022021\n",
      "Droping 8701 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 37 -> X: (8700, 4), Y: (8700, 24) \n",
      "\n",
      "38 - Experiment 19 from 22022021\n",
      "Droping 5815 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 38 -> X: (5814, 4), Y: (5814, 24) \n",
      "\n",
      "39 - Experiment 2 from 22022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 39 -> X: (8732, 4), Y: (8732, 24) \n",
      "\n",
      "40 - Experiment 3 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 40 -> X: (8719, 4), Y: (8719, 24) \n",
      "\n",
      "41 - Experiment 4 from 22022021\n",
      "Droping 8732 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 41 -> X: (8731, 4), Y: (8731, 24) \n",
      "\n",
      "42 - Experiment 5 from 22022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 42 -> X: (8738, 4), Y: (8738, 24) \n",
      "\n",
      "43 - Experiment 6 from 22022021\n",
      "Droping 8721 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 43 -> X: (8720, 4), Y: (8720, 24) \n",
      "\n",
      "44 - Experiment 7 from 22022021\n",
      "Droping 8710 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 44 -> X: (8709, 4), Y: (8709, 24) \n",
      "\n",
      "45 - Experiment 8 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 45 -> X: (8719, 4), Y: (8719, 24) \n",
      "\n",
      "46 - Experiment 9 from 22022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 46 -> X: (8738, 4), Y: (8738, 24) \n",
      "\n",
      "47 - Experiment 1 from 24022021\n",
      "Droping 8690 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 47 -> X: (8689, 4), Y: (8689, 24) \n",
      "\n",
      "48 - Experiment 10 from 24022021\n",
      "Droping 8519 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 48 -> X: (8518, 4), Y: (8518, 24) \n",
      "\n",
      "49 - Experiment 11 from 24022021\n",
      "Droping 8627 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 49 -> X: (8626, 4), Y: (8626, 24) \n",
      "\n",
      "50 - Experiment 12 from 24022021\n",
      "Droping 8744 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 50 -> X: (8743, 4), Y: (8743, 24) \n",
      "\n",
      "51 - Experiment 13 from 24022021\n",
      "Droping 8569 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 51 -> X: (8568, 4), Y: (8568, 24) \n",
      "\n",
      "52 - Experiment 14 from 24022021\n",
      "Droping 8729 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 52 -> X: (8728, 4), Y: (8728, 24) \n",
      "\n",
      "53 - Experiment 15 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 53 -> X: (8686, 4), Y: (8686, 24) \n",
      "\n",
      "54 - Experiment 16 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 54 -> X: (8686, 4), Y: (8686, 24) \n",
      "\n",
      "55 - Experiment 2 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 55 -> X: (8712, 4), Y: (8712, 24) \n",
      "\n",
      "56 - Experiment 3 from 24022021\n",
      "Droping 8543 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 56 -> X: (8542, 4), Y: (8542, 24) \n",
      "\n",
      "57 - Experiment 4 from 24022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 57 -> X: (8724, 4), Y: (8724, 24) \n",
      "\n",
      "58 - Experiment 5 from 24022021\n",
      "Droping 8568 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 58 -> X: (8567, 4), Y: (8567, 24) \n",
      "\n",
      "59 - Experiment 6 from 24022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 59 -> X: (8747, 4), Y: (8747, 24) \n",
      "\n",
      "60 - Experiment 7 from 24022021\n",
      "Droping 8603 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 60 -> X: (8602, 4), Y: (8602, 24) \n",
      "\n",
      "61 - Experiment 8 from 24022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 61 -> X: (8737, 4), Y: (8737, 24) \n",
      "\n",
      "62 - Experiment 9 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 62 -> X: (8712, 4), Y: (8712, 24) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i, exp_path in enumerate(experiments_dirs_path):\n",
    "    print('{} - Experiment {} from {}'.format(i, exp_path.split('/')[-1], exp_path.split('/')[-2]))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "    # leg_df = pd.read_csv(exp_path + '/leg_processed.csv')\n",
    "    # features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    features_df = exo_df\n",
    "    \n",
    "    idx_aux = targets_df.duplicated(keep='first')\n",
    "    targets_df = targets_df.loc[~idx_aux]\n",
    "    features_df = features_df.loc[~idx_aux]\n",
    "    print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "    \n",
    "    # Drop first row to remove noise in the start of the data recording\n",
    "    targets_df = targets_df.iloc[1:]\n",
    "    features_df = features_df.iloc[1:]\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    assert(len(features_df) == len(targets_df))\n",
    "    # Store the final array\n",
    "    targets_dict[i] = targets_df[targets].values\n",
    "    features_dict[i] = features_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-covering",
   "metadata": {},
   "source": [
    "## Normalization and split for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "positive-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (54): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Test experiments ids (9): [31, 32, 16, 2, 26, 56, 48, 24, 54]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(N_EXPERIMENTS))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[:TRAIN_SIZE]\n",
    "test_experiments = experiments[TRAIN_SIZE:]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "assert(len(train_experiments) + len(test_experiments) == N_EXPERIMENTS)\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "plastic-faith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> X: (458154, 4), Y: (458154, 24)\n",
      "Test -> X: (78118, 4), Y: (78118, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([features_dict[i] for i in train_experiments], axis=0)\n",
    "Y_train = np.concatenate([targets_dict[i] for i in train_experiments], axis=0)\n",
    "X_test = np.concatenate([features_dict[i] for i in test_experiments], axis=0)\n",
    "Y_test = np.concatenate([targets_dict[i] for i in test_experiments], axis=0)\n",
    "\n",
    "print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Test -> X: {}, Y: {}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vital-sphere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> \n",
      " min: [0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1.], \n",
      " mean: [0.44314783 0.43423889 0.20248222 0.51939893], \n",
      " std: [0.33189283 0.16799393 0.2800556  0.10925625]\n",
      "\n",
      "Test -> \n",
      " min: [1.61530929e-03 1.42088729e-02 7.84923941e-05 7.75847940e-02], \n",
      " max: [0.99812018 0.9863905  0.99955849 1.01841838], \n",
      " mean: [0.44250961 0.44228214 0.20340597 0.52366693], \n",
      " std: [0.33010356 0.17249585 0.28115103 0.11921901]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = SCALER.fit(X_train)\n",
    "\n",
    "X_train_norm = s.transform(X_train)\n",
    "X_test_norm = s.transform(X_test)\n",
    "\n",
    "print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "print('Test -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_test_norm, axis=0), np.max(X_test_norm, axis=0), np.mean(X_test_norm, axis=0), np.std(X_test_norm, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "grave-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(save_dir + '/X_train_' + DATA_ID + '.npy', X_train_norm)    \n",
    "np.save(save_dir + '/X_test_' + DATA_ID + '.npy', X_test_norm)    \n",
    "np.save(save_dir + '/Y_train_' + DATA_ID + '.npy', Y_train)    \n",
    "np.save(save_dir + '/Y_test_' + DATA_ID + '.npy', Y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sitting-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV folds (6): [[53, 41, 3, 60, 33, 58, 27, 5, 7], [44, 49, 28, 23, 29, 46, 12, 57, 0], [61, 1, 43, 40, 14, 15, 17, 62, 20], [36, 10, 47, 11, 35, 52, 21, 4, 42], [51, 9, 38, 34, 59, 39, 6, 45, 18], [8, 55, 13, 37, 22, 30, 19, 50, 25]]\n",
      "\n",
      "Fold 1\n",
      "Train experiments ids (45): [44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [53, 41, 3, 60, 33, 58, 27, 5, 7]\n",
      "Train -> X: (380017, 4), Y: (380017, 24)\n",
      "Validation -> X: (78137, 4), Y: (78137, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1.], \n",
      " mean: [0.44351394 0.4348919  0.20254859 0.52083614], \n",
      " std: [0.33169423 0.16939104 0.28045827 0.11047305]\n",
      "\n",
      "Valid -> \n",
      " min: [1.23505811e-03 3.29613530e-02 5.34630151e-05 2.88845572e-01], \n",
      " max: [0.99730809 0.97821952 0.99704595 0.98187542], \n",
      " mean: [0.44136729 0.43106301 0.20215944 0.51240907], \n",
      " std: [0.33285127 0.16098862 0.27808865 0.10284793]\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [44, 49, 28, 23, 29, 46, 12, 57, 0]\n",
      "Train -> X: (385544, 4), Y: (385544, 24)\n",
      "Validation -> X: (72610, 4), Y: (72610, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1.], \n",
      " mean: [0.4429157  0.43405403 0.20249231 0.51887621], \n",
      " std: [0.33202332 0.16824115 0.28012007 0.10717508]\n",
      "\n",
      "Valid -> \n",
      " min: [3.29566323e-03 2.66335462e-02 6.89702298e-05 2.38260418e-01], \n",
      " max: [0.99782974 0.99880568 0.99887835 0.99528097], \n",
      " mean: [0.44438039 0.43522048 0.20242868 0.52217445], \n",
      " std: [0.33119637 0.16667169 0.279713   0.11966395]\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [61, 1, 43, 40, 14, 15, 17, 62, 20]\n",
      "Train -> X: (383101, 4), Y: (383101, 24)\n",
      "Validation -> X: (75053, 4), Y: (75053, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1.], \n",
      " mean: [0.44331013 0.43259037 0.20257213 0.37071479], \n",
      " std: [0.33187351 0.16743826 0.28052873 0.14426993]\n",
      "\n",
      "Valid -> \n",
      " min: [-5.22643976e-05  3.89815849e-02  4.33488119e-05 -3.12784610e-01], \n",
      " max: [0.99655716 0.99807146 0.99559885 0.99279099], \n",
      " mean: [0.44214174 0.44265361 0.2020233  0.36070061], \n",
      " std: [0.33209556 0.17055408 0.2776275  0.13876134]\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [36, 10, 47, 11, 35, 52, 21, 4, 42]\n",
      "Train -> X: (379675, 4), Y: (379675, 24)\n",
      "Validation -> X: (78479, 4), Y: (78479, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1.], \n",
      " mean: [0.44327657 0.43034093 0.20270195 0.51891719], \n",
      " std: [0.3316942  0.16927576 0.28034508 0.10952784]\n",
      "\n",
      "Valid -> \n",
      " min: [ 5.22616662e-05 -9.70521888e-03  5.82270080e-05  2.84362561e-01], \n",
      " max: [0.99946462 0.99046971 0.99569935 0.98032619], \n",
      " mean: [0.442525   0.42104186 0.20141922 0.52172954], \n",
      " std: [0.3328514  0.17109146 0.27864839 0.10790231]\n",
      "\n",
      "\n",
      "Fold 5\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [51, 9, 38, 34, 59, 39, 6, 45, 18]\n",
      "Train -> X: (382765, 4), Y: (382765, 24)\n",
      "Validation -> X: (75389, 4), Y: (75389, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1.], \n",
      " mean: [0.44294028 0.43329918 0.20239383 0.52114779], \n",
      " std: [0.33209009 0.16818485 0.27973065 0.10950807]\n",
      "\n",
      "Valid -> \n",
      " min: [ 4.03098395e-03  2.96423832e-02 -4.33846801e-05  2.94241892e-01], \n",
      " max: [0.99756439 1.00119575 1.00078405 1.00474141], \n",
      " mean: [0.4442016  0.44216555 0.20368554 0.52548581], \n",
      " std: [0.33088749 0.16805006 0.28309431 0.11104525]\n",
      "\n",
      "\n",
      "Fold 6\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18]\n",
      "Validation experiments ids (9): [8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Train -> X: (379668, 4), Y: (379668, 24)\n",
      "Validation -> X: (78486, 4), Y: (78486, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1.], \n",
      " mean: [0.44314174 0.43532795 0.20230921 0.51842827], \n",
      " std: [0.33217296 0.16722457 0.27937845 0.1094463 ]\n",
      "\n",
      "Valid -> \n",
      " min: [1.24188488e-02 9.61193297e-03 6.40361673e-05 2.74098057e-01], \n",
      " max: [1.00053567 0.99754101 0.9992166  0.95180024], \n",
      " mean: [0.44456298 0.42897071 0.20331914 0.52409437], \n",
      " std: [0.33157254 0.17156938 0.28330686 0.10820932]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the experiments of the training sets in different folds\n",
    "exp_per_fold = len(train_experiments) // CV\n",
    "cv_folds = [train_experiments[x:x+exp_per_fold] for x in range(0, len(train_experiments), exp_per_fold)]\n",
    "print('CV folds ({}): {}\\n'.format(len(cv_folds), cv_folds))\n",
    "\n",
    "for fold_id in range(CV):\n",
    "    print('Fold {}'.format(fold_id + 1))\n",
    "    \n",
    "    cv_folds_cp = cv_folds.copy()\n",
    "    valid_experiments_fold = cv_folds_cp.pop(fold_id)\n",
    "    train_experiments_fold = [item for sublist in cv_folds_cp for item in sublist]\n",
    "\n",
    "    print('Train experiments ids ({}): {}'.format(len(train_experiments_fold), train_experiments_fold))\n",
    "    print('Validation experiments ids ({}): {}'.format(len(valid_experiments_fold), valid_experiments_fold))\n",
    "\n",
    "    assert(len(train_experiments_fold) + len(valid_experiments_fold) == len(train_experiments))\n",
    "    # Check that no validation experiments are in train\n",
    "    assert(not any([i in valid_experiments_fold for i in train_experiments_fold]))\n",
    "    # Check that no test experiments are in train or validation folds\n",
    "    assert(not any([i in test_experiments for i in train_experiments_fold]))\n",
    "    assert(not any([i in test_experiments for i in valid_experiments_fold]))\n",
    "    \n",
    "    X_train = np.concatenate([features_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    Y_train = np.concatenate([targets_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    X_valid = np.concatenate([features_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "    Y_valid = np.concatenate([targets_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "\n",
    "    print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "    print('Validation -> X: {}, Y: {}'.format(X_valid.shape, Y_valid.shape))\n",
    "    \n",
    "    # Normalize the data\n",
    "    s = SCALER.fit(X_train)\n",
    "\n",
    "    X_train_norm = s.transform(X_train)\n",
    "    X_valid_norm =  s.transform(X_valid)\n",
    "\n",
    "    print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "    print('Valid -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}'.format(np.min(X_valid_norm, axis=0), np.max(X_valid_norm, axis=0), np.mean(X_valid_norm, axis=0), np.std(X_valid_norm, axis=0)))\n",
    "    \n",
    "    save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    np.save(save_dir + '/X_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_train_norm)    \n",
    "    np.save(save_dir + '/X_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_valid_norm)    \n",
    "    np.save(save_dir + '/Y_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_train)    \n",
    "    np.save(save_dir + '/Y_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_valid)      \n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-franchise",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
