{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intelligent-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-fusion",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "paperback-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Directory where the derived data is stored\n",
    "DERIVED_DATA_DIR = '../../../data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "# ID of the training and test data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0005_19042021'\n",
    "\n",
    "# Scaler to normalize the data\n",
    "SCALER = MinMaxScaler() # StandardScaler()\n",
    "\n",
    "# Total number of experiments\n",
    "N_EXPERIMENTS = 63\n",
    "# Number of folds for cross-validation\n",
    "CV = 6\n",
    "# Experiments for training set (int)\n",
    "TRAIN_SIZE = 54\n",
    "# Experiments for test set (int)\n",
    "TEST_SIZE = 9\n",
    "\n",
    "assert(TRAIN_SIZE + TEST_SIZE == N_EXPERIMENTS)\n",
    "assert(TRAIN_SIZE % CV == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-interpretation",
   "metadata": {},
   "source": [
    "## Features and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "square-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 15\n",
      "Selected features: ['LHipPos', 'LHipVel', 'LHipAcc', 'LHipTorque', 'LKneePos', 'LKneeVel', 'LKneeAcc', 'LKneeTorque', 'LAnklePos', 'LAnkleVel', 'LAnkleAcc', 'LAnkleTorque', 'LegKneePositionFiltered', 'LegKneeVelocityFiltered', 'LegKneeTorqueFiltered']\n",
      "\n",
      "\n",
      "Number of targets: 24\n",
      "Selected targets: ['F1x', 'F1y', 'F1z', 'F2x', 'F2y', 'F2z', 'F3x', 'F3y', 'F3z', 'F4x', 'F4y', 'F4z', 'F5x', 'F5y', 'F5z', 'F6x', 'F6y', 'F6z', 'F7x', 'F7y', 'F7z', 'F8x', 'F8y', 'F8z']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "refined-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_dirs_path = glob.glob(DERIVED_DATA_DIR + '/*/*')\n",
    "\n",
    "assert(len(experiments_dirs_path) == N_EXPERIMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "living-method",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Experiment 1 from 10032021\n",
      "Droping 2 data points by null features\n",
      "Experiment 0 -> X: (6072, 15), Y: (6072, 24) \n",
      "\n",
      "1 - Experiment 1 from 16022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 1 -> X: (17907, 15), Y: (17907, 24) \n",
      "\n",
      "2 - Experiment 2 from 16022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 2 -> X: (17875, 15), Y: (17875, 24) \n",
      "\n",
      "3 - Experiment 3 from 16022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 3 -> X: (17914, 15), Y: (17914, 24) \n",
      "\n",
      "4 - Experiment 4 from 16022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 4 -> X: (17960, 15), Y: (17960, 24) \n",
      "\n",
      "5 - Experiment 5 from 16022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 5 -> X: (17898, 15), Y: (17898, 24) \n",
      "\n",
      "6 - Experiment 6 from 16022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 6 -> X: (17841, 15), Y: (17841, 24) \n",
      "\n",
      "7 - Experiment 2 from 17022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 7 -> X: (17914, 15), Y: (17914, 24) \n",
      "\n",
      "8 - Experiment 3 from 17022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 8 -> X: (17960, 15), Y: (17960, 24) \n",
      "\n",
      "9 - Experiment 4 from 17022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 9 -> X: (17884, 15), Y: (17884, 24) \n",
      "\n",
      "10 - Experiment 1 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 10 -> X: (17907, 15), Y: (17907, 24) \n",
      "\n",
      "11 - Experiment 10 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 11 -> X: (17947, 15), Y: (17947, 24) \n",
      "\n",
      "12 - Experiment 11 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 12 -> X: (17935, 15), Y: (17935, 24) \n",
      "\n",
      "13 - Experiment 12 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 13 -> X: (17958, 15), Y: (17958, 24) \n",
      "\n",
      "14 - Experiment 13 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 14 -> X: (11912, 15), Y: (11912, 24) \n",
      "\n",
      "15 - Experiment 14 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 15 -> X: (17930, 15), Y: (17930, 24) \n",
      "\n",
      "16 - Experiment 15 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 16 -> X: (17972, 15), Y: (17972, 24) \n",
      "\n",
      "17 - Experiment 16 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 17 -> X: (16954, 15), Y: (16954, 24) \n",
      "\n",
      "18 - Experiment 17 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 18 -> X: (17966, 15), Y: (17966, 24) \n",
      "\n",
      "19 - Experiment 18 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 19 -> X: (17944, 15), Y: (17944, 24) \n",
      "\n",
      "20 - Experiment 2 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 20 -> X: (17912, 15), Y: (17912, 24) \n",
      "\n",
      "21 - Experiment 3 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 21 -> X: (17953, 15), Y: (17953, 24) \n",
      "\n",
      "22 - Experiment 4 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 22 -> X: (17919, 15), Y: (17919, 24) \n",
      "\n",
      "23 - Experiment 5 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 23 -> X: (17946, 15), Y: (17946, 24) \n",
      "\n",
      "24 - Experiment 6 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 24 -> X: (17969, 15), Y: (17969, 24) \n",
      "\n",
      "25 - Experiment 7 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 25 -> X: (17938, 15), Y: (17938, 24) \n",
      "\n",
      "26 - Experiment 8 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 26 -> X: (17982, 15), Y: (17982, 24) \n",
      "\n",
      "27 - Experiment 9 from 19022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 27 -> X: (17907, 15), Y: (17907, 24) \n",
      "\n",
      "28 - Experiment 1 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 28 -> X: (17932, 15), Y: (17932, 24) \n",
      "\n",
      "29 - Experiment 10 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 29 -> X: (17972, 15), Y: (17972, 24) \n",
      "\n",
      "30 - Experiment 11 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 30 -> X: (17904, 15), Y: (17904, 24) \n",
      "\n",
      "31 - Experiment 12 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 31 -> X: (17911, 15), Y: (17911, 24) \n",
      "\n",
      "32 - Experiment 13 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 32 -> X: (17994, 15), Y: (17994, 24) \n",
      "\n",
      "33 - Experiment 14 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 33 -> X: (17949, 15), Y: (17949, 24) \n",
      "\n",
      "34 - Experiment 15 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 34 -> X: (17892, 15), Y: (17892, 24) \n",
      "\n",
      "35 - Experiment 16 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 35 -> X: (17978, 15), Y: (17978, 24) \n",
      "\n",
      "36 - Experiment 17 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 36 -> X: (17907, 15), Y: (17907, 24) \n",
      "\n",
      "37 - Experiment 18 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 37 -> X: (17900, 15), Y: (17900, 24) \n",
      "\n",
      "38 - Experiment 19 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 38 -> X: (11960, 15), Y: (11960, 24) \n",
      "\n",
      "39 - Experiment 2 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 39 -> X: (17972, 15), Y: (17972, 24) \n",
      "\n",
      "40 - Experiment 3 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 40 -> X: (17941, 15), Y: (17941, 24) \n",
      "\n",
      "41 - Experiment 4 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 41 -> X: (17964, 15), Y: (17964, 24) \n",
      "\n",
      "42 - Experiment 5 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 42 -> X: (17974, 15), Y: (17974, 24) \n",
      "\n",
      "43 - Experiment 6 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 43 -> X: (17939, 15), Y: (17939, 24) \n",
      "\n",
      "44 - Experiment 7 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 44 -> X: (17921, 15), Y: (17921, 24) \n",
      "\n",
      "45 - Experiment 8 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 45 -> X: (17953, 15), Y: (17953, 24) \n",
      "\n",
      "46 - Experiment 9 from 22022021\n",
      "Droping 0 data points by null features\n",
      "Experiment 46 -> X: (17984, 15), Y: (17984, 24) \n",
      "\n",
      "47 - Experiment 1 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 47 -> X: (17951, 15), Y: (17951, 24) \n",
      "\n",
      "48 - Experiment 10 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 48 -> X: (17902, 15), Y: (17902, 24) \n",
      "\n",
      "49 - Experiment 11 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 49 -> X: (17802, 15), Y: (17802, 24) \n",
      "\n",
      "50 - Experiment 12 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 50 -> X: (18076, 15), Y: (18076, 24) \n",
      "\n",
      "51 - Experiment 13 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 51 -> X: (17673, 15), Y: (17673, 24) \n",
      "\n",
      "52 - Experiment 14 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 52 -> X: (17984, 15), Y: (17984, 24) \n",
      "\n",
      "53 - Experiment 15 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 53 -> X: (17918, 15), Y: (17918, 24) \n",
      "\n",
      "54 - Experiment 16 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 54 -> X: (17918, 15), Y: (17918, 24) \n",
      "\n",
      "55 - Experiment 2 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 55 -> X: (17979, 15), Y: (17979, 24) \n",
      "\n",
      "56 - Experiment 3 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 56 -> X: (17677, 15), Y: (17677, 24) \n",
      "\n",
      "57 - Experiment 4 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 57 -> X: (17956, 15), Y: (17956, 24) \n",
      "\n",
      "58 - Experiment 5 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 58 -> X: (17746, 15), Y: (17746, 24) \n",
      "\n",
      "59 - Experiment 6 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 59 -> X: (18024, 15), Y: (18024, 24) \n",
      "\n",
      "60 - Experiment 7 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 60 -> X: (17767, 15), Y: (17767, 24) \n",
      "\n",
      "61 - Experiment 8 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 61 -> X: (18041, 15), Y: (18041, 24) \n",
      "\n",
      "62 - Experiment 9 from 24022021\n",
      "Droping 2 data points by null features\n",
      "Experiment 62 -> X: (18035, 15), Y: (18035, 24) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i, exp_path in enumerate(experiments_dirs_path):\n",
    "    print('{} - Experiment {} from {}'.format(i, exp_path.split('/')[-1], exp_path.split('/')[-2]))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "    leg_df = pd.read_csv(exp_path + '/leg_processed.csv')\n",
    "    features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    \n",
    "#     idx_aux = targets_df.duplicated(keep='first')\n",
    "#     targets_df = targets_df.loc[~idx_aux]\n",
    "#     features_df = features_df.loc[~idx_aux]\n",
    "#     print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "    \n",
    "    # Drop first row to remove noise in the start of the data recording\n",
    "    targets_df = targets_df.iloc[1:]\n",
    "    features_df = features_df.iloc[1:]\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    assert(len(features_df) == len(targets_df))\n",
    "    # Store the final array\n",
    "    targets_dict[i] = targets_df[targets].values\n",
    "    features_dict[i] = features_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-height",
   "metadata": {},
   "source": [
    "## Normalization and split for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "revolutionary-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (54): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Test experiments ids (9): [31, 32, 16, 2, 26, 56, 48, 24, 54]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(N_EXPERIMENTS))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[:TRAIN_SIZE]\n",
    "test_experiments = experiments[TRAIN_SIZE:]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "assert(len(train_experiments) + len(test_experiments) == N_EXPERIMENTS)\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alien-turning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> X: (943372, 15), Y: (943372, 24)\n",
      "Test -> X: (161200, 15), Y: (161200, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([features_dict[i] for i in train_experiments], axis=0)\n",
    "Y_train = np.concatenate([targets_dict[i] for i in train_experiments], axis=0)\n",
    "X_test = np.concatenate([features_dict[i] for i in test_experiments], axis=0)\n",
    "Y_test = np.concatenate([targets_dict[i] for i in test_experiments], axis=0)\n",
    "\n",
    "print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Test -> X: {}, Y: {}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smart-chance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44316332 0.51939702 0.50213284 0.4341789  0.2024983  0.63145772\n",
      " 0.61899083 0.51941409 0.60775526 0.56178418 0.56970183 0.77066847\n",
      " 0.26046828 0.42886819 0.67576674], \n",
      " std: [0.33186054 0.11283305 0.1592397  0.16795629 0.279992   0.07667341\n",
      " 0.11699678 0.1092075  0.23715171 0.06415873 0.09721195 0.06513208\n",
      " 0.24029363 0.0676148  0.24251379]\n",
      "\n",
      "Test -> \n",
      " min: [1.66156419e-03 1.18331027e-01 1.47064973e-02 1.42088729e-02\n",
      " 7.84836338e-05 4.08905180e-03 2.55045132e-02 7.77319844e-02\n",
      " 4.41265436e-02 3.63196507e-01 3.62820351e-01 1.00349322e-02\n",
      " 1.24428368e-02 2.54399174e-01 3.46046431e-02], \n",
      " max: [0.99830005 1.0088766  0.98112243 0.9863905  0.99926046 0.99539474\n",
      " 0.95661875 1.01847498 0.99940925 0.97938504 0.99811034 0.98918614\n",
      " 0.99778234 0.69072286 0.94985511], \n",
      " mean: [0.44285283 0.51929124 0.5011479  0.44217772 0.2034032  0.63144101\n",
      " 0.61860992 0.52355774 0.60726631 0.56183278 0.56948886 0.77657515\n",
      " 0.2598001  0.42861448 0.60923345], \n",
      " std: [0.33016752 0.11193674 0.16892587 0.17246709 0.28110863 0.07928477\n",
      " 0.11980245 0.11906045 0.23685269 0.06141152 0.10270242 0.06720046\n",
      " 0.24321833 0.06697131 0.27334255]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = SCALER.fit(X_train)\n",
    "\n",
    "X_train_norm = s.transform(X_train)\n",
    "X_test_norm = s.transform(X_test)\n",
    "\n",
    "print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "print('Test -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_test_norm, axis=0), np.max(X_test_norm, axis=0), np.mean(X_test_norm, axis=0), np.std(X_test_norm, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "democratic-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(save_dir + '/X_train_' + DATA_ID + '.npy', X_train_norm)    \n",
    "np.save(save_dir + '/X_test_' + DATA_ID + '.npy', X_test_norm)    \n",
    "np.save(save_dir + '/Y_train_' + DATA_ID + '.npy', Y_train)    \n",
    "np.save(save_dir + '/Y_test_' + DATA_ID + '.npy', Y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "industrial-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV folds (6): [[53, 41, 3, 60, 33, 58, 27, 5, 7], [44, 49, 28, 23, 29, 46, 12, 57, 0], [61, 1, 43, 40, 14, 15, 17, 62, 20], [36, 10, 47, 11, 35, 52, 21, 4, 42], [51, 9, 38, 34, 59, 39, 6, 45, 18], [8, 55, 13, 37, 22, 30, 19, 50, 25]]\n",
      "\n",
      "Fold 1\n",
      "Train experiments ids (45): [44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [53, 41, 3, 60, 33, 58, 27, 5, 7]\n",
      "Train -> X: (782395, 15), Y: (782395, 24)\n",
      "Validation -> X: (160977, 15), Y: (160977, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44348832 0.51942255 0.50165771 0.43484463 0.20263287 0.6314615\n",
      " 0.61879597 0.52085447 0.60775433 0.56177931 0.56951611 0.77066066\n",
      " 0.26131274 0.4289091  0.67072041], \n",
      " std: [0.33165048 0.11118223 0.1609837  0.16936138 0.28042794 0.07558097\n",
      " 0.11822377 0.11043009 0.23711158 0.06236006 0.09812544 0.06431239\n",
      " 0.24054351 0.06819558 0.24664755]\n",
      "\n",
      "Valid -> \n",
      " min: [1.23056088e-03 1.12680252e-01 3.09196924e-02 3.23966751e-02\n",
      " 5.34617181e-05 1.96699412e-02 3.27364521e-02 2.88959051e-01\n",
      " 6.11226129e-04 3.65305307e-01 1.60393982e-02 1.25836890e-02\n",
      " 5.41390112e-02 2.66003689e-01 3.00546927e-02], \n",
      " max: [0.99729647 0.95578355 0.9470375  0.97919101 0.99674866 0.99174474\n",
      " 0.9428639  0.98187831 0.99977036 0.98062163 0.99479907 0.90897143\n",
      " 0.9853376  0.70268589 0.95582405], \n",
      " mean: [0.44158372 0.51927288 0.50444211 0.43094325 0.20184424 0.63143933\n",
      " 0.6199379  0.5124134  0.60775982 0.56180789 0.57060448 0.77070643\n",
      " 0.25636399 0.42866938 0.70029339], \n",
      " std: [0.33287512 0.1205349  0.15045444 0.16091336 0.27786249 0.08177538\n",
      " 0.11083512 0.10277192 0.23734661 0.07226598 0.09263862 0.06897746\n",
      " 0.23903294 0.06471747 0.21968099]\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [44, 49, 28, 23, 29, 46, 12, 57, 0]\n",
      "Train -> X: (793852, 15), Y: (793852, 24)\n",
      "Validation -> X: (149520, 15), Y: (149520, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44291942 0.53019491 0.50218765 0.43398529 0.20255235 0.63145628\n",
      " 0.61900487 0.51889573 0.60772209 0.56517495 0.56971637 0.77878116\n",
      " 0.26364801 0.38751801 0.68729765], \n",
      " std: [0.33200992 0.11531945 0.159083   0.16820251 0.28007365 0.07577052\n",
      " 0.11757275 0.10712386 0.2372241  0.06463979 0.09712493 0.0652834\n",
      " 0.24182669 0.14589741 0.23335758]\n",
      "\n",
      "Valid -> \n",
      " min: [ 3.28128457e-03  1.18878036e-01  1.41485613e-02  2.65330777e-02\n",
      "  6.89643088e-05  5.61740448e-03  2.59202877e-02  2.38381970e-01\n",
      " -2.83893931e-04  8.13921610e-03  3.65676111e-01  2.18863934e-02\n",
      "  4.93952492e-02 -5.70799469e-01  3.00700956e-02], \n",
      " max: [0.99781659 1.02077477 0.97014627 0.99885256 0.99883554 0.99105077\n",
      " 0.95810867 0.99538175 0.99872243 1.00602938 0.99377449 1.01086877\n",
      " 1.00614657 1.66363317 0.95296162], \n",
      " mean: [0.44445829 0.53014736 0.50184185 0.43520684 0.20221134 0.63146538\n",
      " 0.6189163  0.52216624 0.60722879 0.5651525  0.56962462 0.78044381\n",
      " 0.25368721 0.38726322 0.61454529], \n",
      " std: [0.33106332 0.11441856 0.16006882 0.16663919 0.27955787 0.08129934\n",
      " 0.11388996 0.1196265  0.2371917  0.06404293 0.09767259 0.06870271\n",
      " 0.2412997  0.1760629  0.27837503]\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [61, 1, 43, 40, 14, 15, 17, 62, 20]\n",
      "Train -> X: (788801, 15), Y: (788801, 24)\n",
      "Validation -> X: (154571, 15), Y: (154571, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44334826 0.51936758 0.50224212 0.43254095 0.20252627 0.63146634\n",
      " 0.61903714 0.37063479 0.60783331 0.56177153 0.5697184  0.76826128\n",
      " 0.23569178 0.42904085 0.69581817], \n",
      " std: [0.3318557  0.11285967 0.15724833 0.16740289 0.28043092 0.07894592\n",
      " 0.11562259 0.14423202 0.23730128 0.06449372 0.09649252 0.06537238\n",
      " 0.24807091 0.0681171  0.25713862]\n",
      "\n",
      "Valid -> \n",
      " min: [-5.20103235e-05  6.90422074e-02  1.89530300e-02  3.86197794e-02\n",
      "  4.33505306e-05  1.80972352e-01  5.34470412e-03 -3.12994126e-01\n",
      "  5.74463938e-02  3.66653725e-01  3.61508738e-01 -1.27440563e-02\n",
      " -3.18904137e-02  2.72353729e-01  3.19584906e-02], \n",
      " max: [0.99651479 0.93040128 0.98857152 0.99815936 0.9954434  0.94294951\n",
      " 0.93207934 0.99299302 0.99940988 0.97186081 0.99769303 0.91155079\n",
      " 0.97647725 0.69272092 1.03914197], \n",
      " mean: [0.44204279 0.5195472  0.50157516 0.44253765 0.20235558 0.63141374\n",
      " 0.61875449 0.36061787 0.60735699 0.56184874 0.56961725 0.76511555\n",
      " 0.24297002 0.4279871  0.73487483], \n",
      " std: [0.33198845 0.11269699 0.16903595 0.17050764 0.27774127 0.06382861\n",
      " 0.12377186 0.13870405 0.23638651 0.06242117 0.10080334 0.06883306\n",
      " 0.24728345 0.0649839  0.22112561]\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [36, 10, 47, 11, 35, 52, 21, 4, 42]\n",
      "Train -> X: (781811, 15), Y: (781811, 24)\n",
      "Validation -> X: (161561, 15), Y: (161561, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44334995 0.51939753 0.50222635 0.43027394 0.202724   0.63096181\n",
      " 0.61903548 0.51892627 0.60764458 0.56179838 0.56407682 0.77116903\n",
      " 0.25895864 0.42872546 0.64686428], \n",
      " std: [0.3316533  0.11337371 0.15935392 0.16922974 0.28030053 0.07778885\n",
      " 0.1169549  0.10947744 0.23713516 0.06471532 0.09900563 0.06584889\n",
      " 0.24083609 0.06840625 0.25105668]\n",
      "\n",
      "Valid -> \n",
      " min: [ 5.20076186e-05  1.17111826e-01  2.45885208e-02 -9.70521888e-03\n",
      "  5.82242904e-05 -1.31556400e-03  1.67098632e-02  2.84197032e-01\n",
      "  5.29828370e-04  3.72540778e-01 -1.63391625e-02  3.32720540e-02\n",
      "  4.21419392e-02  2.72929358e-01 -3.04271362e-02], \n",
      " max: [0.9994646  0.95644724 0.95965497 0.99089773 0.99540247 0.98546455\n",
      " 0.99855546 0.98039404 0.99993962 0.98299396 1.00235009 0.98924809\n",
      " 0.99389097 0.70299701 0.96118629], \n",
      " mean: [0.44226018 0.51939453 0.50168032 0.42101042 0.20140612 0.63102643\n",
      " 0.61877476 0.52177469 0.60829089 0.56171549 0.56368647 0.76824618\n",
      " 0.26777362 0.4295589  0.75802322], \n",
      " std: [0.33286011 0.1101793  0.15868504 0.17109383 0.27849156 0.07166195\n",
      " 0.11719898 0.10786055 0.23723102 0.06139403 0.09914    0.06148803\n",
      " 0.23751561 0.06364147 0.2222269 ]\n",
      "\n",
      "\n",
      "Fold 5\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Validation experiments ids (9): [51, 9, 38, 34, 59, 39, 6, 45, 18]\n",
      "Train -> X: (788207, 15), Y: (788207, 24)\n",
      "Validation -> X: (155165, 15), Y: (155165, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44292508 0.48375041 0.5080356  0.43320413 0.20246636 0.63197172\n",
      " 0.61698194 0.52110972 0.60780322 0.56177771 0.56972788 0.7704403\n",
      " 0.26044556 0.42883525 0.68692009], \n",
      " std: [0.33204181 0.12156358 0.16147326 0.16813933 0.27975225 0.07594256\n",
      " 0.117492   0.10944482 0.23715279 0.06446691 0.09722162 0.06609323\n",
      " 0.23995946 0.06778678 0.24395054]\n",
      "\n",
      "Valid -> \n",
      " min: [ 4.07784448e-03 -7.41625538e-02  6.32067336e-02  2.96409917e-02\n",
      " -4.33993410e-05  1.15279346e-02 -5.37342347e-03  2.93577899e-01\n",
      "  5.49782753e-02  3.08189799e-01  3.85669611e-01  4.45007793e-02\n",
      "  5.76959592e-02  2.55456109e-01  2.95286635e-02], \n",
      " max: [0.99752193 0.97813868 1.0115606  1.00114876 1.00108255 1.00082268\n",
      " 0.94884555 1.00463967 1.00006038 0.97979375 0.96112268 0.95783526\n",
      " 0.98738692 0.69156682 0.95388608], \n",
      " mean: [0.44437354 0.48377386 0.50744094 0.44216292 0.2037829  0.63200511\n",
      " 0.6167483  0.5254524  0.60773476 0.56181705 0.56956948 0.77182748\n",
      " 0.26058373 0.42903554 0.61910998], \n",
      " std: [0.33093557 0.1193424  0.15907013 0.16800006 0.28310718 0.08064885\n",
      " 0.11830084 0.11100152 0.23723326 0.06256975 0.0971627  0.05999902\n",
      " 0.24198398 0.06673407 0.22676119]\n",
      "\n",
      "\n",
      "Fold 6\n",
      "Train experiments ids (45): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18]\n",
      "Validation experiments ids (9): [8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Train -> X: (781794, 15), Y: (781794, 24)\n",
      "Validation -> X: (161578, 15), Y: (161578, 24)\n",
      "Train -> \n",
      " min: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], \n",
      " max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], \n",
      " mean: [0.44316075 0.51939676 0.49510793 0.43527456 0.20227289 0.63146329\n",
      " 0.6199372  0.51844237 0.60769743 0.55821634 0.56976431 0.77015695\n",
      " 0.26072851 0.42881065 0.68255217], \n",
      " std: [0.33214294 0.11342026 0.16141735 0.1671856  0.27927769 0.0761171\n",
      " 0.11689847 0.1093967  0.23706556 0.06515441 0.09711765 0.0653846\n",
      " 0.23965348 0.0678669  0.23834156]\n",
      "\n",
      "Valid -> \n",
      " min: [ 1.24537050e-02  1.19780688e-01 -1.43516161e-02  9.61193297e-03\n",
      "  6.40160375e-05  1.31383557e-03  2.38409049e-02  2.73632866e-01\n",
      "  2.83813359e-04 -8.15642480e-03  3.94649651e-02  2.13998873e-02\n",
      "  3.09048454e-02  2.56543336e-01  3.02593961e-02], \n",
      " max: [1.00053568 0.94193722 0.96145666 0.99809451 0.99891867 0.99917799\n",
      " 1.00144663 0.95221102 0.99982889 0.99395787 0.9805479  0.98548236\n",
      " 0.98916314 0.69837066 0.94913311], \n",
      " mean: [0.44456176 0.51939825 0.49440563 0.42887757 0.20358893 0.63143078\n",
      " 0.61963994 0.52411572 0.6080351  0.5581788  0.56939952 0.77314344\n",
      " 0.25920917 0.42914661 0.64293552], \n",
      " std: [0.33152917 0.10994756 0.16204386 0.1715376  0.28342021 0.07930999\n",
      " 0.1184518  0.10816418 0.23756787 0.06234587 0.09766635 0.06383832\n",
      " 0.24336325 0.0663808  0.25926718]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the experiments of the training sets in different folds\n",
    "exp_per_fold = len(train_experiments) // CV\n",
    "cv_folds = [train_experiments[x:x+exp_per_fold] for x in range(0, len(train_experiments), exp_per_fold)]\n",
    "print('CV folds ({}): {}\\n'.format(len(cv_folds), cv_folds))\n",
    "\n",
    "for fold_id in range(CV):\n",
    "    print('Fold {}'.format(fold_id + 1))\n",
    "    \n",
    "    cv_folds_cp = cv_folds.copy()\n",
    "    valid_experiments_fold = cv_folds_cp.pop(fold_id)\n",
    "    train_experiments_fold = [item for sublist in cv_folds_cp for item in sublist]\n",
    "\n",
    "    print('Train experiments ids ({}): {}'.format(len(train_experiments_fold), train_experiments_fold))\n",
    "    print('Validation experiments ids ({}): {}'.format(len(valid_experiments_fold), valid_experiments_fold))\n",
    "\n",
    "    assert(len(train_experiments_fold) + len(valid_experiments_fold) == len(train_experiments))\n",
    "    # Check that no validation experiments are in train\n",
    "    assert(not any([i in valid_experiments_fold for i in train_experiments_fold]))\n",
    "    # Check that no test experiments are in train or validation folds\n",
    "    assert(not any([i in test_experiments for i in train_experiments_fold]))\n",
    "    assert(not any([i in test_experiments for i in valid_experiments_fold]))\n",
    "    \n",
    "    X_train = np.concatenate([features_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    Y_train = np.concatenate([targets_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    X_valid = np.concatenate([features_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "    Y_valid = np.concatenate([targets_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "\n",
    "    print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "    print('Validation -> X: {}, Y: {}'.format(X_valid.shape, Y_valid.shape))\n",
    "    \n",
    "    # Normalize the data\n",
    "    s = SCALER.fit(X_train)\n",
    "\n",
    "    X_train_norm = s.transform(X_train)\n",
    "    X_valid_norm =  s.transform(X_valid)\n",
    "\n",
    "    print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "    print('Valid -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}'.format(np.min(X_valid_norm, axis=0), np.max(X_valid_norm, axis=0), np.mean(X_valid_norm, axis=0), np.std(X_valid_norm, axis=0)))\n",
    "    \n",
    "    save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    np.save(save_dir + '/X_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_train_norm)    \n",
    "    np.save(save_dir + '/X_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_valid_norm)    \n",
    "    np.save(save_dir + '/Y_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_train)    \n",
    "    np.save(save_dir + '/Y_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_valid)      \n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-genesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
