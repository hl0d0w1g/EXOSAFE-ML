{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "logical-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-december",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "missing-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Directory where the derived data is stored\n",
    "DERIVED_DATA_DIR = '../../../data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "# ID of the training and test data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0011_09082021'\n",
    "\n",
    "# Scaler to normalize the data\n",
    "SCALER = MinMaxScaler() # StandardScaler()\n",
    "\n",
    "# Total number of experiments\n",
    "N_EXPERIMENTS = 63\n",
    "# Number of folds for cross-validation\n",
    "CV = 6\n",
    "# Experiments for training set (int)\n",
    "TRAIN_SIZE = 54\n",
    "# Experiments for test set (int)\n",
    "TEST_SIZE = 9\n",
    "\n",
    "assert(TRAIN_SIZE + TEST_SIZE == N_EXPERIMENTS)\n",
    "assert(TRAIN_SIZE % CV == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "healthy-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_dirs_path = glob.glob(DERIVED_DATA_DIR + '/*/*')\n",
    "assert(len(experiments_dirs_path) == N_EXPERIMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-round",
   "metadata": {},
   "source": [
    "## Experiment selection (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "future-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_dirs_path_filter = []\n",
    "\n",
    "exp_params = {}\n",
    "for exp_path in experiments_dirs_path:\n",
    "    with open(exp_path + '/parameters.json') as f:\n",
    "        exp_params[exp_path] = json.load(f)\n",
    "        \n",
    "    if int(exp_params[exp_path]['SkinConfig']) == 0 or exp_params[exp_path]['SkinConfig'] == 'NaN':\n",
    "        experiments_dirs_path_filter.append(exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atlantic-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of experiments: 40\n"
     ]
    }
   ],
   "source": [
    "print('Final number of experiments:', len(experiments_dirs_path_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dried-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the split of the data for the filtered experiments\n",
    "\n",
    "# Total number of experiments\n",
    "N_EXPERIMENTS = 40\n",
    "# Number of folds for cross-validation\n",
    "CV = 4\n",
    "# Experiments for training set (int)\n",
    "TRAIN_SIZE = 32\n",
    "# Experiments for test set (int)\n",
    "TEST_SIZE = 8\n",
    "\n",
    "assert(TRAIN_SIZE + TEST_SIZE == N_EXPERIMENTS)\n",
    "assert(TRAIN_SIZE % CV == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "other-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_dirs_path = experiments_dirs_path_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-rendering",
   "metadata": {},
   "source": [
    "## Features and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "recorded-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3\n",
      "Selected features: ['LAnklePos', 'F1z', 'F2z']\n",
      "\n",
      "\n",
      "Number of targets: 4\n",
      "Selected targets: ['F1x', 'F1y', 'F2x', 'F2y']\n"
     ]
    }
   ],
   "source": [
    "JOINT = 'Ankle'\n",
    "FORCE_CELLS_PER_JOINT = {\n",
    "    'Hip': [5, 6],\n",
    "    'Knee': [3, 4, 7, 8],\n",
    "    'Ankle': [1, 2]\n",
    "}\n",
    "\n",
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "# features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "# targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "features = ['L{}Pos'.format(JOINT)] + ['F' + str(i) + 'z' for i in FORCE_CELLS_PER_JOINT[JOINT]]\n",
    "targets = ['F' + str(i) + ax for i in FORCE_CELLS_PER_JOINT[JOINT] for ax in ['x', 'y']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "german-writing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Experiment 1 from 10032021\n",
      "Droping 2917 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 0 -> X: (146, 3), Y: (146, 4) \n",
      "\n",
      "1 - Experiment 1 from 16022021\n",
      "Droping 8709 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 1 -> X: (435, 3), Y: (435, 4) \n",
      "\n",
      "2 - Experiment 2 from 16022021\n",
      "Droping 8696 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 2 -> X: (435, 3), Y: (435, 4) \n",
      "\n",
      "3 - Experiment 3 from 16022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 3 -> X: (435, 3), Y: (435, 4) \n",
      "\n",
      "4 - Experiment 2 from 17022021\n",
      "Droping 8711 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 4 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "5 - Experiment 1 from 19022021\n",
      "Droping 8697 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 5 -> X: (435, 3), Y: (435, 4) \n",
      "\n",
      "6 - Experiment 10 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 6 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "7 - Experiment 11 from 19022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 7 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "8 - Experiment 14 from 19022021\n",
      "Droping 8718 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 8 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "9 - Experiment 15 from 19022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 9 -> X: (437, 3), Y: (437, 4) \n",
      "\n",
      "10 - Experiment 18 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 10 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "11 - Experiment 2 from 19022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 11 -> X: (435, 3), Y: (435, 4) \n",
      "\n",
      "12 - Experiment 3 from 19022021\n",
      "Droping 8734 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 12 -> X: (437, 3), Y: (437, 4) \n",
      "\n",
      "13 - Experiment 4 from 19022021\n",
      "Droping 8717 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 13 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "14 - Experiment 5 from 19022021\n",
      "Droping 8730 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 14 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "15 - Experiment 6 from 19022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 15 -> X: (437, 3), Y: (437, 4) \n",
      "\n",
      "16 - Experiment 9 from 19022021\n",
      "Droping 8704 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 16 -> X: (435, 3), Y: (435, 4) \n",
      "\n",
      "17 - Experiment 1 from 22022021\n",
      "Droping 8715 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 17 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "18 - Experiment 14 from 22022021\n",
      "Droping 8727 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 18 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "19 - Experiment 15 from 22022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 19 -> X: (435, 3), Y: (435, 4) \n",
      "\n",
      "20 - Experiment 2 from 22022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 20 -> X: (437, 3), Y: (437, 4) \n",
      "\n",
      "21 - Experiment 3 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 21 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "22 - Experiment 8 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 22 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "23 - Experiment 9 from 22022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 23 -> X: (437, 3), Y: (437, 4) \n",
      "\n",
      "24 - Experiment 1 from 24022021\n",
      "Droping 8690 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 24 -> X: (434, 3), Y: (434, 4) \n",
      "\n",
      "25 - Experiment 10 from 24022021\n",
      "Droping 8519 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 25 -> X: (426, 3), Y: (426, 4) \n",
      "\n",
      "26 - Experiment 11 from 24022021\n",
      "Droping 8627 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 26 -> X: (431, 3), Y: (431, 4) \n",
      "\n",
      "27 - Experiment 12 from 24022021\n",
      "Droping 8744 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 27 -> X: (437, 3), Y: (437, 4) \n",
      "\n",
      "28 - Experiment 13 from 24022021\n",
      "Droping 8569 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 28 -> X: (428, 3), Y: (428, 4) \n",
      "\n",
      "29 - Experiment 14 from 24022021\n",
      "Droping 8729 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 29 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "30 - Experiment 15 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 30 -> X: (434, 3), Y: (434, 4) \n",
      "\n",
      "31 - Experiment 16 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 31 -> X: (434, 3), Y: (434, 4) \n",
      "\n",
      "32 - Experiment 2 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 32 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "33 - Experiment 3 from 24022021\n",
      "Droping 8543 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 33 -> X: (427, 3), Y: (427, 4) \n",
      "\n",
      "34 - Experiment 4 from 24022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 34 -> X: (436, 3), Y: (436, 4) \n",
      "\n",
      "35 - Experiment 5 from 24022021\n",
      "Droping 8568 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 35 -> X: (428, 3), Y: (428, 4) \n",
      "\n",
      "36 - Experiment 6 from 24022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 36 -> X: (437, 3), Y: (437, 4) \n",
      "\n",
      "37 - Experiment 7 from 24022021\n",
      "Droping 8603 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 37 -> X: (430, 3), Y: (430, 4) \n",
      "\n",
      "38 - Experiment 8 from 24022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 38 -> X: (437, 3), Y: (437, 4) \n",
      "\n",
      "39 - Experiment 9 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 39 -> X: (436, 3), Y: (436, 4) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index to crop the data and use only this section of each experiment (start idx, end idx)\n",
    "# The indexes can be defined manually defining crop_by_index=(start idx, end idx) or seleted at random setting crop_by_index=True\n",
    "window_size = 200\n",
    "crop_by_index = False #(1500, 1700) # True\n",
    "\n",
    "# Sample the experiment data to use only this sample datapoints\n",
    "random_sample = True\n",
    "random_sample_pct = 0.05\n",
    "\n",
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i, exp_path in enumerate(experiments_dirs_path):\n",
    "    print('{} - Experiment {} from {}'.format(i, exp_path.split('/')[-1], exp_path.split('/')[-2]))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "    # leg_df = pd.read_csv(exp_path + '/leg_processed.csv')\n",
    "    # features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    features_df = exo_df\n",
    "    \n",
    "    idx_aux = targets_df.duplicated(keep='first')\n",
    "    targets_df = targets_df.loc[~idx_aux]\n",
    "    features_df = features_df.loc[~idx_aux]\n",
    "    print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "    \n",
    "    # Drop first row to remove noise in the start of the data recording\n",
    "    targets_df = targets_df.iloc[1:]\n",
    "    features_df = features_df.iloc[1:]\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    assert(len(features_df) == len(targets_df))\n",
    "    data_df = pd.concat([features_df, targets_df], axis=1)\n",
    "    \n",
    "    # Crop the data by the indicated indexes\n",
    "    if crop_by_index:\n",
    "        if crop_by_index == True:\n",
    "            start_idx = random.randint(100, len(data_df) - window_size - 100)\n",
    "            crop_by_index = (start_idx, start_idx + window_size)\n",
    "            \n",
    "        data_df = data_df.iloc[crop_by_index[0]:crop_by_index[1]]\n",
    "        \n",
    "    if random_sample:\n",
    "        data_df = data_df.sample(frac=random_sample_pct, random_state=0)\n",
    "        \n",
    "    # Store the final array\n",
    "    targets_dict[i] = data_df[targets].values\n",
    "    features_dict[i] = data_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-thompson",
   "metadata": {},
   "source": [
    "## Normalization and split for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moral-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (32): [0, 27, 12, 21, 29, 15, 38, 34, 1, 20, 7, 5, 10, 13, 14, 37, 22, 39, 23, 17, 8, 35, 3, 33, 9, 4, 36, 6, 28, 18, 11, 30]\n",
      "Test experiments ids (8): [19, 25, 31, 32, 16, 2, 26, 24]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(N_EXPERIMENTS))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[:TRAIN_SIZE]\n",
    "test_experiments = experiments[TRAIN_SIZE:]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "assert(len(train_experiments) + len(test_experiments) == N_EXPERIMENTS)\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "international-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> X: (13633, 3), Y: (13633, 4)\n",
      "Test -> X: (3466, 3), Y: (3466, 4)\n",
      "Total data points: 17099\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([features_dict[i] for i in train_experiments], axis=0)\n",
    "Y_train = np.concatenate([targets_dict[i] for i in train_experiments], axis=0)\n",
    "X_test = np.concatenate([features_dict[i] for i in test_experiments], axis=0)\n",
    "Y_test = np.concatenate([targets_dict[i] for i in test_experiments], axis=0)\n",
    "\n",
    "print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Test -> X: {}, Y: {}'.format(X_test.shape, Y_test.shape))\n",
    "print('Total data points: {}'.format(X_train.shape[0] + X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "royal-disability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> \n",
      " min: [0. 0. 0.], \n",
      " max: [1. 1. 1.], \n",
      " mean: [0.60676458 0.85645142 0.20431342], \n",
      " std: [0.23579962 0.14214477 0.12623251]\n",
      "\n",
      "Test -> \n",
      " min: [ 0.00507874  0.00639473 -0.05341807], \n",
      " max: [1.00020325 0.97568065 0.75987633], \n",
      " mean: [0.60369541 0.83270744 0.23239737], \n",
      " std: [0.23922545 0.12833765 0.12092265]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = SCALER.fit(X_train)\n",
    "\n",
    "X_train_norm = s.transform(X_train)\n",
    "X_test_norm = s.transform(X_test)\n",
    "\n",
    "print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "print('Test -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_test_norm, axis=0), np.max(X_test_norm, axis=0), np.mean(X_test_norm, axis=0), np.std(X_test_norm, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suffering-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(save_dir + '/' + JOINT + '_X_train_' + DATA_ID + '.npy', X_train_norm)    \n",
    "np.save(save_dir + '/' + JOINT + '_X_test_' + DATA_ID + '.npy', X_test_norm)    \n",
    "np.save(save_dir + '/' + JOINT + '_Y_train_' + DATA_ID + '.npy', Y_train)    \n",
    "np.save(save_dir + '/' + JOINT + '_Y_test_' + DATA_ID + '.npy', Y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "experienced-kansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV folds (4): [[0, 27, 12, 21, 29, 15, 38, 34], [1, 20, 7, 5, 10, 13, 14, 37], [22, 39, 23, 17, 8, 35, 3, 33], [9, 4, 36, 6, 28, 18, 11, 30]]\n",
      "\n",
      "Fold 1\n",
      "Train experiments ids (24): [1, 20, 7, 5, 10, 13, 14, 37, 22, 39, 23, 17, 8, 35, 3, 33, 9, 4, 36, 6, 28, 18, 11, 30]\n",
      "Validation experiments ids (8): [0, 27, 12, 21, 29, 15, 38, 34]\n",
      "Train -> X: (10431, 3), Y: (10431, 4)\n",
      "Validation -> X: (3202, 3), Y: (3202, 4)\n",
      "Train -> \n",
      " min: [0. 0. 0.], \n",
      " max: [1. 1. 1.], \n",
      " mean: [0.6029747  0.83271775 0.1924386 ], \n",
      " std: [0.23844401 0.17212837 0.12258567]\n",
      "\n",
      "Valid -> \n",
      " min: [-0.0105173  -0.21309356  0.03870377], \n",
      " max: [0.99924993 0.99018571 0.81569907], \n",
      " mean: [0.601502   0.80352896 0.24299746], \n",
      " std: [0.23773973 0.17154007 0.13013385]\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train experiments ids (24): [0, 27, 12, 21, 29, 15, 38, 34, 22, 39, 23, 17, 8, 35, 3, 33, 9, 4, 36, 6, 28, 18, 11, 30]\n",
      "Validation experiments ids (8): [1, 20, 7, 5, 10, 13, 14, 37]\n",
      "Train -> X: (10152, 3), Y: (10152, 4)\n",
      "Validation -> X: (3481, 3), Y: (3481, 4)\n",
      "Train -> \n",
      " min: [0. 0. 0.], \n",
      " max: [1. 1. 1.], \n",
      " mean: [0.6066569  0.84081988 0.21024414], \n",
      " std: [0.2357157  0.14754762 0.1347667 ]\n",
      "\n",
      "Valid -> \n",
      " min: [0.05763266 0.21175274 0.0056109 ], \n",
      " max: [1.00003455 0.99607596 0.41497767], \n",
      " mean: [0.60716074 0.9020393  0.18701705], \n",
      " std: [0.23607567 0.11335617 0.09506235]\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train experiments ids (24): [0, 27, 12, 21, 29, 15, 38, 34, 1, 20, 7, 5, 10, 13, 14, 37, 9, 4, 36, 6, 28, 18, 11, 30]\n",
      "Validation experiments ids (8): [22, 39, 23, 17, 8, 35, 3, 33]\n",
      "Train -> X: (10162, 3), Y: (10162, 4)\n",
      "Validation -> X: (3471, 3), Y: (3471, 4)\n",
      "Train -> \n",
      " min: [0. 0. 0.], \n",
      " max: [1. 1. 1.], \n",
      " mean: [0.60656748 0.86021541 0.21179107], \n",
      " std: [0.23571695 0.14530433 0.1332409 ]\n",
      "\n",
      "Valid -> \n",
      " min: [0.01040784 0.28505423 0.01093553], \n",
      " max: [0.9999125 1.0039395 0.4545975], \n",
      " mean: [0.60734164 0.8586836  0.18242121], \n",
      " std: [0.23604054 0.13479957 0.09983837]\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train experiments ids (24): [0, 27, 12, 21, 29, 15, 38, 34, 1, 20, 7, 5, 10, 13, 14, 37, 22, 39, 23, 17, 8, 35, 3, 33]\n",
      "Validation experiments ids (8): [9, 4, 36, 6, 28, 18, 11, 30]\n",
      "Train -> X: (10154, 3), Y: (10154, 4)\n",
      "Validation -> X: (3479, 3), Y: (3479, 4)\n",
      "Train -> \n",
      " min: [0. 0. 0.], \n",
      " max: [1. 1. 1.], \n",
      " mean: [0.60673883 0.86588559 0.24378605], \n",
      " std: [0.2358068  0.13268129 0.13846821]\n",
      "\n",
      "Valid -> \n",
      " min: [ 0.0544834   0.17566127 -0.00692628], \n",
      " max: [0.99996545 0.99336305 1.22750724], \n",
      " mean: [0.60683975 0.82891633 0.24966013], \n",
      " std: [0.23577865 0.16364001 0.19790138]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the experiments of the training sets in different folds\n",
    "exp_per_fold = len(train_experiments) // CV\n",
    "cv_folds = [train_experiments[x:x+exp_per_fold] for x in range(0, len(train_experiments), exp_per_fold)]\n",
    "print('CV folds ({}): {}\\n'.format(len(cv_folds), cv_folds))\n",
    "\n",
    "for fold_id in range(CV):\n",
    "    print('Fold {}'.format(fold_id + 1))\n",
    "    \n",
    "    cv_folds_cp = cv_folds.copy()\n",
    "    valid_experiments_fold = cv_folds_cp.pop(fold_id)\n",
    "    train_experiments_fold = [item for sublist in cv_folds_cp for item in sublist]\n",
    "\n",
    "    print('Train experiments ids ({}): {}'.format(len(train_experiments_fold), train_experiments_fold))\n",
    "    print('Validation experiments ids ({}): {}'.format(len(valid_experiments_fold), valid_experiments_fold))\n",
    "\n",
    "    assert(len(train_experiments_fold) + len(valid_experiments_fold) == len(train_experiments))\n",
    "    # Check that no validation experiments are in train\n",
    "    assert(not any([i in valid_experiments_fold for i in train_experiments_fold]))\n",
    "    # Check that no test experiments are in train or validation folds\n",
    "    assert(not any([i in test_experiments for i in train_experiments_fold]))\n",
    "    assert(not any([i in test_experiments for i in valid_experiments_fold]))\n",
    "    \n",
    "    X_train = np.concatenate([features_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    Y_train = np.concatenate([targets_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    X_valid = np.concatenate([features_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "    Y_valid = np.concatenate([targets_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "\n",
    "    print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "    print('Validation -> X: {}, Y: {}'.format(X_valid.shape, Y_valid.shape))\n",
    "    \n",
    "    # Normalize the data\n",
    "    s = SCALER.fit(X_train)\n",
    "\n",
    "    X_train_norm = s.transform(X_train)\n",
    "    X_valid_norm =  s.transform(X_valid)\n",
    "\n",
    "    print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "    print('Valid -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}'.format(np.min(X_valid_norm, axis=0), np.max(X_valid_norm, axis=0), np.mean(X_valid_norm, axis=0), np.std(X_valid_norm, axis=0)))\n",
    "    \n",
    "    save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    np.save(save_dir + '/{}_X_train_cv{}_{}.npy'.format(JOINT, fold_id + 1, DATA_ID), X_train_norm)    \n",
    "    np.save(save_dir + '/{}_X_valid_cv{}_{}.npy'.format(JOINT, fold_id + 1, DATA_ID), X_valid_norm)    \n",
    "    np.save(save_dir + '/{}_Y_train_cv{}_{}.npy'.format(JOINT, fold_id + 1, DATA_ID), Y_train)    \n",
    "    np.save(save_dir + '/{}_Y_valid_cv{}_{}.npy'.format(JOINT, fold_id + 1, DATA_ID), Y_valid)      \n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-mexican",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
