{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gisSEwlud_Yn"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import string\n",
    "from time import strftime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.utils.io.Tee at 0x7fdd8a7bf9a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.utils.io import Tee\n",
    "\n",
    "# Redirect all the outputs messages to the terminal and to a log file\n",
    "logs_dir = './logs'\n",
    "logfilename = logs_dir + strftime('/ipython_%Y-%m-%d_%H:%M:%S') + '.log' \n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "    \n",
    "sys.stdout = open('/dev/stdout', 'w')\n",
    "Tee(logfilename, mode='w', channel='stdout')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl0d0w1g/data/EducaciÃ³n/Master Ing de la Informacion para la Salud/Master Thesis/EXOSAFE-ML/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "JOINT = 'Knee'\n",
    "FORCE_CELLS_PER_JOINT = {\n",
    "    'Hip': [5, 6],\n",
    "    'Knee': [3, 4, 7, 8],\n",
    "    'Ankle': [1, 2]\n",
    "}\n",
    "\n",
    "CELLS = FORCE_CELLS_PER_JOINT[JOINT]\n",
    "\n",
    "# Directory where the derived data is stored\n",
    "DERIVED_DATA_DIR = '../../../../data'\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../../results'\n",
    "# Total number of experiments\n",
    "N_EXPERIMENTS = 63\n",
    "# Number of folds for cross-validation\n",
    "CV = 6\n",
    "# Experiments for training set (int)\n",
    "TRAIN_SIZE = 54\n",
    "# Experiments for test set (int)\n",
    "TEST_SIZE = 9\n",
    "\n",
    "# ID of the training and validation data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0012_09082021'\n",
    "# Hyperparameters search date\n",
    "hs_date = '16082021'\n",
    "\n",
    "# Parameters grid\n",
    "param_grid = {\n",
    "    'sequence_len': [5],\n",
    "    'hidden_dim': [32],\n",
    "    'sigma': [1],\n",
    "    'n_layers': [1]\n",
    "}\n",
    "\n",
    "TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', TORCH_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of experiments: 63\n"
     ]
    }
   ],
   "source": [
    "experiments_dirs_path = glob.glob(DERIVED_DATA_DIR + '/*/*')\n",
    "print('Number of experiments:', len(experiments_dirs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment selection (optional)\n",
    "experiments_dirs_path_filter = []\n",
    "\n",
    "exp_params = {}\n",
    "for exp_path in experiments_dirs_path:\n",
    "    with open(exp_path + '/parameters.json') as f:\n",
    "        exp_params[exp_path] = json.load(f)\n",
    "        \n",
    "    if int(exp_params[exp_path]['SkinConfig']) == 0 or exp_params[exp_path]['SkinConfig'] == 'NaN':\n",
    "        experiments_dirs_path_filter.append(exp_path)\n",
    "        \n",
    "print('Final number of experiments:', len(experiments_dirs_path_filter))\n",
    "experiments_dirs_path = experiments_dirs_path_filter\n",
    "\n",
    "# Redefine the split of the data for the filtered experiments\n",
    "\n",
    "# Total number of experiments\n",
    "N_EXPERIMENTS = 40\n",
    "# Number of folds for cross-validation\n",
    "CV = 4\n",
    "# Experiments for training set (int)\n",
    "TRAIN_SIZE = 32\n",
    "# Experiments for test set (int)\n",
    "TEST_SIZE = 8\n",
    "\n",
    "assert(TRAIN_SIZE + TEST_SIZE == N_EXPERIMENTS)\n",
    "assert(TRAIN_SIZE % CV == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 5\n",
      "Selected features: ['LKneePos', 'F3z', 'F4z', 'F7z', 'F8z']\n",
      "\n",
      "\n",
      "Number of targets: 8\n",
      "Selected targets: ['F3x', 'F3y', 'F4x', 'F4y', 'F7x', 'F7y', 'F8x', 'F8y']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "# features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "# targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "features = ['L{}Pos'.format(JOINT)] + ['F' + str(i) + 'z' for i in FORCE_CELLS_PER_JOINT[JOINT]]\n",
    "targets = ['F' + str(i) + ax for i in FORCE_CELLS_PER_JOINT[JOINT] for ax in ['x', 'y']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Experiment 1 from 10032021\n",
      "Droping 2917 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 0 -> X: (2916, 5), Y: (2916, 8) \n",
      "\n",
      "1 - Experiment 1 from 16022021\n",
      "Droping 8709 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 1 -> X: (8708, 5), Y: (8708, 8) \n",
      "\n",
      "2 - Experiment 2 from 16022021\n",
      "Droping 8696 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 2 -> X: (8695, 5), Y: (8695, 8) \n",
      "\n",
      "3 - Experiment 3 from 16022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 3 -> X: (8707, 5), Y: (8707, 8) \n",
      "\n",
      "4 - Experiment 4 from 16022021\n",
      "Droping 8736 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 4 -> X: (8735, 5), Y: (8735, 8) \n",
      "\n",
      "5 - Experiment 5 from 16022021\n",
      "Droping 8706 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 5 -> X: (8705, 5), Y: (8705, 8) \n",
      "\n",
      "6 - Experiment 6 from 16022021\n",
      "Droping 8680 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 6 -> X: (8679, 5), Y: (8679, 8) \n",
      "\n",
      "7 - Experiment 2 from 17022021\n",
      "Droping 8711 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 7 -> X: (8710, 5), Y: (8710, 8) \n",
      "\n",
      "8 - Experiment 3 from 17022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 8 -> X: (8734, 5), Y: (8734, 8) \n",
      "\n",
      "9 - Experiment 4 from 17022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 9 -> X: (8698, 5), Y: (8698, 8) \n",
      "\n",
      "10 - Experiment 1 from 19022021\n",
      "Droping 8697 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 10 -> X: (8696, 5), Y: (8696, 8) \n",
      "\n",
      "11 - Experiment 10 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 11 -> X: (8724, 5), Y: (8724, 8) \n",
      "\n",
      "12 - Experiment 11 from 19022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 12 -> X: (8719, 5), Y: (8719, 8) \n",
      "\n",
      "13 - Experiment 12 from 19022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 13 -> X: (8732, 5), Y: (8732, 8) \n",
      "\n",
      "14 - Experiment 13 from 19022021\n",
      "Droping 5792 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 14 -> X: (5791, 5), Y: (5791, 8) \n",
      "\n",
      "15 - Experiment 14 from 19022021\n",
      "Droping 8718 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 15 -> X: (8717, 5), Y: (8717, 8) \n",
      "\n",
      "16 - Experiment 15 from 19022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 16 -> X: (8738, 5), Y: (8738, 8) \n",
      "\n",
      "17 - Experiment 16 from 19022021\n",
      "Droping 8243 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 17 -> X: (8242, 5), Y: (8242, 8) \n",
      "\n",
      "18 - Experiment 17 from 19022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 18 -> X: (8734, 5), Y: (8734, 8) \n",
      "\n",
      "19 - Experiment 18 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 19 -> X: (8724, 5), Y: (8724, 8) \n",
      "\n",
      "20 - Experiment 2 from 19022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 20 -> X: (8707, 5), Y: (8707, 8) \n",
      "\n",
      "21 - Experiment 3 from 19022021\n",
      "Droping 8734 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 21 -> X: (8733, 5), Y: (8733, 8) \n",
      "\n",
      "22 - Experiment 4 from 19022021\n",
      "Droping 8717 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 22 -> X: (8716, 5), Y: (8716, 8) \n",
      "\n",
      "23 - Experiment 5 from 19022021\n",
      "Droping 8730 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 23 -> X: (8729, 5), Y: (8729, 8) \n",
      "\n",
      "24 - Experiment 6 from 19022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 24 -> X: (8737, 5), Y: (8737, 8) \n",
      "\n",
      "25 - Experiment 7 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 25 -> X: (8724, 5), Y: (8724, 8) \n",
      "\n",
      "26 - Experiment 8 from 19022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 26 -> X: (8747, 5), Y: (8747, 8) \n",
      "\n",
      "27 - Experiment 9 from 19022021\n",
      "Droping 8704 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 27 -> X: (8703, 5), Y: (8703, 8) \n",
      "\n",
      "28 - Experiment 1 from 22022021\n",
      "Droping 8715 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 28 -> X: (8714, 5), Y: (8714, 8) \n",
      "\n",
      "29 - Experiment 10 from 22022021\n",
      "Droping 8736 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 29 -> X: (8735, 5), Y: (8735, 8) \n",
      "\n",
      "30 - Experiment 11 from 22022021\n",
      "Droping 8702 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 30 -> X: (8701, 5), Y: (8701, 8) \n",
      "\n",
      "31 - Experiment 12 from 22022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 31 -> X: (8707, 5), Y: (8707, 8) \n",
      "\n",
      "32 - Experiment 13 from 22022021\n",
      "Droping 8749 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 32 -> X: (8748, 5), Y: (8748, 8) \n",
      "\n",
      "33 - Experiment 14 from 22022021\n",
      "Droping 8727 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 33 -> X: (8726, 5), Y: (8726, 8) \n",
      "\n",
      "34 - Experiment 15 from 22022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 34 -> X: (8698, 5), Y: (8698, 8) \n",
      "\n",
      "35 - Experiment 16 from 22022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 35 -> X: (8732, 5), Y: (8732, 8) \n",
      "\n",
      "36 - Experiment 17 from 22022021\n",
      "Droping 8705 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 36 -> X: (8704, 5), Y: (8704, 8) \n",
      "\n",
      "37 - Experiment 18 from 22022021\n",
      "Droping 8701 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 37 -> X: (8700, 5), Y: (8700, 8) \n",
      "\n",
      "38 - Experiment 19 from 22022021\n",
      "Droping 5815 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 38 -> X: (5814, 5), Y: (5814, 8) \n",
      "\n",
      "39 - Experiment 2 from 22022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 39 -> X: (8732, 5), Y: (8732, 8) \n",
      "\n",
      "40 - Experiment 3 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 40 -> X: (8719, 5), Y: (8719, 8) \n",
      "\n",
      "41 - Experiment 4 from 22022021\n",
      "Droping 8732 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 41 -> X: (8731, 5), Y: (8731, 8) \n",
      "\n",
      "42 - Experiment 5 from 22022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 42 -> X: (8738, 5), Y: (8738, 8) \n",
      "\n",
      "43 - Experiment 6 from 22022021\n",
      "Droping 8721 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 43 -> X: (8720, 5), Y: (8720, 8) \n",
      "\n",
      "44 - Experiment 7 from 22022021\n",
      "Droping 8710 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 44 -> X: (8709, 5), Y: (8709, 8) \n",
      "\n",
      "45 - Experiment 8 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 45 -> X: (8719, 5), Y: (8719, 8) \n",
      "\n",
      "46 - Experiment 9 from 22022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 46 -> X: (8738, 5), Y: (8738, 8) \n",
      "\n",
      "47 - Experiment 1 from 24022021\n",
      "Droping 8690 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 47 -> X: (8689, 5), Y: (8689, 8) \n",
      "\n",
      "48 - Experiment 10 from 24022021\n",
      "Droping 8519 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 48 -> X: (8518, 5), Y: (8518, 8) \n",
      "\n",
      "49 - Experiment 11 from 24022021\n",
      "Droping 8627 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 49 -> X: (8626, 5), Y: (8626, 8) \n",
      "\n",
      "50 - Experiment 12 from 24022021\n",
      "Droping 8744 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 50 -> X: (8743, 5), Y: (8743, 8) \n",
      "\n",
      "51 - Experiment 13 from 24022021\n",
      "Droping 8569 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 51 -> X: (8568, 5), Y: (8568, 8) \n",
      "\n",
      "52 - Experiment 14 from 24022021\n",
      "Droping 8729 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 52 -> X: (8728, 5), Y: (8728, 8) \n",
      "\n",
      "53 - Experiment 15 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 53 -> X: (8686, 5), Y: (8686, 8) \n",
      "\n",
      "54 - Experiment 16 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 54 -> X: (8686, 5), Y: (8686, 8) \n",
      "\n",
      "55 - Experiment 2 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 55 -> X: (8712, 5), Y: (8712, 8) \n",
      "\n",
      "56 - Experiment 3 from 24022021\n",
      "Droping 8543 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 56 -> X: (8542, 5), Y: (8542, 8) \n",
      "\n",
      "57 - Experiment 4 from 24022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 57 -> X: (8724, 5), Y: (8724, 8) \n",
      "\n",
      "58 - Experiment 5 from 24022021\n",
      "Droping 8568 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 58 -> X: (8567, 5), Y: (8567, 8) \n",
      "\n",
      "59 - Experiment 6 from 24022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 59 -> X: (8747, 5), Y: (8747, 8) \n",
      "\n",
      "60 - Experiment 7 from 24022021\n",
      "Droping 8603 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 60 -> X: (8602, 5), Y: (8602, 8) \n",
      "\n",
      "61 - Experiment 8 from 24022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 61 -> X: (8737, 5), Y: (8737, 8) \n",
      "\n",
      "62 - Experiment 9 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "Experiment 62 -> X: (8712, 5), Y: (8712, 8) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i, exp_path in enumerate(experiments_dirs_path):\n",
    "    print('{} - Experiment {} from {}'.format(i, exp_path.split('/')[-1], exp_path.split('/')[-2]))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "    # leg_df = pd.read_csv(exp_path + '/leg_processed.csv')\n",
    "    # features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    features_df = exo_df\n",
    "    \n",
    "    idx_aux = targets_df.duplicated(keep='first')\n",
    "    targets_df = targets_df.loc[~idx_aux]\n",
    "    features_df = features_df.loc[~idx_aux]\n",
    "    print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "    \n",
    "    # Drop first row to remove noise in the start of the data recording\n",
    "    targets_df = targets_df.iloc[1:]\n",
    "    features_df = features_df.iloc[1:]\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    assert(len(features_df) == len(targets_df))\n",
    "    data_df = pd.concat([features_df, targets_df], axis=1)\n",
    "    \n",
    "    # Store the final array\n",
    "    targets_dict[i] = data_df[targets].values\n",
    "    features_dict[i] = data_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (54): [53, 41, 3, 60, 33, 58, 27, 5, 7, 44, 49, 28, 23, 29, 46, 12, 57, 0, 61, 1, 43, 40, 14, 15, 17, 62, 20, 36, 10, 47, 11, 35, 52, 21, 4, 42, 51, 9, 38, 34, 59, 39, 6, 45, 18, 8, 55, 13, 37, 22, 30, 19, 50, 25]\n",
      "Test experiments ids (9): [31, 32, 16, 2, 26, 56, 48, 24, 54]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(N_EXPERIMENTS))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[:TRAIN_SIZE]\n",
    "test_experiments = experiments[TRAIN_SIZE:]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "assert(len(train_experiments) + len(test_experiments) == N_EXPERIMENTS)\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN hyperparameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, sigma, output_size, sequence_len, num_iter, n_layers, device, lr=0.001, batch_size=32, early_stopping_patience=10):\n",
    "        \n",
    "        # input size -> Dimension of the input signal\n",
    "        # outpusize -> Dimension of the output signal\n",
    "        # hidden_dim -> Dimension of the rnn state\n",
    "        # n_layers -> If >1, we are using a stacked RNN\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_size = input_size\n",
    "        self.sigma = sigma\n",
    "        self.output_size = output_size\n",
    "        self.sequence_length = sequence_len\n",
    "        self.num_layers = n_layers\n",
    "        \n",
    "        # define an RNN with specified parameters\n",
    "        # batch_first=True means that the first dimension of the input will be the batch_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, \n",
    "                          nonlinearity='relu', batch_first=True)\n",
    "        \n",
    "        # last, fully-connected layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size) # YOUR CODE HERE\n",
    "\n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        self.batch_size = batch_size\n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        self.num_iter = num_iter\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.criterion = nn.MSELoss() #YOUR CODE HERE     \n",
    "        \n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        self.loss_during_training = [] \n",
    "        self.valid_loss_during_training = []\n",
    "        \n",
    "    def forward(self, x, h0=None):\n",
    "        \n",
    "        '''\n",
    "        About the shape of the different tensors ...:\n",
    "        \n",
    "        - Input signal x has shape (batch_size, seq_length, input_size)\n",
    "        - The initialization of the RNN hidden state h0 has shape (n_layers, batch_size, hidden_dim).\n",
    "          If None value is used, internally it is initialized to zeros.\n",
    "        - The RNN output (batch_size, seq_length, hidden_size). This output is the RNN state along time  \n",
    "\n",
    "        '''\n",
    "        batch_size = x.size(0) # Number of signals N\n",
    "        seq_length = x.size(1) # T\n",
    "        \n",
    "        # get RNN outputs\n",
    "        # r_out is the sequence of states\n",
    "        # hidden is just the last state (we will use it for forecasting)\n",
    "        #print(x.shape)\n",
    "        r_out, hidden = self.rnn(x, h0)\n",
    "        #print(r_out.shape, hidden.shape)\n",
    "        \n",
    "        # shape r_out to be (seq_length, hidden_dim) #UNDERSTANDING THIS POINT IS IMPORTANT!!        \n",
    "        r_out = r_out.reshape(-1, self.hidden_dim) \n",
    "        #print(r_out.shape)\n",
    "        \n",
    "        output = self.fc1(r_out)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        noise = torch.randn_like(output) * self.sigma\n",
    "        \n",
    "        output += noise\n",
    "        \n",
    "        # reshape back to temporal structure\n",
    "        output = output.reshape([-1, self.sequence_length, 1])\n",
    "\n",
    "        return output, hidden\n",
    "           \n",
    "    def trainloop(self, x, y, x_val=None, y_val=None):\n",
    "        \n",
    "        epochs_wo_improvement = self.early_stopping_patience\n",
    "        # SGD Loop\n",
    "        for e in range(int(self.num_iter)):\n",
    "        \n",
    "            running_loss = 0.\n",
    "            \n",
    "            idx = list(range(x.shape[0]))\n",
    "            random.shuffle(idx)\n",
    "            \n",
    "            batchs = [idx[i:i + self.batch_size] for i in range(0, x.shape[0], self.batch_size)]\n",
    "            for b in batchs:\n",
    "\n",
    "                features = x[b, :, :]\n",
    "                target = y[b, :, :]\n",
    "\n",
    "                # print(features.shape, target.shape)\n",
    "                self.optim.zero_grad() \n",
    "                \n",
    "                features = torch.Tensor(features).to(self.device) #YOUR CODE HERE  \n",
    "                target = torch.Tensor(target).to(self.device) #YOUR CODE HERE  \n",
    "                \n",
    "                out,hid = self.forward(features)\n",
    "\n",
    "                # print(out.shape, hid.shape, target.shape)\n",
    "                # print(out[:, -1, 0].size(), target[:, 0].size())\n",
    "                loss = self.criterion(out[:, -1, 0], target[:, -1, 0]) #YOUR CODE HERE\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # This code helps to avoid vanishing exploiting gradients in RNNs\n",
    "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "                nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
    "\n",
    "                self.optim.step()\n",
    "                \n",
    "            self.loss_during_training.append(running_loss / len(batchs))\n",
    "            \n",
    "            if x_val is not None and y_val is not None:\n",
    "                with torch.no_grad():\n",
    "                    # Set the model in evaluation mode \n",
    "                    self.eval()\n",
    "\n",
    "                    running_loss_val = 0.\n",
    "                    idx = list(range(x_val.shape[0]))\n",
    "\n",
    "                    batchs = [idx[i:i + self.batch_size] for i in range(0, x_val.shape[0], self.batch_size)]\n",
    "                    for b in batchs:\n",
    "                        features = x_val[b, :, :]\n",
    "                        target = y_val[b, :, :]\n",
    "\n",
    "                        features = torch.Tensor(features).to(self.device) #YOUR CODE HERE  \n",
    "                        target = torch.Tensor(target).to(self.device) #YOUR CODE HERE  \n",
    "\n",
    "                        out,hid = self.forward(features)\n",
    "\n",
    "                        # print(out.shape, hid.shape, target.shape)\n",
    "                        # print(out[:, -1, 0].size(), target[:, -1, 0].size())\n",
    "                        loss = self.criterion(out[:, -1, 0], target[:, -1, 0]) #YOUR CODE HERE\n",
    "                        running_loss_val += loss.item()\n",
    "\n",
    "                    self.valid_loss_during_training.append(running_loss_val / len(splits))\n",
    "\n",
    "                # Return the model to training mode\n",
    "                self.train()\n",
    "\n",
    "                print(\"%d epochs -> Train loss: %f - Val loss: %f\"%(e, self.loss_during_training[-1], self.valid_loss_during_training[-1]))\n",
    "                \n",
    "                if len(self.valid_loss_during_training) > self.early_stopping_patience + 1:\n",
    "                    if self.valid_loss_during_training[-1] > np.mean(self.valid_loss_during_training[-self.early_stopping_patience:]):\n",
    "                        print('Early stopping at epoch', e + 1)\n",
    "                        return e\n",
    "            else:\n",
    "                print(\"%d epochs -> Train loss: %f\"%(e, self.loss_during_training[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters combinations: 1\n",
      "Parameters (I82QCLS7DT) 1/1  -  2021-08-15 20:05:57\n",
      "{'hidden_dim': 32, 'n_layers': 1, 'sequence_len': 5, 'sigma': 1}\n",
      "0 epochs -> Train loss: 375.860282 - Val loss: 146.279718\n",
      "1 epochs -> Train loss: 271.087544 - Val loss: 134.778701\n",
      "2 epochs -> Train loss: 264.720525 - Val loss: 138.338348\n",
      "3 epochs -> Train loss: 246.643673 - Val loss: 105.755041\n",
      "4 epochs -> Train loss: 212.816215 - Val loss: 103.600939\n",
      "5 epochs -> Train loss: 201.100178 - Val loss: 104.950213\n",
      "6 epochs -> Train loss: 191.990312 - Val loss: 93.546279\n",
      "7 epochs -> Train loss: 183.940083 - Val loss: 82.502934\n",
      "8 epochs -> Train loss: 178.096340 - Val loss: 93.873808\n",
      "9 epochs -> Train loss: 171.284014 - Val loss: 93.499993\n",
      "10 epochs -> Train loss: 164.119390 - Val loss: 82.754420\n",
      "11 epochs -> Train loss: 156.595714 - Val loss: 73.375633\n",
      "12 epochs -> Train loss: 152.523878 - Val loss: 68.053040\n",
      "13 epochs -> Train loss: 150.583894 - Val loss: 100.296576\n",
      "Early stopping at epoch 14\n",
      "0 epochs -> Train loss: 271.877495 - Val loss: 76.059925\n",
      "1 epochs -> Train loss: 244.033644 - Val loss: 67.067700\n",
      "2 epochs -> Train loss: 219.057297 - Val loss: 61.870539\n",
      "3 epochs -> Train loss: 201.858294 - Val loss: 62.492862\n",
      "4 epochs -> Train loss: 188.722973 - Val loss: 64.001575\n",
      "5 epochs -> Train loss: 181.267445 - Val loss: 55.833763\n",
      "6 epochs -> Train loss: 176.996223 - Val loss: 63.854768\n",
      "7 epochs -> Train loss: 172.620842 - Val loss: 54.898926\n",
      "8 epochs -> Train loss: 169.273106 - Val loss: 52.585971\n",
      "9 epochs -> Train loss: 167.025610 - Val loss: 57.910813\n",
      "10 epochs -> Train loss: 163.039489 - Val loss: 53.776535\n",
      "11 epochs -> Train loss: 160.355106 - Val loss: 52.589535\n",
      "12 epochs -> Train loss: 157.866039 - Val loss: 52.502764\n",
      "13 epochs -> Train loss: 155.937027 - Val loss: 51.177164\n",
      "14 epochs -> Train loss: 153.995128 - Val loss: 55.018361\n",
      "Early stopping at epoch 15\n",
      "0 epochs -> Train loss: 173.603225 - Val loss: 41.462388\n",
      "1 epochs -> Train loss: 141.935175 - Val loss: 33.071076\n",
      "2 epochs -> Train loss: 136.904835 - Val loss: 39.867845\n",
      "3 epochs -> Train loss: 131.428012 - Val loss: 37.716334\n",
      "4 epochs -> Train loss: 125.269046 - Val loss: 40.933348\n",
      "5 epochs -> Train loss: 120.487383 - Val loss: 36.134600\n",
      "6 epochs -> Train loss: 115.726461 - Val loss: 29.753021\n",
      "7 epochs -> Train loss: 112.645645 - Val loss: 42.925829\n",
      "8 epochs -> Train loss: 110.187047 - Val loss: 24.335321\n",
      "9 epochs -> Train loss: 106.806878 - Val loss: 30.489369\n",
      "10 epochs -> Train loss: 103.717579 - Val loss: 25.735685\n",
      "11 epochs -> Train loss: 101.247902 - Val loss: 26.642464\n",
      "12 epochs -> Train loss: 99.464761 - Val loss: 27.389195\n",
      "13 epochs -> Train loss: 98.315078 - Val loss: 23.547464\n",
      "14 epochs -> Train loss: 96.797187 - Val loss: 26.374712\n",
      "15 epochs -> Train loss: 95.219202 - Val loss: 26.058693\n",
      "16 epochs -> Train loss: 93.648657 - Val loss: 24.270129\n",
      "17 epochs -> Train loss: 93.331020 - Val loss: 29.067085\n",
      "Early stopping at epoch 18\n",
      "0 epochs -> Train loss: 334.743534 - Val loss: 129.246790\n",
      "1 epochs -> Train loss: 248.749872 - Val loss: 84.259329\n",
      "2 epochs -> Train loss: 214.736622 - Val loss: 79.906858\n",
      "3 epochs -> Train loss: 188.596606 - Val loss: 68.045150\n",
      "4 epochs -> Train loss: 168.584704 - Val loss: 66.531623\n",
      "5 epochs -> Train loss: 156.194305 - Val loss: 78.555968\n",
      "6 epochs -> Train loss: 147.372696 - Val loss: 57.286106\n",
      "7 epochs -> Train loss: 138.758695 - Val loss: 51.433370\n",
      "8 epochs -> Train loss: 130.673620 - Val loss: 44.421214\n",
      "9 epochs -> Train loss: 121.790247 - Val loss: 71.418228\n",
      "10 epochs -> Train loss: 117.588495 - Val loss: 82.859017\n",
      "11 epochs -> Train loss: 110.259062 - Val loss: 66.792096\n",
      "Early stopping at epoch 12\n",
      "0 epochs -> Train loss: 140.996127 - Val loss: 86.085400\n",
      "1 epochs -> Train loss: 110.389531 - Val loss: 85.052288\n",
      "2 epochs -> Train loss: 98.920280 - Val loss: 74.671935\n",
      "3 epochs -> Train loss: 88.231992 - Val loss: 76.372583\n",
      "4 epochs -> Train loss: 81.902764 - Val loss: 71.223764\n",
      "5 epochs -> Train loss: 77.016823 - Val loss: 71.373798\n",
      "6 epochs -> Train loss: 73.798819 - Val loss: 65.296905\n",
      "7 epochs -> Train loss: 70.606265 - Val loss: 69.586937\n",
      "8 epochs -> Train loss: 68.555054 - Val loss: 86.490064\n",
      "9 epochs -> Train loss: 67.107021 - Val loss: 63.475549\n",
      "10 epochs -> Train loss: 65.788728 - Val loss: 71.036865\n",
      "11 epochs -> Train loss: 64.535548 - Val loss: 74.294890\n",
      "Early stopping at epoch 12\n",
      "0 epochs -> Train loss: 28.765272 - Val loss: 8.793678\n",
      "1 epochs -> Train loss: 23.727408 - Val loss: 8.821614\n",
      "2 epochs -> Train loss: 22.881501 - Val loss: 8.611752\n",
      "3 epochs -> Train loss: 22.265835 - Val loss: 9.307006\n",
      "4 epochs -> Train loss: 21.791147 - Val loss: 7.495059\n",
      "5 epochs -> Train loss: 21.167664 - Val loss: 6.829176\n",
      "6 epochs -> Train loss: 20.378985 - Val loss: 7.990367\n",
      "7 epochs -> Train loss: 19.382533 - Val loss: 6.140942\n",
      "8 epochs -> Train loss: 18.653365 - Val loss: 6.504745\n",
      "9 epochs -> Train loss: 17.895638 - Val loss: 5.898416\n",
      "10 epochs -> Train loss: 17.337965 - Val loss: 6.251404\n",
      "11 epochs -> Train loss: 17.019282 - Val loss: 7.556901\n",
      "Early stopping at epoch 12\n",
      "0 epochs -> Train loss: 91.883966 - Val loss: 17.278485\n",
      "1 epochs -> Train loss: 72.192806 - Val loss: 16.309788\n",
      "2 epochs -> Train loss: 64.433949 - Val loss: 14.544047\n",
      "3 epochs -> Train loss: 59.282204 - Val loss: 12.504903\n",
      "4 epochs -> Train loss: 54.342130 - Val loss: 14.574224\n",
      "5 epochs -> Train loss: 50.019316 - Val loss: 11.396889\n",
      "6 epochs -> Train loss: 47.768583 - Val loss: 11.538379\n",
      "7 epochs -> Train loss: 45.484754 - Val loss: 11.637387\n",
      "8 epochs -> Train loss: 44.589100 - Val loss: 11.947551\n",
      "9 epochs -> Train loss: 43.225289 - Val loss: 11.330760\n",
      "10 epochs -> Train loss: 42.463511 - Val loss: 12.389168\n",
      "11 epochs -> Train loss: 41.360527 - Val loss: 14.784128\n",
      "Early stopping at epoch 12\n",
      "0 epochs -> Train loss: 108.313078 - Val loss: 27.977376\n",
      "1 epochs -> Train loss: 86.085133 - Val loss: 24.234760\n",
      "2 epochs -> Train loss: 80.118009 - Val loss: 19.802365\n",
      "3 epochs -> Train loss: 74.958530 - Val loss: 22.900039\n",
      "4 epochs -> Train loss: 71.537169 - Val loss: 18.248357\n",
      "5 epochs -> Train loss: 67.022493 - Val loss: 18.203990\n",
      "6 epochs -> Train loss: 63.264108 - Val loss: 21.129839\n",
      "7 epochs -> Train loss: 60.260488 - Val loss: 21.784878\n",
      "8 epochs -> Train loss: 57.508658 - Val loss: 20.585866\n",
      "9 epochs -> Train loss: 54.747262 - Val loss: 15.891418\n",
      "10 epochs -> Train loss: 52.456505 - Val loss: 16.083810\n",
      "11 epochs -> Train loss: 51.423808 - Val loss: 21.853255\n",
      "Early stopping at epoch 12\n",
      "Training time: 574.1882\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type float32 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c830de544e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}_RNN_{}_{}.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type float32 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "param_grid_ls = list(ParameterGrid(param_grid))\n",
    "random.shuffle(param_grid_ls)\n",
    "param_grid_len = len(param_grid_ls)\n",
    "print('Number of parameters combinations: {}'.format(param_grid_len))\n",
    "\n",
    "for idx, params in enumerate(param_grid_ls):\n",
    "    params_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n",
    "    print('Parameters ({}) {}/{}  -  {}'.format(params_id, idx + 1, param_grid_len, strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    print(params)\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X_train, Y_train = [], [] \n",
    "    for i in train_experiments:\n",
    "        X_aux = features_dict[i]\n",
    "        Y_aux = targets_dict[i]\n",
    "\n",
    "        splits = [i for i in range(0, X_aux.shape[0], params['sequence_len'])]\n",
    "\n",
    "        X_tensor = np.zeros([len(splits), params['sequence_len'], len(features)])\n",
    "        Y_tensor = np.zeros([len(splits), params['sequence_len'], len(targets)])\n",
    "\n",
    "        for idx, i in enumerate(splits[:-1]):\n",
    "            X_tensor[idx, :, :] = X_aux[splits[idx]:splits[idx + 1], :]\n",
    "            Y_tensor[idx, :, :] = Y_aux[splits[idx + 1] - 1, :]\n",
    "\n",
    "        # print(X_tensor.shape)\n",
    "        # print(Y_tensor.shape)\n",
    "\n",
    "        X_train.append(X_tensor)\n",
    "        Y_train.append(Y_tensor)\n",
    "\n",
    "    X_train, Y_train = np.concatenate(X_train, axis=0), np.concatenate(Y_train, axis=0) \n",
    "\n",
    "\n",
    "    X_test, Y_test = [], [] \n",
    "    for i in test_experiments:\n",
    "        X_aux = features_dict[i]\n",
    "        Y_aux = targets_dict[i]\n",
    "\n",
    "        splits = [i for i in range(0, X_aux.shape[0], params['sequence_len'])]\n",
    "\n",
    "        X_tensor = np.zeros([len(splits), params['sequence_len'], len(features)])\n",
    "        Y_tensor = np.zeros([len(splits), params['sequence_len'], len(targets)])\n",
    "\n",
    "        for idx, i in enumerate(splits[:-1]):\n",
    "            X_tensor[idx, :, :] = X_aux[splits[idx]:splits[idx + 1], :]\n",
    "            Y_tensor[idx, :, :] = Y_aux[splits[idx + 1] - 1, :]\n",
    "\n",
    "        # print(X_tensor.shape)\n",
    "        # print(Y_tensor.shape)\n",
    "\n",
    "        X_test.append(X_tensor)\n",
    "        Y_test.append(Y_tensor)\n",
    "\n",
    "    X_test, Y_test = np.concatenate(X_test, axis=0), np.concatenate(Y_test, axis=0) \n",
    "\n",
    "    X_train_norm = np.zeros(list(X_train.shape))\n",
    "    X_test_norm = np.zeros(list(X_test.shape))\n",
    "\n",
    "    scalers = []\n",
    "    for f in range(len(features)):\n",
    "        s = MinMaxScaler().fit(X_train[:, :, f])\n",
    "\n",
    "        scalers.append(s)\n",
    "\n",
    "        X_train_norm[:, :, f] = s.transform(X_train[:, :, f])\n",
    "        X_test_norm[:, :, f] = s.transform(X_test[:, :, f])\n",
    "\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    tr_time = []\n",
    "    early_stopping = []\n",
    "    for t in range(Y_train.shape[2]):\n",
    "\n",
    "        model = RNN(input_size=X_train_norm.shape[2], output_size=1, num_iter=50, device=TORCH_DEVICE, **params)\n",
    "\n",
    "        t_start = time.time()\n",
    "        last_epoch = model.trainloop(X_train_norm, Y_train[:, :, t:t+1], X_test_norm, Y_test[:, :, t:t+1])\n",
    "        tr_time.append(time.time() - t_start)\n",
    "        early_stopping.append(last_epoch)\n",
    "\n",
    "        all_results = {'MAE': [], 'MSE': [], 'R2': []}\n",
    "        for i in test_experiments:\n",
    "            X_aux = features_dict[i]\n",
    "            Y_aux = targets_dict[i]\n",
    "\n",
    "            true = []\n",
    "            pred = []\n",
    "\n",
    "            for i in range(params['sequence_len'], X_aux.shape[0]):\n",
    "                X_aux_ = X_aux[np.newaxis, i-params['sequence_len']:i, :]\n",
    "                for f in range(len(features)):\n",
    "                    s = scalers[f]\n",
    "\n",
    "                    X_aux_[:, :, f] = s.transform(X_aux_[:, :, f])\n",
    "\n",
    "                features_ = torch.Tensor(X_aux_).to(TORCH_DEVICE)\n",
    "                target_ = torch.Tensor(Y_aux[np.newaxis, i, t:t+1]).to(TORCH_DEVICE)\n",
    "\n",
    "                out, hid = model.forward(features_)\n",
    "\n",
    "                true.append(target_[:, 0].cpu().detach().numpy())\n",
    "                pred.append(out[:, -1, 0].cpu().detach().numpy())\n",
    "\n",
    "            all_results['MAE'].append(mean_absolute_error(true, pred))\n",
    "            all_results['MSE'].append(mean_squared_error(true, pred))\n",
    "            all_results['R2'].append(r2_score(true, pred))\n",
    "\n",
    "        # results['Train_MAE'].append(mean_absolute_error(Y_train[:, target], train_preds))\n",
    "        # results['Train_MSE'].append(mean_squared_error(Y_train[:, target], train_preds))\n",
    "        # results['Train_R2'].append(r2_score(Y_train[:, target], train_preds))\n",
    "        results['Valid_MAE'].append(np.mean(all_results['MAE']))\n",
    "        results['Valid_MSE'].append(np.mean(all_results['MSE']))\n",
    "        results['Valid_R2'].append(np.mean(all_results['R2']))\n",
    "\n",
    "    results['early_stopping'] = early_stopping\n",
    "    results['fit_time'] = sum(tr_time)\n",
    "    print('Training time: {:.4f}'.format(results['fit_time']))\n",
    "\n",
    "    for subset in ['Valid']:\n",
    "        for f, force in enumerate(['Fx', 'Fy']):\n",
    "            for loss in ['MAE', 'MSE', 'R2']:\n",
    "                scores = [results['_'.join([subset, loss])][i + f] for i in range(0, len(CELLS) * 2, 2)]\n",
    "                results['_'.join([subset, force, loss, 'mean'])].append(np.mean(scores))\n",
    "                results['_'.join([subset, force, loss, 'std'])].append(np.std(scores))\n",
    "\n",
    "    # Save the obtained results and its parameters into a JSON file\n",
    "    rd = {}\n",
    "    rd['id'] = params_id\n",
    "    rd['parameters'] = params\n",
    "    rd['results'] = dict(results)\n",
    "    \n",
    "    save_dir = os.path.join(RESULTS_PATH, DATA_ID, '{}_RNN_{}'.format(JOINT, hs_date))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    with open(os.path.join(save_dir, '{}_RNN_{}_{}.json'.format(JOINT, hs_date, params_id)), 'w') as fp:\n",
    "        json.dump(rd, fp)\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    del model, results, rd\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "attempt_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
