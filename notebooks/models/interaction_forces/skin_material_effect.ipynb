{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "essential-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quality-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of experiments\n",
    "N_EXPERIMENTS = 63\n",
    "# Directory where the derived data is stored\n",
    "DERIVED_DATA_DIR = '../../../data'\n",
    "\n",
    "experiments_dirs_path = glob.glob(DERIVED_DATA_DIR + '/*/*')\n",
    "assert(len(experiments_dirs_path) == N_EXPERIMENTS)\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "capable-magazine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of experiments: 40\n"
     ]
    }
   ],
   "source": [
    "experiments_dirs_path_filter_wos = []\n",
    "\n",
    "exp_params = {}\n",
    "for exp_path in experiments_dirs_path:\n",
    "    with open(exp_path + '/parameters.json') as f:\n",
    "        exp_params[exp_path] = json.load(f)\n",
    "        \n",
    "    if int(exp_params[exp_path]['SkinConfig']) == 0 or exp_params[exp_path]['SkinConfig'] == 'NaN':\n",
    "        experiments_dirs_path_filter_wos.append(exp_path)\n",
    "        \n",
    "print('Final number of experiments:', len(experiments_dirs_path_filter_wos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fourth-squad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of experiments: 17\n"
     ]
    }
   ],
   "source": [
    "experiments_dirs_path_filter = []\n",
    "\n",
    "exp_params = {}\n",
    "for exp_path in experiments_dirs_path:\n",
    "    with open(exp_path + '/parameters.json') as f:\n",
    "        exp_params[exp_path] = json.load(f)\n",
    "        \n",
    "    if int(exp_params[exp_path]['SkinConfig']) in [1, 2, 4, 5]:\n",
    "        experiments_dirs_path_filter.append(exp_path)\n",
    "        \n",
    "print('Final number of experiments:', len(experiments_dirs_path_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "three-destiny",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3\n",
      "Selected features: ['LHipPos', 'F5z', 'F6z']\n",
      "\n",
      "\n",
      "Number of targets: 4\n",
      "Selected targets: ['F5x', 'F5y', 'F6x', 'F6y']\n",
      "0 - Experiment 1 from 10032021\n",
      "Droping 2917 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "1 - Experiment 1 from 16022021\n",
      "Droping 8709 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "2 - Experiment 2 from 16022021\n",
      "Droping 8696 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "3 - Experiment 3 from 16022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "4 - Experiment 2 from 17022021\n",
      "Droping 8711 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "5 - Experiment 1 from 19022021\n",
      "Droping 8697 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "6 - Experiment 10 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "7 - Experiment 11 from 19022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "8 - Experiment 14 from 19022021\n",
      "Droping 8718 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "9 - Experiment 15 from 19022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "10 - Experiment 18 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "11 - Experiment 2 from 19022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "12 - Experiment 3 from 19022021\n",
      "Droping 8734 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "13 - Experiment 4 from 19022021\n",
      "Droping 8717 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "14 - Experiment 5 from 19022021\n",
      "Droping 8730 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "15 - Experiment 6 from 19022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "16 - Experiment 9 from 19022021\n",
      "Droping 8704 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "17 - Experiment 1 from 22022021\n",
      "Droping 8715 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "18 - Experiment 14 from 22022021\n",
      "Droping 8727 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "19 - Experiment 15 from 22022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "20 - Experiment 2 from 22022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "21 - Experiment 3 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "22 - Experiment 8 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "23 - Experiment 9 from 22022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "24 - Experiment 1 from 24022021\n",
      "Droping 8690 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "25 - Experiment 10 from 24022021\n",
      "Droping 8519 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "26 - Experiment 11 from 24022021\n",
      "Droping 8627 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "27 - Experiment 12 from 24022021\n",
      "Droping 8744 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "28 - Experiment 13 from 24022021\n",
      "Droping 8569 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "29 - Experiment 14 from 24022021\n",
      "Droping 8729 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "30 - Experiment 15 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "31 - Experiment 16 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "32 - Experiment 2 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "33 - Experiment 3 from 24022021\n",
      "Droping 8543 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "34 - Experiment 4 from 24022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "35 - Experiment 5 from 24022021\n",
      "Droping 8568 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "36 - Experiment 6 from 24022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "37 - Experiment 7 from 24022021\n",
      "Droping 8603 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "38 - Experiment 8 from 24022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "39 - Experiment 9 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "0 - Experiment 4 from 16022021\n",
      "Droping 8736 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "1 - Experiment 5 from 16022021\n",
      "Droping 8706 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "2 - Experiment 6 from 16022021\n",
      "Droping 8680 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "3 - Experiment 3 from 17022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "4 - Experiment 4 from 17022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "5 - Experiment 12 from 19022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "6 - Experiment 13 from 19022021\n",
      "Droping 5792 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "7 - Experiment 16 from 19022021\n",
      "Droping 8243 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "8 - Experiment 17 from 19022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "9 - Experiment 7 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "10 - Experiment 8 from 19022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "11 - Experiment 12 from 22022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "12 - Experiment 13 from 22022021\n",
      "Droping 8749 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "13 - Experiment 18 from 22022021\n",
      "Droping 8701 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "14 - Experiment 19 from 22022021\n",
      "Droping 5815 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "15 - Experiment 6 from 22022021\n",
      "Droping 8721 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "16 - Experiment 7 from 22022021\n",
      "Droping 8710 duplicated data points\n",
      "Droping 0 data points by null features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done 2500 out of 2500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 5\n",
      "Selected features: ['LKneePos', 'F3z', 'F4z', 'F7z', 'F8z']\n",
      "\n",
      "\n",
      "Number of targets: 8\n",
      "Selected targets: ['F3x', 'F3y', 'F4x', 'F4y', 'F7x', 'F7y', 'F8x', 'F8y']\n",
      "0 - Experiment 1 from 10032021\n",
      "Droping 2917 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "1 - Experiment 1 from 16022021\n",
      "Droping 8709 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "2 - Experiment 2 from 16022021\n",
      "Droping 8696 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "3 - Experiment 3 from 16022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "4 - Experiment 2 from 17022021\n",
      "Droping 8711 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "5 - Experiment 1 from 19022021\n",
      "Droping 8697 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "6 - Experiment 10 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "7 - Experiment 11 from 19022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "8 - Experiment 14 from 19022021\n",
      "Droping 8718 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "9 - Experiment 15 from 19022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "10 - Experiment 18 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "11 - Experiment 2 from 19022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "12 - Experiment 3 from 19022021\n",
      "Droping 8734 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "13 - Experiment 4 from 19022021\n",
      "Droping 8717 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "14 - Experiment 5 from 19022021\n",
      "Droping 8730 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "15 - Experiment 6 from 19022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "16 - Experiment 9 from 19022021\n",
      "Droping 8704 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "17 - Experiment 1 from 22022021\n",
      "Droping 8715 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "18 - Experiment 14 from 22022021\n",
      "Droping 8727 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "19 - Experiment 15 from 22022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "20 - Experiment 2 from 22022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "21 - Experiment 3 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "22 - Experiment 8 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "23 - Experiment 9 from 22022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "24 - Experiment 1 from 24022021\n",
      "Droping 8690 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "25 - Experiment 10 from 24022021\n",
      "Droping 8519 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "26 - Experiment 11 from 24022021\n",
      "Droping 8627 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "27 - Experiment 12 from 24022021\n",
      "Droping 8744 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "28 - Experiment 13 from 24022021\n",
      "Droping 8569 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "29 - Experiment 14 from 24022021\n",
      "Droping 8729 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "30 - Experiment 15 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "31 - Experiment 16 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "32 - Experiment 2 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "33 - Experiment 3 from 24022021\n",
      "Droping 8543 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "34 - Experiment 4 from 24022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "35 - Experiment 5 from 24022021\n",
      "Droping 8568 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "36 - Experiment 6 from 24022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "37 - Experiment 7 from 24022021\n",
      "Droping 8603 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "38 - Experiment 8 from 24022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "39 - Experiment 9 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "0 - Experiment 4 from 16022021\n",
      "Droping 8736 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "1 - Experiment 5 from 16022021\n",
      "Droping 8706 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "2 - Experiment 6 from 16022021\n",
      "Droping 8680 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "3 - Experiment 3 from 17022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "4 - Experiment 4 from 17022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "5 - Experiment 12 from 19022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "6 - Experiment 13 from 19022021\n",
      "Droping 5792 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "7 - Experiment 16 from 19022021\n",
      "Droping 8243 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "8 - Experiment 17 from 19022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "9 - Experiment 7 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "10 - Experiment 8 from 19022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "11 - Experiment 12 from 22022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "12 - Experiment 13 from 22022021\n",
      "Droping 8749 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "13 - Experiment 18 from 22022021\n",
      "Droping 8701 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "14 - Experiment 19 from 22022021\n",
      "Droping 5815 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "15 - Experiment 6 from 22022021\n",
      "Droping 8721 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "16 - Experiment 7 from 22022021\n",
      "Droping 8710 duplicated data points\n",
      "Droping 0 data points by null features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 2500 out of 2500 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3\n",
      "Selected features: ['LAnklePos', 'F1z', 'F2z']\n",
      "\n",
      "\n",
      "Number of targets: 4\n",
      "Selected targets: ['F1x', 'F1y', 'F2x', 'F2y']\n",
      "0 - Experiment 1 from 10032021\n",
      "Droping 2917 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "1 - Experiment 1 from 16022021\n",
      "Droping 8709 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "2 - Experiment 2 from 16022021\n",
      "Droping 8696 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "3 - Experiment 3 from 16022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "4 - Experiment 2 from 17022021\n",
      "Droping 8711 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "5 - Experiment 1 from 19022021\n",
      "Droping 8697 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "6 - Experiment 10 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "7 - Experiment 11 from 19022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "8 - Experiment 14 from 19022021\n",
      "Droping 8718 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "9 - Experiment 15 from 19022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "10 - Experiment 18 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "11 - Experiment 2 from 19022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "12 - Experiment 3 from 19022021\n",
      "Droping 8734 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "13 - Experiment 4 from 19022021\n",
      "Droping 8717 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "14 - Experiment 5 from 19022021\n",
      "Droping 8730 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "15 - Experiment 6 from 19022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "16 - Experiment 9 from 19022021\n",
      "Droping 8704 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "17 - Experiment 1 from 22022021\n",
      "Droping 8715 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "18 - Experiment 14 from 22022021\n",
      "Droping 8727 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "19 - Experiment 15 from 22022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "20 - Experiment 2 from 22022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "21 - Experiment 3 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "22 - Experiment 8 from 22022021\n",
      "Droping 8720 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "23 - Experiment 9 from 22022021\n",
      "Droping 8739 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "24 - Experiment 1 from 24022021\n",
      "Droping 8690 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "25 - Experiment 10 from 24022021\n",
      "Droping 8519 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "26 - Experiment 11 from 24022021\n",
      "Droping 8627 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "27 - Experiment 12 from 24022021\n",
      "Droping 8744 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "28 - Experiment 13 from 24022021\n",
      "Droping 8569 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "29 - Experiment 14 from 24022021\n",
      "Droping 8729 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "30 - Experiment 15 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "31 - Experiment 16 from 24022021\n",
      "Droping 8687 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "32 - Experiment 2 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "33 - Experiment 3 from 24022021\n",
      "Droping 8543 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "34 - Experiment 4 from 24022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "35 - Experiment 5 from 24022021\n",
      "Droping 8568 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "36 - Experiment 6 from 24022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "37 - Experiment 7 from 24022021\n",
      "Droping 8603 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "38 - Experiment 8 from 24022021\n",
      "Droping 8738 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "39 - Experiment 9 from 24022021\n",
      "Droping 8713 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "0 - Experiment 4 from 16022021\n",
      "Droping 8736 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "1 - Experiment 5 from 16022021\n",
      "Droping 8706 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "2 - Experiment 6 from 16022021\n",
      "Droping 8680 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "3 - Experiment 3 from 17022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "4 - Experiment 4 from 17022021\n",
      "Droping 8699 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "5 - Experiment 12 from 19022021\n",
      "Droping 8733 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "6 - Experiment 13 from 19022021\n",
      "Droping 5792 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "7 - Experiment 16 from 19022021\n",
      "Droping 8243 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "8 - Experiment 17 from 19022021\n",
      "Droping 8735 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "9 - Experiment 7 from 19022021\n",
      "Droping 8725 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "10 - Experiment 8 from 19022021\n",
      "Droping 8748 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "11 - Experiment 12 from 22022021\n",
      "Droping 8708 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "12 - Experiment 13 from 22022021\n",
      "Droping 8749 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "13 - Experiment 18 from 22022021\n",
      "Droping 8701 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "14 - Experiment 19 from 22022021\n",
      "Droping 5815 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "15 - Experiment 6 from 22022021\n",
      "Droping 8721 duplicated data points\n",
      "Droping 0 data points by null features\n",
      "16 - Experiment 7 from 22022021\n",
      "Droping 8710 duplicated data points\n",
      "Droping 0 data points by null features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "RESULTS = {}\n",
    "\n",
    "FORCE_CELLS_PER_JOINT = {\n",
    "    'Hip': [5, 6],\n",
    "    'Knee': [3, 4, 7, 8],\n",
    "    'Ankle': [1, 2]\n",
    "}\n",
    "\n",
    "DATA_ID = '0011_09082021'\n",
    "\n",
    "for JOINT in ['Hip', 'Knee', 'Ankle']:\n",
    "    RESULTS[JOINT] = {}\n",
    "    \n",
    "    CELLS = FORCE_CELLS_PER_JOINT[JOINT]\n",
    "\n",
    "    H3_LEG = 'L' # L|R\n",
    "\n",
    "    features = ['L{}Pos'.format(JOINT)] + ['F' + str(i) + 'z' for i in FORCE_CELLS_PER_JOINT[JOINT]]\n",
    "    targets = ['F' + str(i) + ax for i in FORCE_CELLS_PER_JOINT[JOINT] for ax in ['x', 'y']]\n",
    "\n",
    "    print('Number of features: {}'.format(len(features)))\n",
    "    print('Selected features: {}'.format(features))\n",
    "    print('\\n')\n",
    "    print('Number of targets: {}'.format(len(targets)))\n",
    "    print('Selected targets: {}'.format(targets))\n",
    "    \n",
    "    # Index to crop the data and use only this section of each experiment (start idx, end idx)\n",
    "    # The indexes can be defined manually defining crop_by_index=(start idx, end idx) or seleted at random setting crop_by_index=True\n",
    "    window_size = 200\n",
    "    crop_by_index = False #(1500, 1700) # True\n",
    "\n",
    "    # Sample the experiment data to use only this sample datapoints\n",
    "    random_sample = True\n",
    "    random_sample_pct = 0.05\n",
    "\n",
    "    targets_dict_train = {}\n",
    "    features_dict_train = {}\n",
    "    for i, exp_path in enumerate(experiments_dirs_path_filter_wos):\n",
    "        print('{} - Experiment {} from {}'.format(i, exp_path.split('/')[-1], exp_path.split('/')[-2]))\n",
    "\n",
    "        # Load targets\n",
    "        targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "\n",
    "        # Load features\n",
    "        exo_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "        # leg_df = pd.read_csv(exp_path + '/leg_processed.csv')\n",
    "        # features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "        features_df = exo_df\n",
    "\n",
    "        idx_aux = targets_df.duplicated(keep='first')\n",
    "        targets_df = targets_df.loc[~idx_aux]\n",
    "        features_df = features_df.loc[~idx_aux]\n",
    "        print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "\n",
    "        # Drop first row to remove noise in the start of the data recording\n",
    "        targets_df = targets_df.iloc[1:]\n",
    "        features_df = features_df.iloc[1:]\n",
    "        # Drop null values\n",
    "        idx = features_df.notna().all(axis=1)\n",
    "        features_df = features_df.loc[idx]\n",
    "        targets_df = targets_df.loc[idx]\n",
    "        print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "        assert(len(features_df) == len(targets_df))\n",
    "        data_df = pd.concat([features_df, targets_df], axis=1)\n",
    "\n",
    "        # Crop the data by the indicated indexes\n",
    "        if crop_by_index:\n",
    "            if crop_by_index == True:\n",
    "                start_idx = random.randint(100, len(data_df) - window_size - 100)\n",
    "                crop_by_index = (start_idx, start_idx + window_size)\n",
    "\n",
    "            data_df = data_df.iloc[crop_by_index[0]:crop_by_index[1]]\n",
    "\n",
    "        if random_sample:\n",
    "            data_df = data_df.sample(frac=random_sample_pct, random_state=0)\n",
    "\n",
    "        # Store the final array\n",
    "        targets_dict_train[i] = data_df[targets].values\n",
    "        features_dict_train[i] = data_df[features].values\n",
    "\n",
    "    targets_dict_test = {}\n",
    "    features_dict_test = {}\n",
    "    for i, exp_path in enumerate(experiments_dirs_path_filter):\n",
    "        print('{} - Experiment {} from {}'.format(i, exp_path.split('/')[-1], exp_path.split('/')[-2]))\n",
    "\n",
    "        # Load targets\n",
    "        targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "\n",
    "        # Load features\n",
    "        exo_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "        # leg_df = pd.read_csv(exp_path + '/leg_processed.csv')\n",
    "        # features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "        features_df = exo_df\n",
    "\n",
    "        idx_aux = targets_df.duplicated(keep='first')\n",
    "        targets_df = targets_df.loc[~idx_aux]\n",
    "        features_df = features_df.loc[~idx_aux]\n",
    "        print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "\n",
    "        # Drop first row to remove noise in the start of the data recording\n",
    "        targets_df = targets_df.iloc[1:]\n",
    "        features_df = features_df.iloc[1:]\n",
    "        # Drop null values\n",
    "        idx = features_df.notna().all(axis=1)\n",
    "        features_df = features_df.loc[idx]\n",
    "        targets_df = targets_df.loc[idx]\n",
    "        print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "        assert(len(features_df) == len(targets_df))\n",
    "        data_df = pd.concat([features_df, targets_df], axis=1)\n",
    "\n",
    "        # Crop the data by the indicated indexes\n",
    "        if crop_by_index:\n",
    "            if crop_by_index == True:\n",
    "                start_idx = random.randint(100, len(data_df) - window_size - 100)\n",
    "                crop_by_index = (start_idx, start_idx + window_size)\n",
    "\n",
    "            data_df = data_df.iloc[crop_by_index[0]:crop_by_index[1]]\n",
    "\n",
    "        if random_sample:\n",
    "            data_df = data_df.sample(frac=random_sample_pct, random_state=0)\n",
    "\n",
    "        # Store the final array\n",
    "        targets_dict_test[i] = data_df[targets].values\n",
    "        features_dict_test[i] = data_df[features].values\n",
    "\n",
    "    X_train = np.concatenate([v for k, v in features_dict_train.items()], axis=0)\n",
    "    Y_train = np.concatenate([v for k, v in targets_dict_train.items()], axis=0)\n",
    "    \n",
    "    X_test = np.concatenate([v for k, v in features_dict_test.items()], axis=0)\n",
    "    Y_test = np.concatenate([v for k, v in targets_dict_test.items()], axis=0)\n",
    "\n",
    "    s = MinMaxScaler().fit(X_train)\n",
    "\n",
    "    X_train_norm = s.transform(X_train)\n",
    "    X_test_norm = s.transform(X_test)\n",
    "\n",
    "    for m in ['RF', 'KNN']:\n",
    "        RESULTS[JOINT][m] = {}\n",
    "        \n",
    "        HS_DATE = {'RF':'11082021',  'XGB': '10082021', 'SVM': '24082021', 'KNN': '24082021'}\n",
    "        \n",
    "\n",
    "        if m == 'XGB':\n",
    "            results = defaultdict(list)\n",
    "            for target in range(Y_train.shape[1]):\n",
    "                model = load(os.path.join(RESULTS_PATH, DATA_ID, '{}_{}_{}'.format(JOINT, m, HS_DATE[m]), '{}_{}_best_model_{}_{}_{}.joblib'.format(JOINT, m, target, HS_DATE[m], DATA_ID)))\n",
    "\n",
    "                dtest = xgb.DMatrix(data=X_test, label=Y_test[:, target])\n",
    "\n",
    "                test_preds = model.predict(dtest)\n",
    "\n",
    "                results['MAE'].append(mean_absolute_error(Y_test[:, target], test_preds))\n",
    "                results['MSE'].append(mean_squared_error(Y_test[:, target], test_preds))\n",
    "                results['R2'].append(r2_score(Y_test[:, target], test_preds))\n",
    "        else:\n",
    "            model = load(os.path.join(RESULTS_PATH, DATA_ID, '{}_{}_{}'.format(JOINT, m, HS_DATE[m]), '{}_{}_best_model_{}_{}.joblib'.format(JOINT, m, HS_DATE[m], DATA_ID)))\n",
    "            \n",
    "            test_preds = model.predict(X_test_norm)\n",
    "            \n",
    "            results = {\n",
    "                'MAE': mean_absolute_error(Y_test, test_preds, multioutput='raw_values'),\n",
    "                'MSE': mean_squared_error(Y_test, test_preds, multioutput='raw_values'),\n",
    "                'R2': r2_score(Y_test, test_preds, multioutput='raw_values')\n",
    "            }\n",
    "\n",
    "        for f, force in enumerate(['Fx', 'Fy']):\n",
    "            RESULTS[JOINT][m][force] = {}\n",
    "            \n",
    "            for loss in ['MAE', 'MSE', 'R2']:\n",
    "                scores = [results[loss][i + f] for i in range(0, len(CELLS) * 2, 2)]\n",
    "\n",
    "                RESULTS[JOINT][m][force][loss] = {}\n",
    "                RESULTS[JOINT][m][force][loss]['mean'] = np.mean(scores)\n",
    "                RESULTS[JOINT][m][force][loss]['std'] = np.std(scores)\n",
    "                \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "imposed-chart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hip': {'RF': {'Fx': {'MAE': {'mean': 12.173984018097835,\n",
       "     'std': 0.7654902486334141},\n",
       "    'MSE': {'mean': 300.1962501785406, 'std': 71.9630240789904},\n",
       "    'R2': {'mean': -0.45553750989934294, 'std': 0.373901479524002}},\n",
       "   'Fy': {'MAE': {'mean': 11.907886679156206, 'std': 7.910354919352675},\n",
       "    'MSE': {'mean': 561.739359002701, 'std': 525.6046766422579},\n",
       "    'R2': {'mean': -0.3160627103720196, 'std': 0.5546410752938556}}},\n",
       "  'KNN': {'Fx': {'MAE': {'mean': 12.2286941170864, 'std': 0.7271589828469232},\n",
       "    'MSE': {'mean': 316.1565701579707, 'std': 71.29681653649797},\n",
       "    'R2': {'mean': -0.5325264348289024, 'std': 0.3720944660396929}},\n",
       "   'Fy': {'MAE': {'mean': 12.644708631701844, 'std': 8.575728679244987},\n",
       "    'MSE': {'mean': 676.3415705379705, 'std': 638.9008022983592},\n",
       "    'R2': {'mean': -0.5258655178231292, 'std': 0.7369223444137619}}}},\n",
       " 'Knee': {'RF': {'Fx': {'MAE': {'mean': 10.552393018561657,\n",
       "     'std': 2.8229732431423757},\n",
       "    'MSE': {'mean': 228.64059514934115, 'std': 108.04496172025009},\n",
       "    'R2': {'mean': 0.24522138426952741, 'std': 0.1699417554864724}},\n",
       "   'Fy': {'MAE': {'mean': 7.389059937813926, 'std': 2.753740028569421},\n",
       "    'MSE': {'mean': 117.14562792090916, 'std': 79.0858844523844},\n",
       "    'R2': {'mean': 0.11475625115648902, 'std': 0.3656877005459148}}},\n",
       "  'KNN': {'Fx': {'MAE': {'mean': 10.858553811715863,\n",
       "     'std': 2.6753363559232026},\n",
       "    'MSE': {'mean': 242.26924889522, 'std': 108.58564800751226},\n",
       "    'R2': {'mean': 0.19545578303712657, 'std': 0.15557617557088751}},\n",
       "   'Fy': {'MAE': {'mean': 7.791135485557499, 'std': 2.833183445279506},\n",
       "    'MSE': {'mean': 132.7417133025286, 'std': 85.42461355378148},\n",
       "    'R2': {'mean': -0.03401239161852643, 'std': 0.5345273363133903}}}},\n",
       " 'Ankle': {'RF': {'Fx': {'MAE': {'mean': 4.7983392772243345,\n",
       "     'std': 0.3864006633388706},\n",
       "    'MSE': {'mean': 49.738921961674464, 'std': 7.867321573967704},\n",
       "    'R2': {'mean': 0.2141559508379578, 'std': 0.4541622729361614}},\n",
       "   'Fy': {'MAE': {'mean': 4.9603816525737185, 'std': 0.5968931344099833},\n",
       "    'MSE': {'mean': 40.4124681605098, 'std': 6.530680920580878},\n",
       "    'R2': {'mean': -0.7408256576588597, 'std': 0.3533744479214256}}},\n",
       "  'KNN': {'Fx': {'MAE': {'mean': 5.277528272323114, 'std': 0.4568864273802764},\n",
       "    'MSE': {'mean': 61.48189101234139, 'std': 11.663501460907323},\n",
       "    'R2': {'mean': 0.015436698357424117, 'std': 0.5899318143783869}},\n",
       "   'Fy': {'MAE': {'mean': 5.056913148299506, 'std': 0.7393063434463363},\n",
       "    'MSE': {'mean': 43.83456884613936, 'std': 9.454903252818276},\n",
       "    'R2': {'mean': -0.892578869782674, 'std': 0.4847396587032693}}}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interior-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hip Fx: -0.4940 ± 0.0009\n",
      "Hip Fy: -0.4210 ± 0.0911\n",
      "Knee Fx: 0.2203 ± 0.0072\n",
      "Knee Fy: 0.0404 ± 0.0844\n",
      "Ankle Fx: 0.1148 ± 0.0679\n",
      "Ankle Fy: -0.8167 ± 0.0657\n"
     ]
    }
   ],
   "source": [
    "for JOINT in ['Hip', 'Knee', 'Ankle']:\n",
    "    for FORCE in ['Fx', 'Fy']:\n",
    "        scores_mean = [RESULTS[JOINT][model][FORCE]['R2']['mean'] for model in ['RF', 'KNN']]        \n",
    "        scores_std = [RESULTS[JOINT][model][FORCE]['R2']['std'] for model in ['RF', 'KNN']]\n",
    "        \n",
    "        print('{} {}: {:.4f} ± {:.4f}'.format(JOINT, FORCE, np.mean(scores_mean), np.std(scores_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-berkeley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
