{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "available-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-redhead",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crazy-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(num_workers=8, scheduler='processes')\n",
    "random.seed(0)\n",
    "\n",
    "# Path where the data is stored\n",
    "SOURCE_PATH = '../../../data'\n",
    "# Directory inside SOURCE_PATH where the original data is stored\n",
    "ORIGINAL_DATA_DIR = '/EXOSAFE'\n",
    "# Directory inside SOURCE_PATH where the derived data is stored\n",
    "DERIVED_DATA_DIR = '/derived_data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Experiment params\n",
    "DATE_EXPERIMENTS = '24022021'\n",
    "N_EXPERIMENTS = 15\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "# ID of the training and test data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0003_11042021'\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "CV = 5\n",
    "# % of the data for the train, validation and test sets\n",
    "SPLIT_PCT = (0.6, 0.2, 0.2) # (train, validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-district",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aggressive-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate force vectors of each force cell to align them\n",
    "rotations = {\n",
    "    1: [180, 90, 0],\n",
    "    2: [180, 90, 0],\n",
    "    3: [180, 0, -90],\n",
    "    4: [0, 0, -90],\n",
    "    5: [0, 0, 0],\n",
    "    6: [0, 180, 0],\n",
    "    7: [0, 90, 0],\n",
    "    8: [0, 0, 90],\n",
    "}\n",
    "\n",
    "def rotate_vector(v, axis, angle):\n",
    "    '''\n",
    "    Args:\n",
    "    - v (np.array): Vector to be rotated\n",
    "    - axis (int): Axis along the rotation is performed\n",
    "    - angle (int): Rotation angle\n",
    "    \n",
    "    Returns:\n",
    "    - (np.array)): Rotated vector\n",
    "    '''\n",
    "    if axis == 0:\n",
    "        # X\n",
    "        v = v.dot(np.array([[1, 0, 0], [0, np.cos(np.radians(angle)), np.sin(np.radians(angle))], [0, np.sin(np.radians(angle)), np.cos(np.radians(angle))]]))\n",
    "    elif axis == 1:\n",
    "        # Y\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), 0, np.sin(np.radians(angle))], [0, 1, 0], [-np.sin(np.radians(angle)), 0, np.cos(np.radians(angle))]]))\n",
    "    elif axis == 2:\n",
    "        # Z\n",
    "        v = v.dot(np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle)), 0], [np.sin(np.radians(angle)), np.cos(np.radians(angle)), 0], [0, 0, 1]]))\n",
    "    else:\n",
    "        raise ValueError('Invalid axis')\n",
    "\n",
    "    return v\n",
    "\n",
    "@dask.delayed\n",
    "def rotate_row(row):\n",
    "    '''\n",
    "    Rotate the force vectors in a row. Dask function.\n",
    "    '''\n",
    "    for i in range(1, N_CELLS + 1):\n",
    "        cols = ['F{}x'.format(str(i)), 'F{}y'.format(str(i)), 'F{}z'.format(str(i))]\n",
    "        for ax in range(3):\n",
    "            row[cols] = rotate_vector(row[cols], ax, rotations[i][ax])\n",
    "            \n",
    "    return row\n",
    "\n",
    "def process_parameters_sheet(params_df):\n",
    "    '''\n",
    "    Process the data in the given pd.DataFrame from the raw excel sheet. \n",
    "    \n",
    "    Args:\n",
    "    - params_df (pd.DataFrame): DataFrame of the parameters excel sheet.\n",
    "    \n",
    "    Returns:\n",
    "    - params_dict (dict): Dictionary with all the parameter in the input DataFrame.\n",
    "    '''\n",
    "    params_dict = {}\n",
    "    params_dict['ExoHipMissalign'] = params_df.iloc[2, 1]\n",
    "    params_dict['ExoKneeMissalign'] = params_df.iloc[2, 2]\n",
    "    params_dict['MarchVelocity'] = params_df.iloc[0, 11]\n",
    "    params_dict['TimeShift'] = params_df.iloc[0, 12]\n",
    "    params_dict['SkinConfig'] = params_df.iloc[0, 13]\n",
    "    \n",
    "    return params_dict\n",
    "\n",
    "def shift_leg_data(df, time_shift, total_len, data_res=0.01):\n",
    "    '''\n",
    "    Shift the data from the leg replica using the known time_shift from the experiment\n",
    "    parameters to match the exoskeleton data in time and lenght.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with the data of the leg replica\n",
    "    - time_shift (float): Shifting time to applied to the data.\n",
    "    - total_len (int): Total desired lenght for the data.\n",
    "    - data_res (float): Data resolution (in seconds).\n",
    "    \n",
    "    Returns:\n",
    "    - (pd.DataFrame): DataFrame with the data of the leg replica shifted.\n",
    "    '''\n",
    "    idx_start = math.ceil(time_shift / data_res)\n",
    "    idx_end = total_len + idx_start\n",
    "    return df.iloc[idx_start:idx_end].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessible-brunei",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1:\n",
      "[########################################] | 100% Completed |  1min  6.0s\n",
      "\n",
      "Processing file 2:\n",
      "[########################################] | 100% Completed |  1min  4.3s\n",
      "\n",
      "Processing file 3:\n",
      "[########################################] | 100% Completed |  1min  3.8s\n",
      "\n",
      "Processing file 4:\n",
      "[########################################] | 100% Completed |  1min  6.3s\n",
      "\n",
      "Processing file 5:\n",
      "[########################################] | 100% Completed |  1min  4.4s\n",
      "\n",
      "Processing file 6:\n",
      "[########################################] | 100% Completed |  1min 11.3s\n",
      "\n",
      "Processing file 7:\n",
      "[########################################] | 100% Completed |  1min  3.9s\n",
      "\n",
      "Processing file 8:\n",
      "[########################################] | 100% Completed |  1min 10.8s\n",
      "\n",
      "Processing file 9:\n",
      "[########################################] | 100% Completed |  1min  6.1s\n",
      "\n",
      "Processing file 10:\n",
      "[########################################] | 100% Completed |  1min  2.6s\n",
      "\n",
      "Processing file 11:\n",
      "[########################################] | 100% Completed |  1min  2.7s\n",
      "\n",
      "Processing file 12:\n",
      "[########################################] | 100% Completed |  1min  9.5s\n",
      "\n",
      "Processing file 13:\n",
      "[########################################] | 100% Completed |  1min  5.0s\n",
      "\n",
      "Processing file 14:\n",
      "[########################################] | 100% Completed |  1min  3.7s\n",
      "\n",
      "Processing file 15:\n",
      "[########################################] | 100% Completed |  1min  3.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_EXPERIMENTS):\n",
    "    print('Processing file {}:'.format(i + 1))\n",
    "    # Create the directory to save the resulting data\n",
    "    save_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i + 1))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Select only the relevant excel sheets \n",
    "    sheets = ['Parameters', 'RawForces', 'ForceCells', 'H3raw', 'H3processed', 'Leg-Replica']\n",
    "    # Load the data\n",
    "    data_df = pd.read_excel(SOURCE_PATH + ORIGINAL_DATA_DIR + '/' + DATE_EXPERIMENTS + '/0{}-'.format(i + 1) + DATE_EXPERIMENTS + '.xlsx', sheet_name=sheets)\n",
    "\n",
    "    # Pre-process the data\n",
    "    data_df[sheets[0]] = process_parameters_sheet(data_df[sheets[0]])\n",
    "    \n",
    "    # Apply the rotation matrix to each force vector\n",
    "    forces_ddf = dd.from_pandas(data_df[sheets[2]], npartitions=int(len(data_df[sheets[2]]) / 100))\n",
    "    forces_ddf = forces_ddf.apply(rotate_row, axis=1, meta=forces_ddf)\n",
    "    with ProgressBar():\n",
    "        data_df[sheets[2]] = forces_ddf.compute()\n",
    "        \n",
    "        \n",
    "    leg_df_raw = data_df[sheets[5]].iloc[:, :3]\n",
    "    # Correct the time shift between the data from the leg and the data from the exo\n",
    "    leg_df_processed = shift_leg_data(data_df[sheets[5]].iloc[:, 3:], data_df[sheets[0]]['TimeShift'], len(data_df[sheets[4]]))\n",
    "    \n",
    "    assert(len(leg_df_processed) == len(data_df[sheets[4]]) == len(data_df[sheets[2]]))    \n",
    "\n",
    "    json.dump(data_df[sheets[0]], open(save_dir + '/parameters.json', 'w'))\n",
    "    data_df[sheets[1]].to_csv(save_dir + '/force_cells_raw.csv', index=False)\n",
    "    data_df[sheets[2]].to_csv(save_dir + '/force_cells_processed.csv', index=False)\n",
    "    data_df[sheets[3]].to_csv(save_dir + '/H3_raw.csv', index=False)\n",
    "    data_df[sheets[4]].to_csv(save_dir + '/H3_processed.csv', index=False)\n",
    "    leg_df_raw.to_csv(save_dir + '/leg_raw.csv', index=False)\n",
    "    leg_df_processed.to_csv(save_dir + '/leg_processed.csv', index=False)\n",
    "    \n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-theta",
   "metadata": {},
   "source": [
    "## Features and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "constant-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 15\n",
      "Selected features: ['LHipPos', 'LHipVel', 'LHipAcc', 'LHipTorque', 'LKneePos', 'LKneeVel', 'LKneeAcc', 'LKneeTorque', 'LAnklePos', 'LAnkleVel', 'LAnkleAcc', 'LAnkleTorque', 'LegKneePositionFiltered', 'LegKneeVelocityFiltered', 'LegKneeTorqueFiltered']\n",
      "\n",
      "\n",
      "Number of targets: 24\n",
      "Selected targets: ['F1x', 'F1y', 'F1z', 'F2x', 'F2y', 'F2z', 'F3x', 'F3y', 'F3z', 'F4x', 'F4y', 'F4z', 'F5x', 'F5y', 'F5z', 'F6x', 'F6y', 'F6z', 'F7x', 'F7y', 'F7z', 'F8x', 'F8y', 'F8z']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "features = [H3_LEG + a + m for a in ['Hip', 'Knee', 'Ankle'] for m in ['Pos', 'Vel', 'Acc', 'Torque']] + ['LegKnee{}Filtered'.format(m) for m in ['Position', 'Velocity', 'Torque']]\n",
    "targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charming-module",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping 8722 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 1 -> X: (8721, 15), Y: (8721, 24) \n",
      "\n",
      "Droping 8736 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 2 -> X: (8735, 15), Y: (8735, 24) \n",
      "\n",
      "Droping 8589 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 3 -> X: (8588, 15), Y: (8588, 24) \n",
      "\n",
      "Droping 8726 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 4 -> X: (8725, 15), Y: (8725, 24) \n",
      "\n",
      "Droping 8624 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 5 -> X: (8623, 15), Y: (8623, 24) \n",
      "\n",
      "Droping 8760 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 6 -> X: (8759, 15), Y: (8759, 24) \n",
      "\n",
      "Droping 8639 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 7 -> X: (8638, 15), Y: (8638, 24) \n",
      "\n",
      "Droping 8773 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 8 -> X: (8772, 15), Y: (8772, 24) \n",
      "\n",
      "Droping 8769 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 9 -> X: (8768, 15), Y: (8768, 24) \n",
      "\n",
      "Droping 8705 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 10 -> X: (8704, 15), Y: (8704, 24) \n",
      "\n",
      "Droping 8659 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 11 -> X: (8658, 15), Y: (8658, 24) \n",
      "\n",
      "Droping 8793 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 12 -> X: (8792, 15), Y: (8792, 24) \n",
      "\n",
      "Droping 8599 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 13 -> X: (8598, 15), Y: (8598, 24) \n",
      "\n",
      "Droping 8750 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 14 -> X: (8749, 15), Y: (8749, 24) \n",
      "\n",
      "Droping 8717 duplicated data points\n",
      "Droping 1 data points by null features\n",
      "Experiment 15 -> X: (8716, 15), Y: (8716, 24) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i in range(1, N_EXPERIMENTS + 1):\n",
    "    # Define the path to load the data\n",
    "    data_dir = os.path.join(SOURCE_PATH + DERIVED_DATA_DIR, DATE_EXPERIMENTS, str(i))\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(data_dir + '/force_cells_processed.csv')\n",
    "    \n",
    "    # Load features\n",
    "    exo_df = pd.read_csv(data_dir + '/H3_processed.csv')\n",
    "    leg_df = pd.read_csv(data_dir + '/leg_processed.csv')\n",
    "    features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "    \n",
    "    idx_aux = targets_df.duplicated(keep='first')\n",
    "    targets_df = targets_df.loc[~idx_aux]\n",
    "    features_df = features_df.loc[~idx_aux]\n",
    "    print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "    \n",
    "    # Rename columns to manage with some typos\n",
    "    features_df = features_df.rename(columns={'LankleTorque': 'LAnkleTorque', 'RankleTorque': 'RAnkleTorque'})\n",
    "\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    # Store the final array\n",
    "    targets_dict[i] = targets_df[targets].values\n",
    "    features_dict[i] = features_df[features].values\n",
    "    \n",
    "    print('Experiment {} -> X: {}, Y: {} \\n'.format(i, features_dict[i].shape, targets_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-congo",
   "metadata": {},
   "source": [
    "## Normalization and split for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naval-surname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments ids (12): [2, 11, 10, 6, 12, 3, 4, 8, 9, 5, 1, 15]\n",
      "Test experiments ids (3): [13, 7, 14]\n"
     ]
    }
   ],
   "source": [
    "experiments = list(range(1, N_EXPERIMENTS + 1))\n",
    "random.shuffle(experiments)\n",
    "\n",
    "train_experiments = experiments[:int(N_EXPERIMENTS * (SPLIT_PCT[0] + SPLIT_PCT[1]))]\n",
    "test_experiments = experiments[-int(N_EXPERIMENTS * SPLIT_PCT[2]):]\n",
    "\n",
    "print('Train experiments ids ({}): {}'.format(len(train_experiments), train_experiments))\n",
    "print('Test experiments ids ({}): {}'.format(len(test_experiments), test_experiments))\n",
    "\n",
    "# Check that no test experiment is in train\n",
    "assert(not any([i in test_experiments for i in train_experiments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "correct-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> X: (104561, 15), Y: (104561, 24)\n",
      "Test -> X: (25985, 15), Y: (25985, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([features_dict[i] for i in train_experiments], axis=0)\n",
    "Y_train = np.concatenate([targets_dict[i] for i in train_experiments], axis=0)\n",
    "X_test = np.concatenate([features_dict[i] for i in test_experiments], axis=0)\n",
    "Y_test = np.concatenate([targets_dict[i] for i in test_experiments], axis=0)\n",
    "\n",
    "print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "print('Test -> X: {}, Y: {}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unauthorized-cameroon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test -> \n",
      " min: [-1.31007256 -1.21947194 -2.39689804 -1.98521388 -0.72154655 -2.87493359\n",
      " -4.27516027 -1.81340945 -2.38972514 -2.35581996 -1.61207976 -1.77671285\n",
      " -0.79982154 -2.54003847 -2.684084  ], \n",
      " max: [ 1.6556621   2.07734193  2.74539279  3.07156386  2.83887633  2.19956122\n",
      "  2.22465618  3.90613018  1.65454986  2.09792576  3.56101584 11.30813031\n",
      "  3.02245127  3.71964724  0.77958802], \n",
      " mean: [ 2.16157527e-03  1.70658398e-03 -8.90862413e-04  3.66093570e-03\n",
      " -1.32857969e-02 -1.00024901e-03  3.47354476e-04  8.04899012e-02\n",
      "  5.92649776e-04 -1.47336199e-03 -1.86910567e-04  5.57907982e-01\n",
      " -7.28462021e-02  2.18776814e-03 -6.56859997e-01], \n",
      " std: [0.99188675 0.99029596 0.97062917 0.98235935 0.96690402 0.9477156\n",
      " 0.91826453 1.17648008 0.9993203  1.00255372 1.00280558 1.43140207\n",
      " 0.96839934 0.93785194 1.07885434]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Only transform test data because the training data has to be split in CV folds\n",
    "X_test_norm =  scaler.transform(X_test)\n",
    "\n",
    "print('Test -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_test_norm, axis=0), np.max(X_test_norm, axis=0), np.mean(X_test_norm, axis=0), np.std(X_test_norm, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "simplified-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(save_dir + '/X_test_' + DATA_ID + '.npy', X_test_norm)    \n",
    "np.save(save_dir + '/Y_test_' + DATA_ID + '.npy', Y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "northern-agency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train experiments ids (9): [3, 2, 6, 8, 4, 15, 5, 11, 1]\n",
      "Validation experiments ids (3): [12, 10, 9]\n",
      "Train -> X: (78297, 15), Y: (78297, 24)\n",
      "Validation -> X: (26264, 15), Y: (26264, 24)\n",
      "Train -> \n",
      " min: [-1.33046351 -1.26041507 -3.07943234 -2.32243966 -0.72124103 -2.88740256\n",
      " -4.3262475  -1.84145692 -2.55391811 -2.36680569 -4.68163954 -2.20758705\n",
      " -0.75096241 -2.50721035 -3.28458617], \n",
      " max: [1.66244978 2.11289163 2.8470798  3.18053987 2.82469056 2.17309058\n",
      " 2.52474468 3.99678025 1.65037305 3.29353628 3.691768   7.21402248\n",
      " 3.02857536 3.89603932 0.65756598], \n",
      " mean: [-1.08899611e-18  8.07672114e-18 -5.44498054e-18 -2.90398962e-18\n",
      " -3.62998703e-18 -4.26523476e-18 -4.26523476e-18  1.08899611e-17\n",
      "  4.90048249e-18 -1.54274449e-18 -3.62998703e-18  5.80797924e-17\n",
      " -1.45199481e-18  1.45199481e-18  2.38127149e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.29641619 -1.20590834 -2.67022555 -2.08122493 -0.72108716 -2.87900821\n",
      " -4.25836556 -1.8748658  -2.55272371 -2.36251222 -4.72159551 -1.68099111\n",
      " -0.93480014 -2.55426318 -3.28358792], \n",
      " max: [1.64922374 2.13283356 2.89026582 3.16188246 2.8267086  2.19684972\n",
      " 2.51911312 3.57469654 1.64451494 3.27429773 3.65607091 6.17996689\n",
      " 3.01971692 3.71036199 0.69709277], \n",
      " mean: [ 3.57288800e-03  1.94588330e-03  9.19453073e-04 -6.49409009e-02\n",
      " -6.26871941e-03  8.01531125e-04 -1.21646612e-03  6.26176128e-03\n",
      " -3.61269487e-03  7.65463022e-04  1.21161940e-03  9.41260206e-02\n",
      " -5.04497542e-02 -2.05662449e-03 -9.16754472e-01], \n",
      " std: [0.98521024 0.98753058 0.96949596 1.07369294 0.98726658 0.97987081\n",
      " 0.9718146  1.11648744 1.00143034 1.0014857  1.00180663 0.82428239\n",
      " 1.01147922 0.99623739 1.24672877]\n",
      "\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train experiments ids (9): [3, 12, 4, 1, 5, 8, 6, 10, 11]\n",
      "Validation experiments ids (3): [15, 2, 9]\n",
      "Train -> X: (78342, 15), Y: (78342, 24)\n",
      "Validation -> X: (26219, 15), Y: (26219, 24)\n",
      "Train -> \n",
      " min: [-1.3400184  -1.26657162 -3.10056314 -2.21181993 -0.71782263 -2.82541993\n",
      " -4.21121316 -1.93923271 -2.54967363 -2.36590792 -4.717002   -2.32905313\n",
      " -0.91562976 -2.46856948 -2.66791952], \n",
      " max: [1.66732102 2.14031346 2.91022205 3.10578557 2.77881292 2.14989888\n",
      " 2.45846538 3.75066685 1.64763427 3.29131374 3.68755938 6.56786445\n",
      " 2.94132239 3.68410588 0.81530777], \n",
      " mean: [-3.59162293e-17  3.13813518e-17 -5.16976027e-18  1.85929975e-17\n",
      "  4.87952812e-17 -4.17208724e-18  5.71394557e-18  6.67533958e-17\n",
      " -1.94092754e-17 -1.17906813e-18 -1.31624818e-17 -2.90232156e-18\n",
      " -1.08111478e-16  2.29238054e-17 -2.32185725e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.31000263 -1.16503241 -2.68853994 -1.61599544 -0.71767099 -2.80708888\n",
      " -4.20071947 -1.84486278 -2.31135219 -2.36161681 -1.62916506 -2.26703039\n",
      " -0.73672837 -2.34668478 -2.63644522], \n",
      " max: [1.66425088 2.02002215 2.60326658 3.09297171 2.73988835 2.02859494\n",
      " 2.44019597 4.30864674 1.65123188 2.11974642 3.5388226  7.6647167\n",
      " 2.82550149 3.76602618 0.79015626], \n",
      " mean: [-8.93369944e-03 -2.39512099e-03  1.18138857e-03  6.62928989e-02\n",
      " -3.41561201e-02  1.25675883e-03  9.62638219e-04  2.29074681e-01\n",
      "  5.10914060e-03 -8.56533411e-04 -1.68470078e-04  1.50251727e-01\n",
      " -7.28304324e-02 -8.91843375e-04  1.60913171e-02], \n",
      " std: [1.00438644 1.00353884 0.99703157 0.93725462 0.92736825 0.89034281\n",
      " 0.85961794 1.32958334 0.99821485 0.9992896  0.99763524 1.06223593\n",
      " 0.8991358  0.85481332 0.98551292]\n",
      "\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train experiments ids (9): [15, 12, 4, 6, 1, 8, 9, 2, 3]\n",
      "Validation experiments ids (3): [5, 11, 10]\n",
      "Train -> X: (78576, 15), Y: (78576, 24)\n",
      "Validation -> X: (25985, 15), Y: (25985, 24)\n",
      "Train -> \n",
      " min: [-1.32894418 -1.2593966  -3.09117093 -2.32528397 -0.72550007 -2.95762036\n",
      " -4.45534928 -1.79117056 -2.55485549 -2.36710754 -4.7262897  -2.24816824\n",
      " -0.74745451 -2.5939626  -3.27805704], \n",
      " max: [1.66539029 2.11237494 2.85642661 3.20054812 2.87661066 2.24209538\n",
      " 2.60051898 3.69698624 1.64989589 3.27555753 3.65950087 7.25246043\n",
      " 3.09940989 4.01276436 0.66185037], \n",
      " mean: [-1.70003607e-17 -3.21921724e-17  2.23355803e-17 -4.05114979e-17\n",
      " -7.59590585e-17  1.41066823e-17 -2.12504509e-18  8.97040310e-17\n",
      " -2.38547615e-16  2.55005411e-17  5.61780537e-18 -3.90646587e-17\n",
      " -1.22981333e-16  1.26598431e-18  1.96770132e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.29530405 -1.26117967 -2.4209494  -2.08306753 -0.72455436 -2.94902505\n",
      " -4.37689316 -1.76776836 -2.55605036 -2.36157016 -4.68629498 -2.06978811\n",
      " -0.93500662 -2.6284821  -3.27905226], \n",
      " max: [1.66049877 2.13231833 2.89976628 3.18181322 2.8783986  2.24835319\n",
      " 2.54227093 2.26827514 1.64566067 3.2948014  3.69523257 4.23727166\n",
      " 3.10844731 3.82159055 0.6905136 ], \n",
      " mean: [ 1.22687792e-02 -7.37028015e-04 -2.21627690e-03 -3.85473352e-02\n",
      "  2.21920875e-02 -3.56038531e-03 -1.68342561e-04 -1.50034954e-01\n",
      " -8.18726273e-03  2.19577704e-03  7.61154173e-04  7.14266641e-03\n",
      "  2.31479166e-02  3.46589675e-03 -9.41719147e-01], \n",
      " std: [0.98692185 0.98769468 0.98362845 1.09148277 1.0493716  1.07365518\n",
      " 1.0884313  0.83208174 1.00300749 1.0026127  1.00571923 0.86319302\n",
      " 1.09093457 1.11072328 1.2240176 ]\n",
      "\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train experiments ids (9): [15, 2, 4, 9, 5, 3, 1, 11, 12]\n",
      "Validation experiments ids (3): [10, 6, 8]\n",
      "Train -> X: (78326, 15), Y: (78326, 24)\n",
      "Validation -> X: (26235, 15), Y: (26235, 24)\n",
      "Train -> \n",
      " min: [-1.33599185 -1.26468908 -2.68693708 -2.26306943 -0.72109465 -2.88688468\n",
      " -4.32865658 -1.83260374 -2.55118013 -2.36479106 -4.71275276 -2.26499826\n",
      " -0.74753651 -2.5161373  -2.63482827], \n",
      " max: [1.66640377 2.11916334 2.86511812 3.11620261 2.82458787 2.19117471\n",
      " 2.52110102 3.93326953 1.64980845 3.29036961 3.68506922 7.27693836\n",
      " 3.01403476 3.88727282 0.80143321], \n",
      " mean: [-4.39065807e-17 -1.26095345e-17  1.75082026e-17  1.45145721e-18\n",
      " -1.76714916e-16  4.69455693e-18 -3.17506266e-18 -5.22524597e-17\n",
      " -6.35919692e-17  1.55124490e-17  1.69639062e-17  3.77378876e-17\n",
      " -6.53155746e-18 -1.54217329e-18 -6.96699463e-17], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.3017683  -1.21001196 -3.09872119 -2.02727723 -0.72124851 -2.87849047\n",
      " -4.2524296  -1.80801732 -2.31123528 -2.36104398 -2.12573579 -1.96866036\n",
      " -0.93050035 -2.54959449 -2.61382411], \n",
      " max: [1.66329319 2.13916761 2.90857614 3.09796462 2.82634787 2.1972861\n",
      " 2.5267362  3.41833621 1.64621077 2.09773945 3.52254482 5.39579814\n",
      " 2.99928163 3.70198222 0.82642606], \n",
      " mean: [-0.00164519  0.00062426  0.00135055 -0.03500838 -0.00659104  0.00268196\n",
      "  0.00025526  0.03998485 -0.00069717  0.00016032  0.00169221 -0.0210134\n",
      " -0.05083894 -0.00465132  0.08079087], \n",
      " std: [0.9978589  0.99999745 0.99471509 0.98544927 0.9868514  0.97978128\n",
      " 0.97437851 1.04620059 0.99830443 0.99783701 0.99443207 0.88304676\n",
      " 0.9924962  0.98788326 0.9574925 ]\n",
      "\n",
      "\n",
      "\n",
      "Fold 5\n",
      "Train experiments ids (9): [6, 1, 9, 10, 5, 4, 15, 11, 12]\n",
      "Validation experiments ids (3): [3, 8, 2]\n",
      "Train -> X: (78466, 15), Y: (78466, 24)\n",
      "Validation -> X: (26095, 15), Y: (26095, 24)\n",
      "Train -> \n",
      " min: [-1.34034318 -1.26738729 -3.11361405 -2.08133327 -0.72013546 -2.88261883\n",
      " -4.32404836 -1.79467106 -2.54912389 -2.36124458 -4.71258851 -2.44341499\n",
      " -0.8958817  -2.52422978 -2.30665795], \n",
      " max: [1.67053951 2.1452577  2.92215842 3.10520865 2.82364883 2.19335482\n",
      " 2.52380638 3.882681   1.64979237 3.29006005 3.68384681 7.69488488\n",
      " 3.0044381  3.85027768 0.95860749], \n",
      " mean: [ 5.43325315e-19 -7.33489175e-18 -2.25480006e-17  3.62216876e-17\n",
      "  8.29476647e-17  1.00062412e-17  1.57111570e-17  1.31846943e-16\n",
      " -5.58719532e-17 -1.70694703e-17 -1.15456629e-18  0.00000000e+00\n",
      "  7.17189415e-17 -1.02779039e-17  1.04318460e-16], \n",
      " std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Valid -> \n",
      " min: [-1.31027654 -1.16567641 -2.43109708 -2.21666421 -0.71988672 -2.86391693\n",
      " -4.31327434 -1.70891856 -2.29736267 -2.36553446 -1.67496555 -2.38049528\n",
      " -0.68103027 -2.4173307   0.28415681], \n",
      " max: [1.67209208 2.0218924  2.52865727 3.12331342 2.7841994  2.0695968\n",
      " 2.50504892 3.37565325 1.64408959 2.08230692 3.483889   5.69617493\n",
      " 2.89958416 3.79793015 0.92592438], \n",
      " mean: [-1.19448621e-03  2.90134087e-03  5.44251447e-04  8.47640856e-02\n",
      " -5.26483109e-03  1.13480290e-03 -3.50610844e-04  7.88398393e-02\n",
      "  2.49602582e-03 -2.09062233e-03 -7.58942327e-04 -1.70126889e-01\n",
      "  4.63827360e-02 -1.96856403e-03  7.93773309e-01], \n",
      " std: [1.01119425 1.01030008 1.01359682 0.9531894  0.98244444 0.97312922\n",
      " 0.96976636 0.98313805 0.99631339 0.99813145 0.99373931 1.11589291\n",
      " 0.94671334 0.94815739 0.12715487]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_id in range(CV):\n",
    "    print('Fold {}'.format(fold_id + 1))\n",
    "    \n",
    "    # From the training experiment ids, a different validation set is created for each fold\n",
    "    random.shuffle(train_experiments)\n",
    "    train_experiments_fold = train_experiments[:int(N_EXPERIMENTS * SPLIT_PCT[0])]\n",
    "    valid_experiments_fold = train_experiments[-int(N_EXPERIMENTS * SPLIT_PCT[1]):]\n",
    "\n",
    "    print('Train experiments ids ({}): {}'.format(len(train_experiments_fold), train_experiments_fold))\n",
    "    print('Validation experiments ids ({}): {}'.format(len(valid_experiments_fold), valid_experiments_fold))\n",
    "\n",
    "    # Check that no validation experiments are in train\n",
    "    assert(not any([i in valid_experiments_fold for i in train_experiments_fold]))\n",
    "    # Check that no test experiments are in train or validation folds\n",
    "    assert(not any([i in test_experiments for i in train_experiments_fold]))\n",
    "    assert(not any([i in test_experiments for i in valid_experiments_fold]))\n",
    "    \n",
    "    X_train = np.concatenate([features_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    Y_train = np.concatenate([targets_dict[i] for i in train_experiments_fold], axis=0)\n",
    "    X_valid = np.concatenate([features_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "    Y_valid = np.concatenate([targets_dict[i] for i in valid_experiments_fold], axis=0)\n",
    "\n",
    "    print('Train -> X: {}, Y: {}'.format(X_train.shape, Y_train.shape))\n",
    "    print('Validation -> X: {}, Y: {}'.format(X_valid.shape, Y_valid.shape))\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_valid_norm =  scaler.transform(X_valid)\n",
    "\n",
    "    print('Train -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_train_norm, axis=0), np.max(X_train_norm, axis=0), np.mean(X_train_norm, axis=0), np.std(X_train_norm, axis=0)))\n",
    "    print('Valid -> \\n min: {}, \\n max: {}, \\n mean: {}, \\n std: {}\\n'.format(np.min(X_valid_norm, axis=0), np.max(X_valid_norm, axis=0), np.mean(X_valid_norm, axis=0), np.std(X_valid_norm, axis=0)))\n",
    "    \n",
    "    save_dir = os.path.join(RESULTS_PATH, DATA_ID, 'data')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    np.save(save_dir + '/X_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_train_norm)    \n",
    "    np.save(save_dir + '/X_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), X_valid_norm)    \n",
    "    np.save(save_dir + '/Y_train_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_train)    \n",
    "    np.save(save_dir + '/Y_valid_cv{}_{}.npy'.format(fold_id + 1, DATA_ID), Y_valid)      \n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-coast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
