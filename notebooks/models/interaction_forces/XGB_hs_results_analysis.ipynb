{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-corpus",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "# ID of the training and test data resulting from this notebook, stored in RESULTS_PATH\n",
    "DATA_ID = '0004_15042021'\n",
    "# Hyperparameters search date\n",
    "HS_DATE = '15042021'\n",
    "# Number of folds in cross-validation\n",
    "CV = 6\n",
    "\n",
    "print('Model trained with data: ' + DATA_ID)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-cleaners",
   "metadata": {},
   "source": [
    "## Hyperparameters seach analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_files_ls = glob.glob(os.path.join(RESULTS_PATH, DATA_ID, 'XGB_{}'.format(HS_DATE), 'XGB_{}_*.json'.format(HS_DATE)))\n",
    "\n",
    "print('Number of results files: {}'.format(len(results_files_ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the results and generates a pandas dataframe\n",
    "results_ls = []\n",
    "for results_file in results_files_ls:\n",
    "    with open(results_file) as json_file:\n",
    "        results_dict = json.load(json_file)\n",
    "        \n",
    "    dict_aux = {}\n",
    "    dict_aux['params_ID'] = results_dict['id']\n",
    "    for key, value in results_dict['parameters'].items():\n",
    "        dict_aux['param_' + key] = value\n",
    "    for key, value in results_dict['cv_results'].items():\n",
    "        dict_aux['__'.join([key, 'mean'])] = np.mean(value)\n",
    "        dict_aux['__'.join([key, 'std'])] = np.std(value)\n",
    "\n",
    "    results_ls.append(dict_aux)\n",
    "        \n",
    "results_df = pd.DataFrame(results_ls)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the scores by force axis in only one sortable score\n",
    "for subset in ['Train', 'Test']:\n",
    "    for loss in ['MAE', 'MSE', 'R2']:\n",
    "        results_df[subset + '_' + loss] = results_df[[subset + '_' + force + '_' + loss + '_mean__mean' for force in ['Fx', 'Fy', 'Fz']]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by the most relevant score\n",
    "results_df = results_df.sort_values(['Test_R2'], ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = dict(results_df.iloc[0][[col for col in results_df.columns if 'param_' in col]])\n",
    "best_params = {key.replace('param_', ''): value for key, value in best_params.items()}\n",
    "print('Best parameters: {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-graham",
   "metadata": {},
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = np.load(os.path.join(RESULTS_PATH, DATA_ID, 'data', 'X_train_{}.npy'.format(DATA_ID)))\n",
    "X_test = np.load(os.path.join(RESULTS_PATH, DATA_ID, 'data', 'X_test_{}.npy'.format(DATA_ID)))\n",
    "Y_train = np.load(os.path.join(RESULTS_PATH, DATA_ID, 'data', 'Y_train_{}.npy'.format(DATA_ID)))\n",
    "Y_test = np.load(os.path.join(RESULTS_PATH, DATA_ID, 'data', 'Y_test_{}.npy'.format(DATA_ID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "tr_time = []\n",
    "for target in range(Y_train.shape[1]):\n",
    "\n",
    "    dtrain = xgb.DMatrix(data=X_train, label=Y_train[:, target])\n",
    "    dtest = xgb.DMatrix(data=X_test, label=Y_test[:, target])\n",
    "\n",
    "    callbacks = [xgb.callback.EarlyStopping(rounds=5, metric_name='rmse', maximize=False, save_best=True)]\n",
    "    \n",
    "    t_start = time.time()\n",
    "    model = xgb.train(best_params, dtrain, evals=[(dtest, 'rmse')], callbacks=callbacks, verbose_eval=False)\n",
    "    tr_time.append(time.time() - t_start)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_model(os.path.join(RESULTS_PATH, DATA_ID, 'XGB_{}'.format(HS_DATE), 'XGB_best_model_{}_{}.joblib'.format(HS_DATE, DATA_ID)))\n",
    "    \n",
    "    train_preds = model.predict(dtrain)\n",
    "    test_preds = model.predict(dtest)\n",
    "\n",
    "    results['Train_MAE'].append(mean_absolute_error(Y_train[:, target], train_preds))\n",
    "    results['Train_MSE'].append(mean_squared_error(Y_train[:, target], train_preds))\n",
    "    results['Train_R2'].append(r2_score(Y_train[:, target], train_preds))\n",
    "    results['Test_MAE'].append(mean_absolute_error(Y_test[:, target], test_preds))\n",
    "    results['Test_MSE'].append(mean_squared_error(Y_test[:, target], test_preds))\n",
    "    results['Test_R2'].append(r2_score(Y_test[:, target], test_preds))\n",
    "\n",
    "print('Training time: {:.4f}'.format(cv_results['fit_time'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the score for each axis of each force cell\n",
    "# for subset in ['Train', 'Test']:\n",
    "#     for f, force in enumerate(['Fx', 'Fy', 'Fz']):\n",
    "#         for c in range(N_CELLS):\n",
    "#             for loss in ['MAE', 'MSE', 'R2']:\n",
    "#                 scores = [results[subset][loss][i + f] for i in range(0, N_CELLS * 3, 3)]\n",
    "#                 print('{} {}{}{} {}: {:.4f}'.format(subset, force[0], c + 1, force[-1], loss, scores[c]))\n",
    "# print('\\n')\n",
    "\n",
    "# Display the score mean and standard deviation of each axis\n",
    "for subset in ['Train', 'Test']:\n",
    "    for f, force in enumerate(['Fx', 'Fy', 'Fz']):\n",
    "        for loss in ['MAE', 'MSE', 'R2']:\n",
    "            scores = [results['_'.join([subset, loss])][i + f] for i in range(0, N_CELLS * 3, 3)]\n",
    "            print(' '.join([subset, force, loss]) + ': {:.4f} Â± {:.4f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,15))\n",
    "# plt.scatter(Y_train[:, 3], Y_train[:, 4], label='true', alpha=0.3)\n",
    "# plt.scatter(train_preds[:, 3], train_preds[:, 4], label='preds', alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(20,15))\n",
    "# plt.scatter(Y_train[:100, 3], Y_train[:100, 4], label='true', alpha=0.3)\n",
    "# plt.scatter(train_preds[:100, 3], train_preds[:100, 4], label='preds', alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(20,15))\n",
    "# plt.scatter(Y_test[:, 3], Y_test[:, 4], label='true', alpha=0.3)\n",
    "# plt.scatter(test_preds[:, 3], test_preds[:, 4], label='preds', alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-equality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
