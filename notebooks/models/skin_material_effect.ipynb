{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-nirvana",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of experiments\n",
    "N_EXPERIMENTS = 63\n",
    "# Directory where the derived data is stored\n",
    "DERIVED_DATA_DIR = '../../../data'\n",
    "\n",
    "experiments_dirs_path = glob.glob(DERIVED_DATA_DIR + '/*/*')\n",
    "assert(len(experiments_dirs_path) == N_EXPERIMENTS)\n",
    "\n",
    "# Path where the results are stored\n",
    "RESULTS_PATH = '../../../results'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths from experiments without skin\n",
    "experiments_dirs_path_filter_wos = []\n",
    "\n",
    "exp_params = {}\n",
    "for exp_path in experiments_dirs_path:\n",
    "    with open(exp_path + '/parameters.json') as f:\n",
    "        exp_params[exp_path] = json.load(f)\n",
    "        \n",
    "    if int(exp_params[exp_path]['SkinConfig']) == 0 or exp_params[exp_path]['SkinConfig'] == 'NaN':\n",
    "        experiments_dirs_path_filter_wos.append(exp_path)\n",
    "        \n",
    "print('Final number of experiments:', len(experiments_dirs_path_filter_wos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths from experiments that pass certain filter\n",
    "experiments_dirs_path_filter = []\n",
    "\n",
    "exp_params = {}\n",
    "for exp_path in experiments_dirs_path:\n",
    "    with open(exp_path + '/parameters.json') as f:\n",
    "        exp_params[exp_path] = json.load(f)\n",
    "        \n",
    "    if int(exp_params[exp_path]['SkinConfig']) in [3]:\n",
    "        experiments_dirs_path_filter.append(exp_path)\n",
    "        \n",
    "print('Final number of experiments:', len(experiments_dirs_path_filter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-ebony",
   "metadata": {},
   "source": [
    "## Skin-like material evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-festival",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate skin-like materials experiments over baseline models\n",
    "RESULTS = {}\n",
    "\n",
    "FORCE_CELLS_PER_JOINT = {\n",
    "    'Hip': [5, 6],\n",
    "    'Knee': [3, 4, 7, 8],\n",
    "    'Ankle': [1, 2]\n",
    "}\n",
    "\n",
    "DATA_ID = '0011_09082021'\n",
    "\n",
    "for JOINT in ['Hip', 'Knee', 'Ankle']:\n",
    "    RESULTS[JOINT] = {}\n",
    "    \n",
    "    CELLS = FORCE_CELLS_PER_JOINT[JOINT]\n",
    "\n",
    "    H3_LEG = 'L' # L|R\n",
    "\n",
    "    features = ['L{}Pos'.format(JOINT)] + ['F' + str(i) + 'z' for i in FORCE_CELLS_PER_JOINT[JOINT]]\n",
    "    targets = ['F' + str(i) + ax for i in FORCE_CELLS_PER_JOINT[JOINT] for ax in ['x', 'y']]\n",
    "\n",
    "    print('Number of features: {}'.format(len(features)))\n",
    "    print('Selected features: {}'.format(features))\n",
    "    print('\\n')\n",
    "    print('Number of targets: {}'.format(len(targets)))\n",
    "    print('Selected targets: {}'.format(targets))\n",
    "    \n",
    "    # Index to crop the data and use only this section of each experiment (start idx, end idx)\n",
    "    # The indexes can be defined manually defining crop_by_index=(start idx, end idx) or seleted at random setting crop_by_index=True\n",
    "    window_size = 200\n",
    "    crop_by_index = False #(1500, 1700) # True\n",
    "\n",
    "    # Sample the experiment data to use only this sample datapoints\n",
    "    random_sample = True\n",
    "    random_sample_pct = 0.05\n",
    "\n",
    "    targets_dict_train = {}\n",
    "    features_dict_train = {}\n",
    "    for i, exp_path in enumerate(experiments_dirs_path_filter_wos):\n",
    "        print('{} - Experiment {} from {}'.format(i, exp_path.split('/')[-1], exp_path.split('/')[-2]))\n",
    "\n",
    "        # Load targets\n",
    "        targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "\n",
    "        # Load features\n",
    "        exo_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "        # leg_df = pd.read_csv(exp_path + '/leg_processed.csv')\n",
    "        # features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "        features_df = exo_df\n",
    "\n",
    "        idx_aux = targets_df.duplicated(keep='first')\n",
    "        targets_df = targets_df.loc[~idx_aux]\n",
    "        features_df = features_df.loc[~idx_aux]\n",
    "        print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "\n",
    "        # Drop first row to remove noise in the start of the data recording\n",
    "        targets_df = targets_df.iloc[1:]\n",
    "        features_df = features_df.iloc[1:]\n",
    "        # Drop null values\n",
    "        idx = features_df.notna().all(axis=1)\n",
    "        features_df = features_df.loc[idx]\n",
    "        targets_df = targets_df.loc[idx]\n",
    "        print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "        assert(len(features_df) == len(targets_df))\n",
    "        data_df = pd.concat([features_df, targets_df], axis=1)\n",
    "\n",
    "        # Crop the data by the indicated indexes\n",
    "        if crop_by_index:\n",
    "            if crop_by_index == True:\n",
    "                start_idx = random.randint(100, len(data_df) - window_size - 100)\n",
    "                crop_by_index = (start_idx, start_idx + window_size)\n",
    "\n",
    "            data_df = data_df.iloc[crop_by_index[0]:crop_by_index[1]]\n",
    "\n",
    "        if random_sample:\n",
    "            data_df = data_df.sample(frac=random_sample_pct, random_state=0)\n",
    "\n",
    "        # Store the final array\n",
    "        targets_dict_train[i] = data_df[targets].values\n",
    "        features_dict_train[i] = data_df[features].values\n",
    "\n",
    "    targets_dict_test = {}\n",
    "    features_dict_test = {}\n",
    "    for i, exp_path in enumerate(experiments_dirs_path_filter):\n",
    "        print('{} - Experiment {} from {}'.format(i, exp_path.split('/')[-1], exp_path.split('/')[-2]))\n",
    "\n",
    "        # Load targets\n",
    "        targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "\n",
    "        # Load features\n",
    "        exo_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "        # leg_df = pd.read_csv(exp_path + '/leg_processed.csv')\n",
    "        # features_df = pd.concat([exo_df, leg_df], axis=1)\n",
    "        features_df = exo_df\n",
    "\n",
    "        idx_aux = targets_df.duplicated(keep='first')\n",
    "        targets_df = targets_df.loc[~idx_aux]\n",
    "        features_df = features_df.loc[~idx_aux]\n",
    "        print('Droping {} duplicated data points'.format(len(idx_aux[idx_aux == False])))\n",
    "\n",
    "        # Drop first row to remove noise in the start of the data recording\n",
    "        targets_df = targets_df.iloc[1:]\n",
    "        features_df = features_df.iloc[1:]\n",
    "        # Drop null values\n",
    "        idx = features_df.notna().all(axis=1)\n",
    "        features_df = features_df.loc[idx]\n",
    "        targets_df = targets_df.loc[idx]\n",
    "        print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "        assert(len(features_df) == len(targets_df))\n",
    "        data_df = pd.concat([features_df, targets_df], axis=1)\n",
    "\n",
    "        # Crop the data by the indicated indexes\n",
    "        if crop_by_index:\n",
    "            if crop_by_index == True:\n",
    "                start_idx = random.randint(100, len(data_df) - window_size - 100)\n",
    "                crop_by_index = (start_idx, start_idx + window_size)\n",
    "\n",
    "            data_df = data_df.iloc[crop_by_index[0]:crop_by_index[1]]\n",
    "\n",
    "        if random_sample:\n",
    "            data_df = data_df.sample(frac=random_sample_pct, random_state=0)\n",
    "\n",
    "        # Store the final array\n",
    "        targets_dict_test[i] = data_df[targets].values\n",
    "        features_dict_test[i] = data_df[features].values\n",
    "\n",
    "    X_train = np.concatenate([v for k, v in features_dict_train.items()], axis=0)\n",
    "    Y_train = np.concatenate([v for k, v in targets_dict_train.items()], axis=0)\n",
    "    \n",
    "    X_test = np.concatenate([v for k, v in features_dict_test.items()], axis=0)\n",
    "    Y_test = np.concatenate([v for k, v in targets_dict_test.items()], axis=0)\n",
    "\n",
    "    s = MinMaxScaler().fit(X_train)\n",
    "\n",
    "    X_train_norm = s.transform(X_train)\n",
    "    X_test_norm = s.transform(X_test)\n",
    "\n",
    "    for m in ['RF', 'XGB', 'SVM', 'KNN']:\n",
    "        RESULTS[JOINT][m] = {}\n",
    "        \n",
    "        HS_DATE = {'RF':'11082021',  'XGB': '10082021', 'SVM': '24082021', 'KNN': '24082021'}\n",
    "        \n",
    "\n",
    "        if m == 'XGB':\n",
    "            results = defaultdict(list)\n",
    "            for target in range(Y_train.shape[1]):\n",
    "                model = load(os.path.join(RESULTS_PATH, DATA_ID, '{}_{}_{}'.format(JOINT, m, HS_DATE[m]), '{}_{}_best_model_{}_{}_{}.joblib'.format(JOINT, m, target, HS_DATE[m], DATA_ID)))\n",
    "\n",
    "                dtest = xgb.DMatrix(data=X_test, label=Y_test[:, target])\n",
    "\n",
    "                test_preds = model.predict(dtest)\n",
    "\n",
    "                results['MAE'].append(mean_absolute_error(Y_test[:, target], test_preds))\n",
    "                results['MSE'].append(mean_squared_error(Y_test[:, target], test_preds))\n",
    "                results['R2'].append(r2_score(Y_test[:, target], test_preds))\n",
    "        else:\n",
    "            model = load(os.path.join(RESULTS_PATH, DATA_ID, '{}_{}_{}'.format(JOINT, m, HS_DATE[m]), '{}_{}_best_model_{}_{}.joblib'.format(JOINT, m, HS_DATE[m], DATA_ID)))\n",
    "            \n",
    "            test_preds = model.predict(X_test_norm)\n",
    "            \n",
    "            results = {\n",
    "                'MAE': mean_absolute_error(Y_test, test_preds, multioutput='raw_values'),\n",
    "                'MSE': mean_squared_error(Y_test, test_preds, multioutput='raw_values'),\n",
    "                'R2': r2_score(Y_test, test_preds, multioutput='raw_values')\n",
    "            }\n",
    "\n",
    "        for f, force in enumerate(['Fx', 'Fy']):\n",
    "            RESULTS[JOINT][m][force] = {}\n",
    "            \n",
    "            for loss in ['MAE', 'MSE', 'R2']:\n",
    "                scores = [results[loss][i + f] for i in range(0, len(CELLS) * 2, 2)]\n",
    "\n",
    "                RESULTS[JOINT][m][force][loss] = {}\n",
    "                RESULTS[JOINT][m][force][loss]['mean'] = np.mean(scores)\n",
    "                RESULTS[JOINT][m][force][loss]['std'] = np.std(scores)\n",
    "                \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "for JOINT in ['Hip', 'Knee', 'Ankle']:\n",
    "    for FORCE in ['Fx', 'Fy']:\n",
    "        scores_mean = [RESULTS[JOINT][model][FORCE]['R2']['mean'] for model in ['RF', 'KNN']]        \n",
    "        scores_std = [RESULTS[JOINT][model][FORCE]['R2']['std'] for model in ['RF', 'KNN']]\n",
    "        \n",
    "        print('{} {}: {:.4f} ± {:.4f}'.format(JOINT, FORCE, np.mean(scores_mean), np.std(scores_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-hanging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
