{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "competent-asthma",
   "metadata": {},
   "source": [
    "# Gap sanity check\n",
    "\n",
    "This notebook do some test to check the source of the gap between the position and torque in the exoskeleton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prospective-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-probe",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crazy-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the figures of this notebooks will be stored\n",
    "FIGURES_DIR = './figures/gap_sanity_check'\n",
    "if not os.path.exists(FIGURES_DIR):\n",
    "    os.makedirs(FIGURES_DIR)\n",
    "    \n",
    "# Directory where the original data is stored\n",
    "ORIGINAL_DATA_DIR = '../../../../../EXOSAFE-DATA'\n",
    "# Directory where the derived data is stored\n",
    "DERIVED_DATA_DIR = '../../../../data'\n",
    "\n",
    "# Number of force cells in the robotic leg\n",
    "N_CELLS = 8\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-toolbox",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-intranet",
   "metadata": {},
   "source": [
    "## Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developing-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found (101):\n",
      "['01-06022021.xlsx', '02-06022021.xlsx', '03-06022021.xlsx', '07-06022021.xlsx', '08-06022021.xlsx', '10-06022021.xlsx', '03-08022021.xlsx', '04-08022021.xlsx', '02-10022021.xlsx', '03-10022021.xlsx', '04-10022021.xlsx', '05-10022021.xlsx', '01-10032021.xlsx', '01-12022021.xlsx', '02-12022021.xlsx', '03-12022021.xlsx', '04-12022021.xlsx', '05-12022021.xlsx', '06-12022021.xlsx', '07-12022021.xlsx', '02-15022021.xlsx', '03-15022021.xlsx', '04-15022021.xlsx', '01-16022021.xlsx', '02-16022021.xlsx', '03-16022021.xlsx', '04-16022021.xlsx', '05-16022021.xlsx', '06-16022021.xlsx', '02-17022021.xlsx', '03-17022021.xlsx', '04-17022021.xlsx', '01-19022021.xlsx', '010-19022021.xlsx', '011-19022021.xlsx', '012-19022021.xlsx', '013-19022021.xlsx', '014-19022021.xlsx', '015-19022021.xlsx', '016-19022021.xlsx', '017-19022021.xlsx', '018-19022021.xlsx', '02-19022021.xlsx', '03-19022021.xlsx', '04-19022021.xlsx', '05-19022021.xlsx', '06-19022021.xlsx', '07-19022021.xlsx', '08-19022021.xlsx', '09-19022021.xlsx', '02-21042021.xlsx', '03-21042021.xlsx', '04-21042021.xlsx', '05-21042021.xlsx', '06-21042021.xlsx', '07-21042021.xlsx', '08-21042021.xlsx', '01-22022021.xlsx', '010-22022021.xlsx', '011-22022021.xlsx', '012-22022021.xlsx', '013-22022021.xlsx', '014-22022021.xlsx', '015-22022021.xlsx', '016-22022021.xlsx', '017-22022021.xlsx', '018-22022021.xlsx', '019-22022021.xlsx', '02-22022021.xlsx', '03-22022021.xlsx', '04-22022021.xlsx', '05-22022021.xlsx', '06-22022021.xlsx', '07-22022021.xlsx', '08-22022021.xlsx', '09-22022021.xlsx', '01-24022021.xlsx', '010-24022021.xlsx', '011-24022021.xlsx', '012-24022021.xlsx', '013-24022021.xlsx', '014-24022021.xlsx', '015-24022021.xlsx', '016-24022021.xlsx', '02-24022021.xlsx', '03-24022021.xlsx', '04-24022021.xlsx', '05-24022021.xlsx', '06-24022021.xlsx', '07-24022021.xlsx', '08-24022021.xlsx', '09-24022021.xlsx', '01-26032021.xlsx', '011-26032021.xlsx', '012-26032021.xlsx', '02-26032021.xlsx', '03-26032021.xlsx', '04-26032021.xlsx', '06-26032021.xlsx', '07-26032021.xlsx', '09-26032021.xlsx']\n"
     ]
    }
   ],
   "source": [
    "data_ls = glob.glob(ORIGINAL_DATA_DIR + '/*/*.xlsx')\n",
    "\n",
    "print('Files found ({}):'.format(len(data_ls)))\n",
    "print([file.split('/')[-1] for file in data_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demographic-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process (63):\n",
      "['01-10032021.xlsx', '01-16022021.xlsx', '02-16022021.xlsx', '03-16022021.xlsx', '04-16022021.xlsx', '05-16022021.xlsx', '06-16022021.xlsx', '02-17022021.xlsx', '03-17022021.xlsx', '04-17022021.xlsx', '01-19022021.xlsx', '010-19022021.xlsx', '011-19022021.xlsx', '012-19022021.xlsx', '013-19022021.xlsx', '014-19022021.xlsx', '015-19022021.xlsx', '016-19022021.xlsx', '017-19022021.xlsx', '018-19022021.xlsx', '02-19022021.xlsx', '03-19022021.xlsx', '04-19022021.xlsx', '05-19022021.xlsx', '06-19022021.xlsx', '07-19022021.xlsx', '08-19022021.xlsx', '09-19022021.xlsx', '01-22022021.xlsx', '010-22022021.xlsx', '011-22022021.xlsx', '012-22022021.xlsx', '013-22022021.xlsx', '014-22022021.xlsx', '015-22022021.xlsx', '016-22022021.xlsx', '017-22022021.xlsx', '018-22022021.xlsx', '019-22022021.xlsx', '02-22022021.xlsx', '03-22022021.xlsx', '04-22022021.xlsx', '05-22022021.xlsx', '06-22022021.xlsx', '07-22022021.xlsx', '08-22022021.xlsx', '09-22022021.xlsx', '01-24022021.xlsx', '010-24022021.xlsx', '011-24022021.xlsx', '012-24022021.xlsx', '013-24022021.xlsx', '014-24022021.xlsx', '015-24022021.xlsx', '016-24022021.xlsx', '02-24022021.xlsx', '03-24022021.xlsx', '04-24022021.xlsx', '05-24022021.xlsx', '06-24022021.xlsx', '07-24022021.xlsx', '08-24022021.xlsx', '09-24022021.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to exclude specific experiments (date: [experiment ids])\n",
    "# These experiments are excluded by lack of data\n",
    "EXPERIMENTS_TO_EXCLUDE = {\n",
    "    '06022021': ['01', '02', '03', '07', '08', '10'],\n",
    "    '08022021': ['03', '04'],\n",
    "    '10022021': ['02', '03', '04', '05'],\n",
    "    '12022021': ['01', '02', '03', '04', '05', '06', '07'],\n",
    "    '15022021': ['02', '03', '04'],\n",
    "    '26032021': ['01', '02', '03', '04', '06', '07', '09', '011', '012'],\n",
    "    '21042021': ['02', '03', '04', '05', '06', '07', '08']\n",
    "}\n",
    "\n",
    "\n",
    "# Exclude some experiments from the list of files to process\n",
    "for exp_date, exp_ids in EXPERIMENTS_TO_EXCLUDE.items():\n",
    "    for i in exp_ids:\n",
    "        data_ls.remove(ORIGINAL_DATA_DIR + '/{}/{}-{}.xlsx'.format(exp_date, i, exp_date))\n",
    "        \n",
    "print('Files to process ({}):'.format(len(data_ls)))\n",
    "print([file.split('/')[-1] for file in data_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-blood",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the algorithm for all files\n",
    "for example, file in enumerate(data_ls):\n",
    "    print(file)\n",
    "    data_df = pd.read_excel(file, sheet_name=['Parameters', 'H3processed', 'Leg-Replica', 'ForceCells'])\n",
    "    \n",
    "    exp_time_shift = data_df['Parameters'].iloc[0]['TimeShift']\n",
    "    print('Experimental time shift: {}'.format(exp_time_shift))\n",
    "    \n",
    "    # exo_arr = data_df['H3processed'].iloc[:, 1].values\n",
    "    # leg_arr = data_df['Leg-Replica'].iloc[:, 3].values\n",
    "    # leg_df_processed_exp = shift_leg_data(data_df['Leg-Replica'], exp_time_shift, len(data_df['H3processed']))\n",
    "    \n",
    "    # Plot hip signals\n",
    "    fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax1.twinx()\n",
    "    \n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.xaxis.set_ticks([])\n",
    "\n",
    "    lns1 = ax1.plot(data_df['H3processed'].iloc[:, 0], label='Hip pos (exo)', c='c')\n",
    "    \n",
    "    lns2 = ax2.plot(data_df['ForceCells']['F6z'], label='Fz top-front (leg)', alpha=0.4)\n",
    "    lns3 = ax2.plot(data_df['ForceCells']['F5z'], label='Fz top-back (leg)', alpha=0.4)\n",
    "    \n",
    "    lns4 = ax3.plot(data_df['H3processed'].iloc[:, 12], label='Hip torque (exo)', c='m')\n",
    "\n",
    "    ax1.set_ylabel('Position (ยบ)')\n",
    "    ax2.set_ylabel('Force Z (N)')\n",
    "    ax3.set_ylabel('Torque (N/m)')\n",
    "    plt.title(example)\n",
    "    \n",
    "    lns = lns1 + lns2 + lns3 + lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper left')\n",
    "\n",
    "    plt.savefig(FIGURES_DIR + '/original_data_hip_{}_{}.png'.format(example, file.split('/')[-1].replace('.xlsx', '')))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot knee signals\n",
    "    fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax1.twinx()\n",
    "    \n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.xaxis.set_ticks([])\n",
    "    \n",
    "    lns1 = ax1.plot(data_df['Leg-Replica'].iloc[:, 3], label='Knee pos (leg)')\n",
    "    lns2 = ax1.plot(data_df['H3processed'].iloc[:, 1], label='Knee pos (exo)', c='c')\n",
    "    \n",
    "    lns3 = ax2.plot(data_df['ForceCells']['F7z'], label='Fz top-front (leg)', alpha=0.4)\n",
    "    lns4 = ax2.plot(data_df['ForceCells']['F8z'], label='Fz top-back (leg)', alpha=0.4)\n",
    "    lns5 = ax2.plot(data_df['ForceCells']['F3z'], label='Fz bottom-front (leg)', alpha=0.8)\n",
    "    lns6 = ax2.plot(data_df['ForceCells']['F4z'], label='Fz bottom-back (leg)', alpha=0.8)\n",
    "    \n",
    "    lns7 = ax3.plot(data_df['H3processed'].iloc[:, 13], label='Knee torque (exo)', c='m')\n",
    "\n",
    "    ax1.set_ylabel('Position (ยบ)')\n",
    "    ax2.set_ylabel('Forze Z (N)')\n",
    "    ax3.set_ylabel('Torque (N/m)')\n",
    "    \n",
    "    lns = lns1 + lns2 + lns3 + lns4 + lns5 + lns6 + lns7\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper left')\n",
    "\n",
    "    plt.savefig(FIGURES_DIR + '/original_data_knee_{}_{}.png'.format(example, file.split('/')[-1].replace('.xlsx', '')))\n",
    "    plt.show()\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-effort",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the algorithm for all files\n",
    "for example, file in enumerate(data_ls):\n",
    "    print(file)\n",
    "    data_df = pd.read_excel(file, sheet_name=['H3processed'])\n",
    "    \n",
    "    # Plot hip signals\n",
    "    joint = 'Hip'\n",
    "    fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    lns1 = ax1.plot(data_df['H3processed'].iloc[:, 0], label='Left pos ({})'.format(joint), linestyle='--')\n",
    "    lns2 = ax1.plot(data_df['H3processed'].iloc[:, 9], label='Right pos ({})'.format(joint), linestyle='--')\n",
    "    \n",
    "    lns3 = ax2.plot(data_df['H3processed'].iloc[:, 12], label='Left torque ({})'.format(joint))\n",
    "    lns4 = ax2.plot(data_df['H3processed'].iloc[:, 15], label='Right torque ({})'.format(joint))\n",
    "\n",
    "    ax1.set_ylabel('Position (ยบ)')\n",
    "    ax2.set_ylabel('Torque (N/m)')\n",
    "    \n",
    "    lns = lns1 + lns2 + lns3 + lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper left')\n",
    "\n",
    "    plt.savefig(FIGURES_DIR + '/original_data_left_right_exo_hip_{}_{}.png'.format(example, file.split('/')[-1].replace('.xlsx', '')))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot knee signals\n",
    "    joint = 'Knee'\n",
    "    fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    lns1 = ax1.plot(data_df['H3processed'].iloc[:, 1], label='Left pos ({})'.format(joint), linestyle='--')\n",
    "    lns2 = ax1.plot(data_df['H3processed'].iloc[:, 10], label='Right pos ({})'.format(joint), linestyle='--')\n",
    "    \n",
    "    lns3 = ax2.plot(data_df['H3processed'].iloc[:, 13], label='Left torque ({})'.format(joint))\n",
    "    lns4 = ax2.plot(data_df['H3processed'].iloc[:, 16], label='Right torque ({})'.format(joint))\n",
    "\n",
    "    ax1.set_ylabel('Position (ยบ)')\n",
    "    ax2.set_ylabel('Torque (N/m)')\n",
    "    \n",
    "    lns = lns1 + lns2 + lns3 + lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper left')\n",
    "\n",
    "    plt.savefig(FIGURES_DIR + '/original_data_left_right_exo_knee_{}_{}.png'.format(example, file.split('/')[-1].replace('.xlsx', '')))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ankle signals\n",
    "    joint = 'Ankle'\n",
    "    fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    lns1 = ax1.plot(data_df['H3processed'].iloc[:, 2], label='Left pos ({})'.format(joint), linestyle='--')\n",
    "    lns2 = ax1.plot(data_df['H3processed'].iloc[:, 11], label='Right pos ({})'.format(joint), linestyle='--')\n",
    "    \n",
    "    lns3 = ax2.plot(data_df['H3processed'].iloc[:, 14], label='Left torque ({})'.format(joint))\n",
    "    lns4 = ax2.plot(data_df['H3processed'].iloc[:, 17], label='Right torque ({})'.format(joint))\n",
    "\n",
    "    ax1.set_ylabel('Position (ยบ)')\n",
    "    ax2.set_ylabel('Torque (N/m)')\n",
    "    \n",
    "    lns = lns1 + lns2 + lns3 + lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper left')\n",
    "\n",
    "    plt.savefig(FIGURES_DIR + '/original_data_left_right_exo_ankle_{}_{}.png'.format(example, file.split('/')[-1].replace('.xlsx', '')))\n",
    "    plt.show()\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-lyric",
   "metadata": {},
   "source": [
    "## Derived data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aquatic-generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 4\n",
      "Selected features: ['LHipPos', 'LHipTorque', 'LKneePos', 'LKneeTorque']\n",
      "\n",
      "\n",
      "Number of targets: 24\n",
      "Selected targets: ['F1x', 'F1y', 'F1z', 'F2x', 'F2y', 'F2z', 'F3x', 'F3y', 'F3z', 'F4x', 'F4y', 'F4z', 'F5x', 'F5y', 'F5z', 'F6x', 'F6y', 'F6z', 'F7x', 'F7y', 'F7z', 'F8x', 'F8y', 'F8z']\n"
     ]
    }
   ],
   "source": [
    "H3_LEG = 'L' # L|R\n",
    "\n",
    "features = [H3_LEG + a + m for a in ['Hip', 'Knee'] for m in ['Pos', 'Torque']]\n",
    "targets = ['F' + str(i + 1) + ax for i in range(N_CELLS) for ax in ['x', 'y', 'z']]\n",
    "\n",
    "print('Number of features: {}'.format(len(features)))\n",
    "print('Selected features: {}'.format(features))\n",
    "print('\\n')\n",
    "print('Number of targets: {}'.format(len(targets)))\n",
    "print('Selected targets: {}'.format(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "excess-affiliation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../../data/10032021/1',\n",
       " '../../../../data/16022021/1',\n",
       " '../../../../data/16022021/2',\n",
       " '../../../../data/16022021/3',\n",
       " '../../../../data/16022021/4',\n",
       " '../../../../data/16022021/5',\n",
       " '../../../../data/16022021/6',\n",
       " '../../../../data/17022021/2',\n",
       " '../../../../data/17022021/3',\n",
       " '../../../../data/17022021/4',\n",
       " '../../../../data/19022021/1',\n",
       " '../../../../data/19022021/10',\n",
       " '../../../../data/19022021/11',\n",
       " '../../../../data/19022021/12',\n",
       " '../../../../data/19022021/13',\n",
       " '../../../../data/19022021/14',\n",
       " '../../../../data/19022021/15',\n",
       " '../../../../data/19022021/16',\n",
       " '../../../../data/19022021/17',\n",
       " '../../../../data/19022021/18',\n",
       " '../../../../data/19022021/2',\n",
       " '../../../../data/19022021/3',\n",
       " '../../../../data/19022021/4',\n",
       " '../../../../data/19022021/5',\n",
       " '../../../../data/19022021/6',\n",
       " '../../../../data/19022021/7',\n",
       " '../../../../data/19022021/8',\n",
       " '../../../../data/19022021/9',\n",
       " '../../../../data/22022021/1',\n",
       " '../../../../data/22022021/10',\n",
       " '../../../../data/22022021/11',\n",
       " '../../../../data/22022021/12',\n",
       " '../../../../data/22022021/13',\n",
       " '../../../../data/22022021/14',\n",
       " '../../../../data/22022021/15',\n",
       " '../../../../data/22022021/16',\n",
       " '../../../../data/22022021/17',\n",
       " '../../../../data/22022021/18',\n",
       " '../../../../data/22022021/19',\n",
       " '../../../../data/22022021/2',\n",
       " '../../../../data/22022021/3',\n",
       " '../../../../data/22022021/4',\n",
       " '../../../../data/22022021/5',\n",
       " '../../../../data/22022021/6',\n",
       " '../../../../data/22022021/7',\n",
       " '../../../../data/22022021/8',\n",
       " '../../../../data/22022021/9',\n",
       " '../../../../data/24022021/1',\n",
       " '../../../../data/24022021/10',\n",
       " '../../../../data/24022021/11',\n",
       " '../../../../data/24022021/12',\n",
       " '../../../../data/24022021/13',\n",
       " '../../../../data/24022021/14',\n",
       " '../../../../data/24022021/15',\n",
       " '../../../../data/24022021/16',\n",
       " '../../../../data/24022021/2',\n",
       " '../../../../data/24022021/3',\n",
       " '../../../../data/24022021/4',\n",
       " '../../../../data/24022021/5',\n",
       " '../../../../data/24022021/6',\n",
       " '../../../../data/24022021/7',\n",
       " '../../../../data/24022021/8',\n",
       " '../../../../data/24022021/9']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_dirs_path = glob.glob(DERIVED_DATA_DIR + '/*/*')\n",
    "experiments_dirs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "understood-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some examples\n",
    "targets_dict = {}\n",
    "features_dict = {}\n",
    "for i, exp_path in enumerate(experiments_dirs_path):\n",
    "    #print(exp_path)\n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(exp_path + '/force_cells_processed.csv')\n",
    "\n",
    "    # Load features\n",
    "    features_df = pd.read_csv(exp_path + '/H3_processed.csv')\n",
    "\n",
    "    # Drop first row to remove noise in the start of the data recording\n",
    "    targets_df = targets_df.iloc[1:]\n",
    "    features_df = features_df.iloc[1:]\n",
    "    # Drop null values\n",
    "    idx = features_df.notna().all(axis=1)\n",
    "    features_df = features_df.loc[idx]\n",
    "    targets_df = targets_df.loc[idx]\n",
    "    #print('Droping {} data points by null features'.format(len(idx[idx == False])))\n",
    "\n",
    "    assert(len(features_df) == len(targets_df))\n",
    "    # Store the final array\n",
    "    targets_dict[i] = targets_df[targets]\n",
    "    features_dict[i] = features_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-tender",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot hip and knee torque and z forces to check the relationship between data\n",
    "for example in range(len(targets_dict)):\n",
    "    print(experiments_dirs_path[example])\n",
    "    \n",
    "    # Plot hip signals\n",
    "    fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax1.twinx()\n",
    "    \n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.xaxis.set_ticks([])\n",
    "\n",
    "    lns1 = ax1.plot(features_dict[example]['LHipPos'], label='Hip pos (exo)', c='c')\n",
    "    \n",
    "    lns2 = ax2.plot(targets_dict[example]['F6z'], label='Fz top-front (leg)', alpha=0.4)\n",
    "    lns3 = ax2.plot(targets_dict[example]['F5z'], label='Fz top-back (leg)', alpha=0.4)\n",
    "    \n",
    "    lns4 = ax3.plot(features_dict[example]['LHipTorque'], label='Hip torque (exo)', c='m')\n",
    "\n",
    "    ax1.set_ylabel('Position (ยบ)')\n",
    "    ax2.set_ylabel('Force Z (N)')\n",
    "    ax3.set_ylabel('Torque (N/m)')\n",
    "    plt.title(example)\n",
    "    \n",
    "    lns = lns1 + lns2 + lns3 + lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper left')\n",
    "\n",
    "    plt.savefig(FIGURES_DIR + '/derived_data_hip_{}_{}.png'.format(example, experiments_dirs_path[example].split('/')[-2]))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot knee signals\n",
    "    fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax1.twinx()\n",
    "    \n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.xaxis.set_ticks([])\n",
    "\n",
    "    lns1 = ax1.plot(features_dict[example]['LKneePos'], label='Knee pos (exo)', c='c')\n",
    "    \n",
    "    lns2 = ax2.plot(targets_dict[example]['F7z'], label='Fz top-front (leg)', alpha=0.4)\n",
    "    lns3 = ax2.plot(targets_dict[example]['F8z'], label='Fz top-back (leg)', alpha=0.4)\n",
    "    lns4 = ax2.plot(targets_dict[example]['F3z'], label='Fz bottom-front (leg)', alpha=0.8)\n",
    "    lns5 = ax2.plot(targets_dict[example]['F4z'], label='Fz bottom-back (leg)', alpha=0.8)\n",
    "    \n",
    "    lns6 = ax3.plot(features_dict[example]['LKneeTorque'], label='Knee torque (exo)', c='m')\n",
    "\n",
    "    ax1.set_ylabel('Position (ยบ)')\n",
    "    ax2.set_ylabel('Force Z (N)')\n",
    "    ax3.set_ylabel('Torque (N/m)')\n",
    "    plt.title(example)\n",
    "    \n",
    "    lns = lns1 + lns2 + lns3 + lns4 + lns5 + lns6\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper left')\n",
    "\n",
    "    plt.savefig(FIGURES_DIR + '/derived_data_knee_{}_{}.png'.format(example, experiments_dirs_path[example].split('/')[-2]))\n",
    "    plt.show()\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-treaty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
